/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
ClipTestArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
attn_implementation=flash_attention_2,
augmentation=False,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
consecutive_n_frames_threshold=1,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
dataset_config=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
embed_mark=2fps_384_1+3x3,
end_idx=None,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=no,
eval_use_gather_object=False,
evaluation_strategy=None,
finetune_modules=['connector', 'mm_projector', 'response_head', 'related_head'],
first_n_frames_no_generate=0,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
frame_fps=1.0,
frame_num_tokens=49,
frame_resolution=384,
frame_token_cls=False,
frame_token_pooled=[7, 7],
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
grounding_mode=False,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
input_dir=/share3/public_share/Ego4D/v2/360_sec-clip_540ss_6fps,
is_online_model=True,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
live_version=test,
llm_pretrained=lmms-lab/llava-onevision-qwen2-7b-ov,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=outputs/debug/runs/Apr30_07-46-27_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lora_alpha=32,
lora_modules=model\.layers.*(q_proj|k_proj|v_proj|o_proj|gate_proj|up_proj|down_proj)$,
lora_pretrained=None,
lora_r=16,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_num_frames=100,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
no_output_before_user_input=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=outputs/debug,
output_fname=outputs/llava-ov/ego4d_goalstep/incremental/8sec.jsonl,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_assistant_turns=False,
remove_unused_columns=True,
repetition_penalty=None,
report_to=['tensorboard', 'wandb'],
response_min_interval_frames=None,
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=outputs/debug,
running_list_length=20,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
score_heads=informative_score,
seed=42,
skip_memory_metrics=True,
split_batches=None,
start_idx=0,
stream_end_prob_threshold=None,
stream_end_score_sum_threshold=None,
stream_loss_weight=1.0,
system_prompt=A multimodal AI assistant is helping users with some activities. Below is their conversation, interleaved with the list of video frames received by the assistant.,
test_fname=../judge_questions/outputs/ego4d_goalstep/val-with_checked_judge_questions.json,
tf32=None,
threshold_z=None,
time_instruction_format=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
v_placeholder=<image>,
video_chunk_sec=8,
video_pooling_stride=4,
vision_pretrained=google/siglip-large-patch16-384,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Loaded LLaVA model: lmms-lab/llava-onevision-qwen2-7b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
Loading vision tower: google/siglip-so400m-patch14-384
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.patch_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.patch_embedding.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.position_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.probe: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.attention.in_proj_weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.attention.in_proj_bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.attention.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.attention.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:14<00:43, 14.47s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:25<00:24, 12.35s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:31<00:09,  9.32s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  6.25s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  8.15s/it]
Model Class: LlavaQwenForCausalLM
  0%|          | 0/321 [00:00<?, ?it/s]question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 8 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 16 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 24 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 32 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 40 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 48 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 56 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 64 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 72 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 80 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 88 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 96 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 104 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 112 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 120 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 128 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 136 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 144 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 152 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 160 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 168 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 176 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 184 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 192 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 200 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 208 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 216 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 224 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 232 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 240 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 248 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 256 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 264 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 272 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-0-360.mp4 0 280 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 8 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 16 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 24 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 32 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 40 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 48 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 56 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 64 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 72 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 80 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 88 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 96 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 104 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 112 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 120 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 128 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 136 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 144 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 152 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 160 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 168 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 176 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 184 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 192 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 200 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 208 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 216 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 224 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 232 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 240 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 248 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 256 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 264 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 272 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 280 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 288 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 296 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 304 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 312 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 320 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 328 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 336 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 344 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-360-720.mp4 0 352 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-720-1080.mp4 0 8 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-720-1080.mp4 0 16 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-720-1080.mp4 0 24 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-720-1080.mp4 0 32 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-720-1080.mp4 0 40 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-720-1080.mp4 0 48 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-720-1080.mp4 0 56 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-720-1080.mp4 0 64 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-720-1080.mp4 0 72 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-720-1080.mp4 0 80 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-720-1080.mp4 0 88 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-720-1080.mp4 0 96 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-720-1080.mp4 0 104 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-720-1080.mp4 0 112 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-720-1080.mp4 0 120 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-720-1080.mp4 0 128 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-720-1080.mp4 0 136 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-720-1080.mp4 0 144 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-720-1080.mp4 0 152 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-720-1080.mp4 0 160 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-720-1080.mp4 0 168 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-720-1080.mp4 0 176 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-720-1080.mp4 0 184 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-720-1080.mp4 0 192 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-720-1080.mp4 0 200 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-720-1080.mp4 0 208 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 8 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 16 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 24 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 32 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 40 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 48 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 56 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 64 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 72 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 80 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 88 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 96 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 104 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 112 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 120 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 128 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 136 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 144 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 152 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 160 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 168 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 176 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 184 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 192 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 200 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 208 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 216 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 224 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 232 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 240 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 248 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 256 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 264 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 272 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 280 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 288 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 296 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 304 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 312 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 320 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 328 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 336 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 344 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1440-1800.mp4 0 352 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 8 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 16 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 24 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 32 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 40 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 48 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 56 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 64 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 72 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 80 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 88 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 96 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 104 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 112 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 120 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 128 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 136 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 144 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 152 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 160 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 168 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 176 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 184 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 192 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 200 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 208 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 216 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 224 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 232 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 240 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 248 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 256 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 264 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 272 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 280 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 288 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 296 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 304 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 312 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 320 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 328 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 336 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 344 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 352 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-1800-2160.mp4 0 360 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 8 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 16 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 24 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 32 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 40 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 48 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 56 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 64 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 72 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 80 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 88 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 96 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 104 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 112 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 120 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 128 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 136 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 144 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 152 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 160 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 168 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 176 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 184 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 192 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 200 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 208 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 216 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 224 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 232 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 240 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 248 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 256 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 264 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 272 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 280 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 288 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 296 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 304 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 312 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 320 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 328 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 336 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 344 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 352 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-2160-2520.mp4 0 360 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 8 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 16 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 24 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 32 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 40 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 48 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 56 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 64 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 72 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 80 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 88 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 96 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 104 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 112 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 120 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 128 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 136 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 144 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 152 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 160 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 168 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 176 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 184 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 192 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 200 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 208 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 216 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 224 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 232 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 240 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 248 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 256 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 264 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 272 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 280 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 288 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3240-3600.mp4 0 296 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 8 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 16 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 24 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 32 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 40 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 48 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 56 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 64 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 72 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 80 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 88 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 96 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 104 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 112 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 120 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 128 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 136 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 144 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 152 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 160 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 168 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 176 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 184 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 192 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 200 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 208 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 216 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 224 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 232 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 240 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 248 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 256 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 264 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 272 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 280 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 288 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 296 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 304 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 312 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 320 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 328 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 336 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3600-3960.mp4 0 344 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3960-4320.mp4 0 8 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3960-4320.mp4 0 16 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3960-4320.mp4 0 24 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3960-4320.mp4 0 32 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3960-4320.mp4 0 40 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3960-4320.mp4 0 48 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3960-4320.mp4 0 56 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3960-4320.mp4 0 64 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3960-4320.mp4 0 72 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3960-4320.mp4 0 80 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3960-4320.mp4 0 88 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3960-4320.mp4 0 96 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3960-4320.mp4 0 104 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3960-4320.mp4 0 112 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3960-4320.mp4 0 120 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3960-4320.mp4 0 128 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3960-4320.mp4 0 136 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3960-4320.mp4 0 144 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3960-4320.mp4 0 152 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3960-4320.mp4 0 160 has been tested, skipping
question c2d06df7-5d3a-4116-9edb-f1c81a4f669b-3960-4320.mp4 0 168 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-0-360.mp4 0 8 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-0-360.mp4 0 16 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-0-360.mp4 0 24 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-0-360.mp4 0 32 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-0-360.mp4 0 40 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-0-360.mp4 0 48 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-0-360.mp4 0 56 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-0-360.mp4 0 64 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-0-360.mp4 0 72 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-0-360.mp4 0 80 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-0-360.mp4 0 88 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-0-360.mp4 0 96 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-0-360.mp4 0 104 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-0-360.mp4 0 112 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-0-360.mp4 0 120 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-0-360.mp4 0 128 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-0-360.mp4 0 136 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-0-360.mp4 0 144 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-0-360.mp4 0 152 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-0-360.mp4 0 160 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-0-360.mp4 0 168 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-360-720.mp4 0 8 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-360-720.mp4 0 16 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-360-720.mp4 0 24 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-360-720.mp4 0 32 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-360-720.mp4 0 40 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-360-720.mp4 0 48 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-360-720.mp4 0 56 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-360-720.mp4 0 64 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-360-720.mp4 0 72 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-360-720.mp4 0 80 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-360-720.mp4 0 88 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-360-720.mp4 0 96 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-360-720.mp4 0 104 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-360-720.mp4 0 112 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-360-720.mp4 0 120 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-360-720.mp4 0 128 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-360-720.mp4 0 136 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-720-1080.mp4 0 8 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-720-1080.mp4 0 16 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-720-1080.mp4 0 24 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-720-1080.mp4 0 32 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-720-1080.mp4 0 40 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-720-1080.mp4 0 48 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-720-1080.mp4 0 56 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-720-1080.mp4 0 64 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-720-1080.mp4 0 72 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-720-1080.mp4 0 80 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-720-1080.mp4 0 88 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-720-1080.mp4 0 96 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-720-1080.mp4 0 104 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-720-1080.mp4 0 112 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-720-1080.mp4 0 120 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-720-1080.mp4 0 128 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-720-1080.mp4 0 136 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-720-1080.mp4 0 144 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-720-1080.mp4 0 152 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-720-1080.mp4 0 160 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-720-1080.mp4 0 168 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-720-1080.mp4 0 176 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-720-1080.mp4 0 184 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-720-1080.mp4 0 192 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-720-1080.mp4 0 200 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-720-1080.mp4 0 208 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-720-1080.mp4 0 216 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-720-1080.mp4 0 224 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-720-1080.mp4 0 232 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 8 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 16 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 24 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 32 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 40 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 48 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 56 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 64 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 72 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 80 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 88 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 96 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 104 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 112 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 120 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 128 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 136 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 144 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 152 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 160 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 168 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 176 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 184 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 192 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 200 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 208 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 216 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 224 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 232 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 240 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 248 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 256 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 264 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 272 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 280 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 288 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 296 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 304 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 312 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 320 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 328 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 336 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1080-1440.mp4 0 344 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 8 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 16 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 24 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 32 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 40 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 48 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 56 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 64 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 72 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 80 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 88 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 96 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 104 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 112 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 120 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 128 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 136 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 144 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 152 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 160 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 168 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 176 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 184 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 192 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 200 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 208 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 216 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 224 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 232 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 240 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 248 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 256 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 264 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 272 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 280 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 288 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 296 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 304 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 312 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 320 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 328 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 336 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 344 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 352 has been tested, skipping
question 0e6fb738-05fc-4dd5-9746-a8e10efe8c20-1440-1800.mp4 0 360 has been tested, skipping
  4%|▍         | 14/321 [00:00<00:02, 129.86it/s]question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 8 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 16 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 24 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 32 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 40 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 48 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 56 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 64 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 72 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 80 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 88 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 96 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 104 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 112 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 120 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 128 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 136 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 144 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 152 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 160 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 168 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 176 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 184 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 192 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 200 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 208 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 216 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 224 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 232 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 240 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 248 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 256 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 264 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 272 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 280 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 288 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 296 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 304 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 312 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 320 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 328 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 336 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 344 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 352 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-720-1080.mp4 0 360 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 8 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 16 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 24 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 32 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 40 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 48 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 56 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 64 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 72 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 80 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 88 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 96 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 104 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 112 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 120 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 128 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 136 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 144 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 152 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 160 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 168 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 176 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 184 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 192 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 200 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 208 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 216 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 224 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 232 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 240 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 248 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 256 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 264 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 272 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 280 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 288 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 296 has been tested, skipping
question b739682e-97d3-468c-ab32-eff051aaf9ec-1440-1800.mp4 0 304 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 8 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 16 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 24 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 32 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 40 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 48 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 56 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 64 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 72 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 80 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 88 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 96 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 104 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 112 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 120 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 128 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 136 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 144 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 152 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 160 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 168 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 176 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 184 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 192 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 200 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 208 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 216 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 224 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 232 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 240 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-0-360.mp4 0 248 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 8 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 16 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 24 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 32 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 40 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 48 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 56 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 64 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 72 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 80 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 88 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 96 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 104 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 112 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 120 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 128 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 136 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 144 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 152 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 160 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 168 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 176 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 184 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 192 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 200 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 208 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 216 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 224 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 232 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 240 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 248 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 256 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 264 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 272 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 280 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 288 has been tested, skipping
question fb7cc35d-3272-44a4-b8f2-15cd24fa345b-360-720.mp4 0 296 has been tested, skipping
get video length sec error for grp-88bae242-7f3a-45d4-b129-5d69b1a1e15a-0-360.mp4, skipping
get video length sec error for grp-88bae242-7f3a-45d4-b129-5d69b1a1e15a-360-720.mp4, skipping
get video length sec error for grp-88bae242-7f3a-45d4-b129-5d69b1a1e15a-720-1080.mp4, skipping
get video length sec error for grp-88bae242-7f3a-45d4-b129-5d69b1a1e15a-1080-1440.mp4, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 8 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 16 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 24 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 32 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 40 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 48 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 56 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 64 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 72 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 80 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 88 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 96 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 104 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 112 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 120 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 128 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 136 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 144 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 152 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 160 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 168 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 176 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 184 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 192 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 200 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 208 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 216 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 224 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 232 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 240 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 248 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 256 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 264 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 272 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 280 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 288 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 296 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 304 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 312 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 320 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 328 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 336 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 344 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 352 has been tested, skipping
question 45dc74e1-c8dd-443a-a7a6-ca4215144e97-0-360.mp4 0 360 has been tested, skipping
get video length sec error for grp-719d9e89-4eb2-49ea-be14-dc2637dc303f-0-360.mp4, skipping
get video length sec error for grp-719d9e89-4eb2-49ea-be14-dc2637dc303f-360-720.mp4, skipping
get video length sec error for grp-719d9e89-4eb2-49ea-be14-dc2637dc303f-1080-1440.mp4, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 8 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 16 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 24 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 32 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 40 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 48 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 56 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 64 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 72 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 80 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 88 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 96 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 104 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 112 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 120 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 128 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 136 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 144 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 152 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 160 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 168 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 176 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 184 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 192 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 200 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 208 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 216 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 224 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 232 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 240 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 248 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 256 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 264 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 272 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 280 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 288 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 296 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 304 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 312 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 320 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 328 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 336 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-0-360.mp4 0 344 has been tested, skipping
  8%|▊         | 27/321 [00:00<00:02, 127.28it/s]question cc575a16-64fd-4cda-9248-5d85f506fdfd-720-1080.mp4 0 8 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-720-1080.mp4 0 16 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-720-1080.mp4 0 24 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-720-1080.mp4 0 32 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-720-1080.mp4 0 40 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-720-1080.mp4 0 48 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-720-1080.mp4 0 56 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-720-1080.mp4 0 64 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-720-1080.mp4 0 72 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-720-1080.mp4 0 80 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-720-1080.mp4 0 88 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-720-1080.mp4 0 96 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-720-1080.mp4 0 104 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-720-1080.mp4 0 112 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-720-1080.mp4 0 120 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-720-1080.mp4 0 128 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-720-1080.mp4 0 136 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-720-1080.mp4 0 144 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-720-1080.mp4 0 152 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-720-1080.mp4 0 160 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-720-1080.mp4 0 168 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 8 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 16 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 24 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 32 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 40 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 48 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 56 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 64 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 72 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 80 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 88 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 96 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 104 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 112 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 120 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 128 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 136 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 144 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 152 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 160 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 168 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 176 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 184 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 192 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 200 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 208 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 216 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 224 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 232 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 240 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 248 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 256 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 264 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 272 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 280 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 288 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 296 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 304 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 312 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 320 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1080-1440.mp4 0 328 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 8 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 16 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 24 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 32 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 40 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 48 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 56 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 64 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 72 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 80 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 88 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 96 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 104 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 112 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 120 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 128 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 136 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 144 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 152 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 160 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 168 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 176 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 184 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 192 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 200 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 208 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 216 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 224 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 232 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 240 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 248 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 256 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 264 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 272 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 280 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 288 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 296 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 304 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 312 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 320 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 328 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 336 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 344 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 352 has been tested, skipping
question cc575a16-64fd-4cda-9248-5d85f506fdfd-1440-1800.mp4 0 360 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 8 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 16 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 24 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 32 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 40 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 48 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 56 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 64 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 72 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 80 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 88 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 96 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 104 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 112 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 120 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 128 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 136 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 144 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 152 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 160 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 168 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 176 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 184 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 192 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 200 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 208 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 216 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 224 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 232 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 240 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 248 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 256 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 264 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 272 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 280 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 288 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 296 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 304 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 312 has been tested, skipping
question 77362fa3-e23a-4942-bd53-7d58b23b979b-0-360.mp4 0 320 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 8 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 16 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 24 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 32 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 40 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 48 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 56 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 64 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 72 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 80 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 88 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 96 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 104 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 112 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 120 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 128 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 136 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 144 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 152 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 160 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 168 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 176 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 184 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 192 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 200 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 208 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 216 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 224 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 232 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 240 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 248 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-0-360.mp4 0 256 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 8 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 16 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 24 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 32 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 40 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 48 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 56 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 64 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 72 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 80 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 88 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 96 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 104 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 112 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 120 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 128 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 136 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 144 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 152 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 160 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 168 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 176 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 184 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 192 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 200 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 208 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 216 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 224 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 232 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 240 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 248 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 256 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 264 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 272 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-360-720.mp4 0 280 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 8 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 16 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 24 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 32 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 40 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 48 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 56 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 64 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 72 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 80 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 88 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 96 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 104 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 112 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 120 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 128 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 136 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 144 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 152 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 160 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 168 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 176 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 184 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 192 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 200 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 208 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 216 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 224 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 232 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 240 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 248 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 256 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 264 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 272 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 280 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-720-1080.mp4 0 288 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 8 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 16 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 24 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 32 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 40 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 48 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 56 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 64 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 72 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 80 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 88 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 96 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 104 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 112 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 120 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 128 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 136 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 144 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 152 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 160 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 168 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 176 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 184 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 192 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 200 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 208 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 216 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 224 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 232 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 240 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 248 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 256 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 264 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 272 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 280 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1080-1440.mp4 0 288 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1440-1800.mp4 0 8 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1440-1800.mp4 0 16 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1440-1800.mp4 0 24 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1440-1800.mp4 0 32 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1440-1800.mp4 0 40 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1440-1800.mp4 0 48 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1440-1800.mp4 0 56 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1440-1800.mp4 0 64 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1440-1800.mp4 0 72 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1440-1800.mp4 0 80 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1440-1800.mp4 0 88 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1440-1800.mp4 0 96 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1440-1800.mp4 0 104 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1440-1800.mp4 0 112 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1440-1800.mp4 0 120 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1440-1800.mp4 0 128 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1440-1800.mp4 0 136 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1440-1800.mp4 0 144 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1440-1800.mp4 0 152 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1440-1800.mp4 0 160 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1440-1800.mp4 0 168 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1440-1800.mp4 0 176 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1440-1800.mp4 0 184 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1440-1800.mp4 0 192 has been tested, skipping
question 2f23b607-f2e6-4f58-85d3-004c840bead2-1440-1800.mp4 0 200 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 8 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 16 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 24 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 32 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 40 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 48 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 56 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 64 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 72 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 80 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 88 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 96 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 104 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 112 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 120 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 128 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 136 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 144 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 152 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 160 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 168 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 176 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 184 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 192 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 200 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 208 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 216 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 224 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 232 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 240 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 248 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 256 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 264 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 272 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 280 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 288 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 296 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 304 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 312 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 320 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 328 has been tested, skipping
question 0fcf23a0-fc53-4378-9a99-18c4f109f659-0-360.mp4 0 336 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 8 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 16 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 24 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 32 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 40 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 48 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 56 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 64 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 72 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 80 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 88 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 96 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 104 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 112 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 120 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 128 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 136 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 144 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 152 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 160 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 168 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 176 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 184 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 192 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 200 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 208 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 216 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 224 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 232 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 240 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 248 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 256 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 264 has been tested, skipping
question 90e4acc5-28a2-4bd4-972c-c6f7e18e1cde-0-360.mp4 0 272 has been tested, skipping
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:612: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  8%|▊         | 27/321 [00:18<00:02, 127.28it/s] 12%|█▏        | 38/321 [01:14<13:05,  2.78s/it]  12%|█▏        | 39/321 [05:00<1:05:40, 13.97s/it] 12%|█▏        | 40/321 [09:06<2:18:18, 29.53s/it] 13%|█▎        | 41/321 [11:58<3:16:01, 42.01s/it] 13%|█▎        | 42/321 [13:10<3:30:44, 45.32s/it] 13%|█▎        | 43/321 [15:34<4:32:48, 58.88s/it] 14%|█▎        | 44/321 [19:15<6:33:54, 85.32s/it] 14%|█▍        | 45/321 [21:44<7:28:00, 97.39s/it] 14%|█▍        | 46/321 [25:23<9:25:09, 123.31s/it] 15%|█▍        | 47/321 [29:01<11:04:47, 145.57s/it] 15%|█▍        | 48/321 [32:45<12:31:07, 165.08s/it] 15%|█▌        | 49/321 [34:48<11:37:38, 153.89s/it] 16%|█▌        | 50/321 [37:12<11:23:52, 151.41s/it] 16%|█▌        | 51/321 [41:32<13:37:55, 181.76s/it] 16%|█▌        | 52/321 [44:56<14:03:50, 188.22s/it] 17%|█▋        | 53/321 [48:28<14:31:12, 195.05s/it] 17%|█▋        | 54/321 [51:26<14:05:33, 190.01s/it] 17%|█▋        | 55/321 [54:52<14:23:51, 194.85s/it] 17%|█▋        | 56/321 [58:17<14:32:46, 197.61s/it] 18%|█▊        | 57/321 [1:02:13<15:20:24, 209.18s/it] 18%|█▊        | 58/321 [1:04:10<13:16:36, 181.73s/it] 18%|█▊        | 59/321 [1:07:00<12:58:39, 178.32s/it] 19%|█▊        | 60/321 [1:10:51<14:04:01, 194.03s/it] 19%|█▉        | 61/321 [1:14:28<14:30:12, 200.82s/it]get video length sec error for grp-8912a138-6c68-424d-8be5-59e0d4f5c173-0-360.mp4, skipping
get video length sec error for grp-8912a138-6c68-424d-8be5-59e0d4f5c173-360-720.mp4, skipping
get video length sec error for grp-8912a138-6c68-424d-8be5-59e0d4f5c173-720-1080.mp4, skipping
 20%|██        | 65/321 [1:17:38<7:23:40, 103.99s/it]  21%|██        | 66/321 [1:19:08<7:11:33, 101.54s/it] 21%|██        | 67/321 [1:22:36<8:43:15, 123.60s/it] 21%|██        | 68/321 [1:26:20<10:18:06, 146.59s/it] 21%|██▏       | 69/321 [1:28:40<10:09:32, 145.13s/it] 22%|██▏       | 70/321 [1:29:29<8:22:09, 120.04s/it]  22%|██▏       | 71/321 [1:32:37<9:36:47, 138.43s/it] 22%|██▏       | 72/321 [1:35:23<10:06:22, 146.12s/it] 23%|██▎       | 73/321 [1:38:56<11:23:00, 165.24s/it] 23%|██▎       | 74/321 [1:40:12<9:33:34, 139.33s/it]  23%|██▎       | 75/321 [1:43:46<11:01:02, 161.23s/it] 24%|██▎       | 76/321 [1:47:31<12:15:04, 180.02s/it] 24%|██▍       | 77/321 [1:48:47<10:06:25, 149.12s/it] 24%|██▍       | 78/321 [1:52:32<11:35:13, 171.66s/it] 25%|██▍       | 79/321 [1:56:18<12:37:37, 187.84s/it] 25%|██▍       | 80/321 [1:59:32<12:41:55, 189.69s/it] 25%|██▌       | 81/321 [2:02:58<12:58:31, 194.63s/it] 26%|██▌       | 82/321 [2:05:51<12:28:58, 188.03s/it] 26%|██▌       | 83/321 [2:06:57<10:01:36, 151.67s/it] 26%|██▌       | 84/321 [2:09:15<9:42:26, 147.45s/it]  26%|██▋       | 85/321 [2:11:03<8:53:46, 135.70s/it] 27%|██▋       | 86/321 [2:14:06<9:46:30, 149.75s/it] 27%|██▋       | 87/321 [2:17:20<10:36:00, 163.08s/it] 27%|██▋       | 88/321 [2:20:20<10:53:28, 168.28s/it] 28%|██▊       | 89/321 [2:24:05<11:55:51, 185.14s/it] 28%|██▊       | 90/321 [2:27:30<12:15:57, 191.16s/it] 28%|██▊       | 91/321 [2:31:18<12:55:07, 202.21s/it] 29%|██▊       | 92/321 [2:33:05<11:03:03, 173.73s/it] 29%|██▉       | 93/321 [2:36:09<11:11:27, 176.70s/it] 29%|██▉       | 94/321 [2:39:47<11:56:05, 189.27s/it] 30%|██▉       | 95/321 [2:41:21<10:04:20, 160.44s/it] 30%|██▉       | 96/321 [2:42:17<8:04:28, 129.19s/it]  30%|███       | 97/321 [2:44:47<8:25:52, 135.50s/it] 31%|███       | 98/321 [2:47:24<8:47:49, 142.02s/it]get video length sec error for grp-cec778f9-9b54-4b67-b013-116378fd7a85-0-360.mp4, skipping
 31%|███       | 100/321 [2:49:48<6:44:04, 109.70s/it]get video length sec error for grp-ebce88dd-4852-4506-9dcc-5f5798ce1cbf-0-360.mp4, skipping
get video length sec error for grp-ebce88dd-4852-4506-9dcc-5f5798ce1cbf-360-720.mp4, skipping
get video length sec error for grp-ebce88dd-4852-4506-9dcc-5f5798ce1cbf-1080-1440.mp4, skipping
 32%|███▏      | 104/321 [2:52:25<4:11:51, 69.64s/it]  33%|███▎      | 105/321 [2:54:44<4:52:43, 81.31s/it] 33%|███▎      | 106/321 [2:58:09<6:17:18, 105.30s/it] 33%|███▎      | 107/321 [3:00:52<7:00:41, 117.95s/it] 34%|███▎      | 108/321 [3:04:14<8:09:16, 137.82s/it] 34%|███▍      | 109/321 [3:06:33<8:08:13, 138.18s/it] 34%|███▍      | 110/321 [3:10:21<9:29:42, 162.00s/it] 35%|███▍      | 111/321 [3:12:44<9:08:26, 156.70s/it] 35%|███▍      | 112/321 [3:16:16<10:00:27, 172.38s/it] 35%|███▌      | 113/321 [3:16:57<7:46:49, 134.66s/it]  36%|███▌      | 114/321 [3:18:08<6:40:40, 116.14s/it] 36%|███▌      | 115/321 [3:19:55<6:29:23, 113.41s/it] 36%|███▌      | 116/321 [3:21:07<5:45:39, 101.17s/it] 36%|███▋      | 117/321 [3:24:41<7:37:47, 134.64s/it] 37%|███▋      | 118/321 [3:28:18<8:58:41, 159.22s/it]get video length sec error for grp-3549e738-4cce-492f-a530-e473777e3220-0-360.mp4, skipping
get video length sec error for grp-3549e738-4cce-492f-a530-e473777e3220-360-720.mp4, skipping
get video length sec error for grp-3549e738-4cce-492f-a530-e473777e3220-720-1080.mp4, skipping
get video length sec error for grp-1a9bb53f-01ae-4bd7-afa3-a92570679a7a-0-360.mp4, skipping
get video length sec error for grp-1a9bb53f-01ae-4bd7-afa3-a92570679a7a-360-720.mp4, skipping
get video length sec error for grp-1a9bb53f-01ae-4bd7-afa3-a92570679a7a-720-1080.mp4, skipping
get video length sec error for grp-1a9bb53f-01ae-4bd7-afa3-a92570679a7a-1080-1440.mp4, skipping
get video length sec error for grp-1a9bb53f-01ae-4bd7-afa3-a92570679a7a-1440-1800.mp4, skipping
 40%|███▉      | 127/321 [3:31:35<2:42:37, 50.30s/it]  40%|███▉      | 128/321 [3:34:58<3:36:52, 67.42s/it] 40%|████      | 129/321 [3:35:51<3:29:06, 65.35s/it] 40%|████      | 130/321 [3:38:00<4:01:27, 75.85s/it]get video length sec error for grp-3d7ccb44-b05d-4b67-baae-4e0f55d8307b-0-360.mp4, skipping
get video length sec error for grp-3d7ccb44-b05d-4b67-baae-4e0f55d8307b-360-720.mp4, skipping
get video length sec error for grp-3d7ccb44-b05d-4b67-baae-4e0f55d8307b-1080-1440.mp4, skipping
get video length sec error for grp-3d7ccb44-b05d-4b67-baae-4e0f55d8307b-1440-1800.mp4, skipping
get video length sec error for grp-3d7ccb44-b05d-4b67-baae-4e0f55d8307b-2520-2880.mp4, skipping
get video length sec error for grp-3d7ccb44-b05d-4b67-baae-4e0f55d8307b-3240-3600.mp4, skipping
 43%|████▎     | 137/321 [3:39:36<1:54:13, 37.25s/it] 43%|████▎     | 138/321 [3:43:07<2:53:16, 56.81s/it] 43%|████▎     | 139/321 [3:46:54<4:03:50, 80.39s/it] 44%|████▎     | 140/321 [3:50:40<5:14:54, 104.39s/it] 44%|████▍     | 141/321 [3:52:44<5:24:34, 108.19s/it] 44%|████▍     | 142/321 [3:56:27<6:36:14, 132.82s/it] 45%|████▍     | 143/321 [3:59:41<7:16:28, 147.13s/it] 45%|████▍     | 144/321 [4:00:35<6:05:01, 123.74s/it] 45%|████▌     | 145/321 [4:04:24<7:24:13, 151.44s/it] 45%|████▌     | 146/321 [4:07:11<7:34:08, 155.71s/it] 46%|████▌     | 147/321 [4:09:26<7:15:01, 150.01s/it] 46%|████▌     | 148/321 [4:13:14<8:17:03, 172.39s/it] 46%|████▋     | 149/321 [4:14:38<6:59:52, 146.47s/it]get video length sec error for grp-056db3f1-f957-46c8-b16b-c8fce22e78f9-0-360.mp4, skipping
get video length sec error for grp-056db3f1-f957-46c8-b16b-c8fce22e78f9-360-720.mp4, skipping
get video length sec error for grp-056db3f1-f957-46c8-b16b-c8fce22e78f9-720-1080.mp4, skipping
 48%|████▊     | 153/321 [4:16:51<3:32:20, 75.83s/it]  48%|████▊     | 154/321 [4:21:20<5:09:06, 111.06s/it] 48%|████▊     | 155/321 [4:24:57<6:08:07, 133.06s/it] 49%|████▊     | 156/321 [4:28:58<7:13:30, 157.64s/it] 49%|████▉     | 157/321 [4:29:32<5:48:00, 127.32s/it] 49%|████▉     | 158/321 [4:33:37<7:08:41, 157.80s/it] 50%|████▉     | 159/321 [4:35:08<6:17:24, 139.78s/it] 50%|████▉     | 160/321 [4:37:59<6:38:16, 148.43s/it] 50%|█████     | 161/321 [4:39:12<5:38:43, 127.02s/it] 50%|█████     | 162/321 [4:42:21<6:23:23, 144.68s/it]get video length sec error for grp-2bccee1b-0ade-47ad-8e15-ad6c00861540-0-360.mp4, skipping
get video length sec error for grp-2bccee1b-0ade-47ad-8e15-ad6c00861540-360-720.mp4, skipping
get video length sec error for grp-2bccee1b-0ade-47ad-8e15-ad6c00861540-720-1080.mp4, skipping
get video length sec error for grp-2bccee1b-0ade-47ad-8e15-ad6c00861540-1440-1800.mp4, skipping
get video length sec error for grp-c56fa4ea-b7e1-4d2c-b1f3-b97bb6fe0c56-0-360.mp4, skipping
get video length sec error for grp-c56fa4ea-b7e1-4d2c-b1f3-b97bb6fe0c56-360-720.mp4, skipping
get video length sec error for grp-93d50e4c-0a3b-430a-8267-01fbe5f302f4-0-360.mp4, skipping
get video length sec error for grp-93d50e4c-0a3b-430a-8267-01fbe5f302f4-360-720.mp4, skipping
 53%|█████▎    | 171/321 [4:44:43<1:47:48, 43.12s/it]  54%|█████▎    | 172/321 [4:47:17<2:17:41, 55.45s/it] 54%|█████▍    | 173/321 [4:51:09<3:16:41, 79.74s/it] 54%|█████▍    | 174/321 [4:53:15<3:33:44, 87.24s/it] 55%|█████▍    | 175/321 [4:56:54<4:33:04, 112.22s/it] 55%|█████▍    | 176/321 [5:00:38<5:28:54, 136.10s/it] 55%|█████▌    | 177/321 [5:03:48<5:56:42, 148.63s/it] 55%|█████▌    | 178/321 [5:07:06<6:23:20, 160.85s/it] 56%|█████▌    | 179/321 [5:11:09<7:12:08, 182.59s/it] 56%|█████▌    | 180/321 [5:15:13<7:48:10, 199.23s/it]get video length sec error for grp-b93ab731-52c3-43b2-9d99-4229663ba67c-360-720.mp4, skipping
 57%|█████▋    | 182/321 [5:17:12<5:19:58, 138.12s/it] 57%|█████▋    | 183/321 [5:20:50<6:01:06, 157.00s/it]get video length sec error for grp-c56e7e04-8787-4df1-98c6-352076f61e53-0-360.mp4, skipping
get video length sec error for grp-c56e7e04-8787-4df1-98c6-352076f61e53-360-720.mp4, skipping
get video length sec error for grp-c56e7e04-8787-4df1-98c6-352076f61e53-720-1080.mp4, skipping
get video length sec error for grp-c56e7e04-8787-4df1-98c6-352076f61e53-1080-1440.mp4, skipping
get video length sec error for grp-c56e7e04-8787-4df1-98c6-352076f61e53-1440-1800.mp4, skipping
get video length sec error for grp-c56e7e04-8787-4df1-98c6-352076f61e53-1800-2160.mp4, skipping
 59%|█████▉    | 190/321 [5:24:44<2:32:44, 69.96s/it]  60%|█████▉    | 191/321 [5:24:59<2:16:36, 63.05s/it] 60%|█████▉    | 192/321 [5:26:00<2:14:54, 62.75s/it]get video length sec error for grp-67759ea1-52a2-45ee-b129-f482532d83df-360-720.mp4, skipping
 60%|██████    | 194/321 [5:26:34<1:43:37, 48.96s/it] 61%|██████    | 195/321 [5:27:43<1:50:04, 52.42s/it] 61%|██████    | 196/321 [5:31:34<3:04:46, 88.69s/it] 61%|██████▏   | 197/321 [5:34:56<3:55:43, 114.06s/it] 62%|██████▏   | 198/321 [5:38:00<4:28:56, 131.19s/it] 62%|██████▏   | 199/321 [5:41:24<5:04:46, 149.89s/it]get video length sec error for grp-d8f0089b-cac5-41f4-af66-e11d6f54a43c-0-360.mp4, skipping
get video length sec error for grp-d8f0089b-cac5-41f4-af66-e11d6f54a43c-360-720.mp4, skipping
get video length sec error for grp-d8f0089b-cac5-41f4-af66-e11d6f54a43c-720-1080.mp4, skipping
get video length sec error for grp-d8f0089b-cac5-41f4-af66-e11d6f54a43c-1080-1440.mp4, skipping
 64%|██████▎   | 204/321 [5:45:37<2:46:55, 85.60s/it]  64%|██████▍   | 205/321 [5:49:06<3:22:46, 104.89s/it] 64%|██████▍   | 206/321 [5:51:57<3:44:07, 116.94s/it] 64%|██████▍   | 207/321 [5:55:00<4:07:50, 130.45s/it] 65%|██████▍   | 208/321 [5:56:53<3:58:12, 126.48s/it] 65%|██████▌   | 209/321 [6:00:38<4:41:09, 150.62s/it] 65%|██████▌   | 210/321 [6:04:27<5:16:24, 171.03s/it]get video length sec error for grp-84322f50-7fce-4d49-a04b-9ecd7afd4119-0-360.mp4, skipping
get video length sec error for grp-84322f50-7fce-4d49-a04b-9ecd7afd4119-360-720.mp4, skipping
get video length sec error for grp-79f47a60-f1e9-4232-88b8-a1836e7dfd30-0-360.mp4, skipping
get video length sec error for grp-79f47a60-f1e9-4232-88b8-a1836e7dfd30-360-720.mp4, skipping
get video length sec error for grp-79f47a60-f1e9-4232-88b8-a1836e7dfd30-720-1080.mp4, skipping
 67%|██████▋   | 216/321 [6:06:25<1:56:28, 66.56s/it]  68%|██████▊   | 217/321 [6:09:29<2:24:12, 83.20s/it] 68%|██████▊   | 218/321 [6:11:54<2:40:43, 93.63s/it] 68%|██████▊   | 219/321 [6:15:26<3:17:48, 116.35s/it] 69%|██████▊   | 220/321 [6:19:04<3:52:59, 138.41s/it] 69%|██████▉   | 221/321 [6:22:47<4:24:06, 158.47s/it] 69%|██████▉   | 222/321 [6:26:33<4:49:19, 175.35s/it] 69%|██████▉   | 223/321 [6:30:12<5:05:39, 187.14s/it] 70%|██████▉   | 224/321 [6:32:47<4:48:01, 178.16s/it] 70%|███████   | 225/321 [6:35:47<4:45:55, 178.70s/it] 70%|███████   | 226/321 [6:39:32<5:04:08, 192.09s/it] 71%|███████   | 227/321 [6:41:54<4:37:51, 177.36s/it] 71%|███████   | 228/321 [6:45:38<4:56:06, 191.04s/it] 71%|███████▏  | 229/321 [6:47:20<4:12:45, 164.85s/it] 72%|███████▏  | 230/321 [6:49:30<3:54:13, 154.44s/it] 72%|███████▏  | 231/321 [6:53:16<4:23:37, 175.75s/it] 72%|███████▏  | 232/321 [6:56:35<4:30:57, 182.67s/it] 73%|███████▎  | 233/321 [7:00:19<4:46:17, 195.20s/it] 73%|███████▎  | 234/321 [7:04:33<5:08:22, 212.67s/it] 73%|███████▎  | 235/321 [7:08:09<5:06:10, 213.61s/it] 74%|███████▎  | 236/321 [7:12:00<5:10:16, 219.01s/it] 74%|███████▍  | 237/321 [7:15:54<5:12:37, 223.30s/it] 74%|███████▍  | 238/321 [7:18:54<4:51:04, 210.41s/it] 74%|███████▍  | 239/321 [7:22:17<4:44:42, 208.32s/it] 75%|███████▍  | 240/321 [7:25:01<4:22:56, 194.77s/it] 75%|███████▌  | 241/321 [7:28:26<4:24:02, 198.03s/it] 75%|███████▌  | 242/321 [7:30:51<3:59:32, 181.93s/it] 76%|███████▌  | 243/321 [7:34:19<4:06:49, 189.86s/it] 76%|███████▌  | 244/321 [7:37:52<4:12:41, 196.91s/it] 76%|███████▋  | 245/321 [7:37:59<2:57:01, 139.75s/it] 76%|███████▋  | 245/321 [7:38:09<2:57:01, 139.75s/it] 77%|███████▋  | 246/321 [7:41:18<3:16:51, 157.49s/it] 77%|███████▋  | 247/321 [7:45:05<3:39:57, 178.34s/it] 77%|███████▋  | 248/321 [7:45:24<2:38:53, 130.59s/it] 78%|███████▊  | 249/321 [7:46:41<2:17:32, 114.61s/it] 78%|███████▊  | 250/321 [7:51:22<3:14:37, 164.47s/it] 78%|███████▊  | 251/321 [7:52:46<2:43:49, 140.42s/it] 79%|███████▊  | 252/321 [7:55:24<2:47:26, 145.60s/it] 79%|███████▉  | 253/321 [7:57:21<2:35:18, 137.04s/it]get video length sec error for grp-f248a4aa-dd14-42fb-91fc-ec9f6bc16f20-0-360.mp4, skipping
get video length sec error for grp-f248a4aa-dd14-42fb-91fc-ec9f6bc16f20-360-720.mp4, skipping
get video length sec error for grp-5251b41e-91b1-4b87-9a18-c631b1d17e2e-0-360.mp4, skipping
get video length sec error for grp-5251b41e-91b1-4b87-9a18-c631b1d17e2e-360-720.mp4, skipping
get video length sec error for grp-5251b41e-91b1-4b87-9a18-c631b1d17e2e-720-1080.mp4, skipping
 81%|████████  | 259/321 [7:58:13<46:03, 44.57s/it]    81%|████████  | 260/321 [8:01:15<1:05:47, 64.72s/it] 81%|████████▏ | 261/321 [8:04:34<1:27:56, 87.94s/it] 82%|████████▏ | 262/321 [8:05:30<1:20:21, 81.71s/it] 82%|████████▏ | 263/321 [8:08:27<1:39:16, 102.70s/it] 82%|████████▏ | 264/321 [8:12:06<2:03:53, 130.41s/it] 83%|████████▎ | 265/321 [8:12:49<1:40:58, 108.19s/it]get video length sec error for grp-304735ba-6bf5-4d39-bcb5-0dabddb11d68-0-360.mp4, skipping
get video length sec error for grp-304735ba-6bf5-4d39-bcb5-0dabddb11d68-360-720.mp4, skipping
 83%|████████▎ | 268/321 [8:15:22<1:09:16, 78.42s/it] get video length sec error for grp-7fb63b81-0a4f-4e9e-906b-60d1935d53c7-360-720.mp4, skipping
get video length sec error for grp-7fb63b81-0a4f-4e9e-906b-60d1935d53c7-1080-1440.mp4, skipping
get video length sec error for grp-7fb63b81-0a4f-4e9e-906b-60d1935d53c7-1440-1800.mp4, skipping
get video length sec error for grp-7fb63b81-0a4f-4e9e-906b-60d1935d53c7-1800-2160.mp4, skipping
 85%|████████▌ | 273/321 [8:16:31<34:04, 42.59s/it]   85%|████████▌ | 274/321 [8:19:31<48:05, 61.40s/it] 86%|████████▌ | 275/321 [8:23:20<1:08:03, 88.77s/it]get video length sec error for grp-51fc62f8-00f4-44e3-af9c-7ebb63da6c3d-360-720.mp4, skipping
get video length sec error for grp-51fc62f8-00f4-44e3-af9c-7ebb63da6c3d-1080-1440.mp4, skipping
get video length sec error for grp-51fc62f8-00f4-44e3-af9c-7ebb63da6c3d-1440-1800.mp4, skipping
get video length sec error for grp-51fc62f8-00f4-44e3-af9c-7ebb63da6c3d-1800-2160.mp4, skipping
get video length sec error for grp-51fc62f8-00f4-44e3-af9c-7ebb63da6c3d-2160-2520.mp4, skipping
 88%|████████▊ | 281/321 [8:26:46<37:58, 56.97s/it]   88%|████████▊ | 282/321 [8:29:52<47:17, 72.75s/it] 88%|████████▊ | 283/321 [8:30:04<40:22, 63.74s/it]get video length sec error for grp-e5fb613b-1c6a-4439-8735-6414a6344c76-0-360.mp4, skipping
get video length sec error for grp-e5fb613b-1c6a-4439-8735-6414a6344c76-360-720.mp4, skipping
 89%|████████▉ | 286/321 [8:30:11<23:13, 39.81s/it] 89%|████████▉ | 286/321 [8:30:29<23:13, 39.81s/it] 89%|████████▉ | 287/321 [8:34:09<40:05, 70.74s/it] 90%|████████▉ | 288/321 [8:36:19<44:55, 81.67s/it] 90%|█████████ | 289/321 [8:37:04<39:26, 73.95s/it] 90%|█████████ | 290/321 [8:37:39<33:38, 65.11s/it] 91%|█████████ | 291/321 [8:39:37<39:07, 78.26s/it] 91%|█████████ | 292/321 [8:42:30<49:40, 102.77s/it]get video length sec error for grp-9c5c9efc-608f-4fdf-9c29-2251a451c8f9-0-360.mp4, skipping
get video length sec error for grp-9c5c9efc-608f-4fdf-9c29-2251a451c8f9-360-720.mp4, skipping
get video length sec error for grp-9c5c9efc-608f-4fdf-9c29-2251a451c8f9-720-1080.mp4, skipping
 92%|█████████▏| 296/321 [8:46:18<31:26, 75.45s/it]  93%|█████████▎| 297/321 [8:48:48<35:23, 88.48s/it] 93%|█████████▎| 298/321 [8:49:32<30:29, 79.55s/it] 93%|█████████▎| 299/321 [8:51:12<30:51, 84.15s/it] 93%|█████████▎| 300/321 [8:53:14<32:36, 93.17s/it]get video length sec error for grp-cfe4d1f3-20c1-45d9-8bd3-5a7a8e7443f4-360-720.mp4, skipping
get video length sec error for grp-cfe4d1f3-20c1-45d9-8bd3-5a7a8e7443f4-720-1080.mp4, skipping
 94%|█████████▍| 303/321 [8:55:11<19:42, 65.68s/it] 95%|█████████▍| 304/321 [8:56:39<19:51, 70.07s/it] 95%|█████████▌| 305/321 [8:58:59<22:46, 85.38s/it] 95%|█████████▌| 306/321 [9:02:21<28:15, 113.03s/it]get video length sec error for grp-0ee8d0ef-c5ba-42c5-9c40-8718faa081ec-0-360.mp4, skipping
get video length sec error for grp-0ee8d0ef-c5ba-42c5-9c40-8718faa081ec-720-1080.mp4, skipping
get video length sec error for grp-0ee8d0ef-c5ba-42c5-9c40-8718faa081ec-2160-2520.mp4, skipping
get video length sec error for grp-a8ce8831-58e4-4c84-926e-8f948fc782a7-0-360.mp4, skipping
get video length sec error for grp-a9c519a7-4776-42d6-bcf1-270f0d302843-0-360.mp4, skipping
get video length sec error for grp-a9c519a7-4776-42d6-bcf1-270f0d302843-360-720.mp4, skipping
get video length sec error for grp-a9c519a7-4776-42d6-bcf1-270f0d302843-720-1080.mp4, skipping
 98%|█████████▊| 314/321 [9:03:22<04:12, 36.02s/it]  98%|█████████▊| 315/321 [9:07:10<05:48, 58.15s/it] 98%|█████████▊| 316/321 [9:10:28<06:29, 77.93s/it] 99%|█████████▉| 317/321 [9:13:57<06:40, 100.02s/it] 99%|█████████▉| 318/321 [9:17:01<05:48, 116.32s/it] 99%|█████████▉| 319/321 [9:20:14<04:25, 132.85s/it]100%|█████████▉| 320/321 [9:23:58<02:34, 154.42s/it]100%|██████████| 321/321 [9:26:23<00:00, 152.12s/it]100%|██████████| 321/321 [9:26:23<00:00, 105.87s/it]
