/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
ClipTestArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
attn_implementation=flash_attention_2,
augmentation=False,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
consecutive_n_frames_threshold=1,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
dataset_config=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
embed_mark=2fps_384_1+3x3,
end_idx=500,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=no,
eval_use_gather_object=False,
evaluation_strategy=None,
finetune_modules=['connector', 'mm_projector', 'response_head', 'related_head'],
first_n_frames_no_generate=0,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
frame_fps=1.0,
frame_num_tokens=49,
frame_resolution=384,
frame_token_cls=False,
frame_token_pooled=[7, 7],
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
grounding_mode=False,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
input_dir=/share3/public_share/TVQA/video/videos_val,
is_online_model=True,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
live_version=test,
llm_pretrained=lmms-lab/llava-onevision-qwen2-7b-ov,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=outputs/debug/runs/Apr30_10-29-49_zkti,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lora_alpha=32,
lora_modules=model\.layers.*(q_proj|k_proj|v_proj|o_proj|gate_proj|up_proj|down_proj)$,
lora_pretrained=None,
lora_r=16,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_num_frames=100,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
no_output_before_user_input=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=outputs/debug,
output_fname=outputs/llava-ov/tvqa/incremental/5sec.jsonl.0,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_assistant_turns=False,
remove_unused_columns=True,
repetition_penalty=None,
report_to=['tensorboard', 'wandb'],
response_min_interval_frames=None,
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=outputs/debug,
running_list_length=20,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
score_heads=informative_score,
seed=42,
skip_memory_metrics=True,
split_batches=None,
start_idx=0,
stream_end_prob_threshold=None,
stream_end_score_sum_threshold=None,
stream_loss_weight=1.0,
system_prompt=A multimodal AI assistant is helping users with some activities. Below is their conversation, interleaved with the list of video frames received by the assistant.,
test_fname=../judge_questions/outputs/tvqa/tvqa-val-online-1question-open_ended-long_answers-with_checked_judge_questions-1000_videos-with_sub.json,
tf32=None,
threshold_z=None,
time_instruction_format=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
v_placeholder=<image>,
video_chunk_sec=5,
video_pooling_stride=4,
vision_pretrained=google/siglip-large-patch16-384,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Loaded LLaVA model: lmms-lab/llava-onevision-qwen2-7b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
Loading vision tower: google/siglip-so400m-patch14-384
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.patch_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.patch_embedding.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.position_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.probe: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.attention.in_proj_weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.attention.in_proj_bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.attention.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.attention.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|       | 1/4 [00:43<02:10, 43.55s/it]Loading checkpoint shards:  50%|     | 2/4 [01:27<01:27, 43.95s/it]Loading checkpoint shards:  75%|  | 3/4 [02:13<00:44, 44.59s/it]Loading checkpoint shards: 100%|| 4/4 [02:24<00:00, 31.52s/it]Loading checkpoint shards: 100%|| 4/4 [02:24<00:00, 36.15s/it]
Model Class: LlavaQwenForCausalLM
  0%|          | 0/500 [00:00<?, ?it/s]/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:612: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
question: <image>
Subtitle: See it? "The common field cricket, a.k.a. Gryllus assimilis"...
Subtitle: ...which is Latin for "suck it, you lose."

What did Howard say Gryllus assimilis stood for in Latin when he was showing Sheldon a book?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: See it? "The common field cricket, a.k.a. Gryllus assimilis"...
Subtitle: ...which is Latin for "suck it, you lose."

What did Howard say Gryllus assimilis stood for in Latin when he was showing Sheldon a book?<|im_end|>
<|im_start|>assistant

text_outputs: Howard said Gryllus assimilis stood for 'suck it, you lose' in Latin.
question: <image>
Subtitle: See it? "The common field cricket, a.k.a. Gryllus assimilis"...
Subtitle: ...which is Latin for "suck it, you lose."
Subtitle: (Sheldon:)Hang on.
Subtitle: Voil. "The snowy tree cricket, a.k.a. Oecanthus fultoni"...

What did Howard say Gryllus assimilis stood for in Latin when he was showing Sheldon a book?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: See it? "The common field cricket, a.k.a. Gryllus assimilis"...
Subtitle: ...which is Latin for "suck it, you lose."
Subtitle: (Sheldon:)Hang on.
Subtitle: Voil. "The snowy tree cricket, a.k.a. Oecanthus fultoni"...

What did Howard say Gryllus assimilis stood for in Latin when he was showing Sheldon a book?<|im_end|>
<|im_start|>assistant
Howard said Gryllus assimilis stood for 'suck it, you lose' in Latin.

text_outputs:  
question: <image>
Subtitle: See it? "The common field cricket, a.k.a. Gryllus assimilis"...
Subtitle: ...which is Latin for "suck it, you lose."
Subtitle: (Sheldon:)Hang on.
Subtitle: Voil. "The snowy tree cricket, a.k.a. Oecanthus fultoni"...
Subtitle: ...which is Latin for "I'll suck nothing."

What did Howard say Gryllus assimilis stood for in Latin when he was showing Sheldon a book?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: See it? "The common field cricket, a.k.a. Gryllus assimilis"...
Subtitle: ...which is Latin for "suck it, you lose."
Subtitle: (Sheldon:)Hang on.
Subtitle: Voil. "The snowy tree cricket, a.k.a. Oecanthus fultoni"...
Subtitle: ...which is Latin for "I'll suck nothing."

What did Howard say Gryllus assimilis stood for in Latin when he was showing Sheldon a book?<|im_end|>
<|im_start|>assistant
Howard said Gryllus assimilis stood for 'suck it, you lose' in Latin. 

text_outputs: (Note: The original subtitle was incorrect. The correct translation of Gryllus assimilis in Latin is 'suck it, you lose', not 'hang on'.)
  0%|          | 1/500 [00:06<54:21,  6.54s/it]question: <image>
Subtitle: See it? "The common field cricket, a.k.a. Gryllus assimilis"...
Subtitle: ...which is Latin for "suck it, you lose."

What was Raj doing with his food when Sheldon asked him for his opinion on his argument with Howard?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: See it? "The common field cricket, a.k.a. Gryllus assimilis"...
Subtitle: ...which is Latin for "suck it, you lose."

What was Raj doing with his food when Sheldon asked him for his opinion on his argument with Howard?<|im_end|>
<|im_start|>assistant

text_outputs: Raj was eating a sandwich.
question: <image>
Subtitle: See it? "The common field cricket, a.k.a. Gryllus assimilis"...
Subtitle: ...which is Latin for "suck it, you lose."
Subtitle: (Sheldon:)Hang on.
Subtitle: Voil. "The snowy tree cricket, a.k.a. Oecanthus fultoni"...

What was Raj doing with his food when Sheldon asked him for his opinion on his argument with Howard?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: See it? "The common field cricket, a.k.a. Gryllus assimilis"...
Subtitle: ...which is Latin for "suck it, you lose."
Subtitle: (Sheldon:)Hang on.
Subtitle: Voil. "The snowy tree cricket, a.k.a. Oecanthus fultoni"...

What was Raj doing with his food when Sheldon asked him for his opinion on his argument with Howard?<|im_end|>
<|im_start|>assistant
Raj was eating a sandwich.

text_outputs:  
question: <image>
Subtitle: See it? "The common field cricket, a.k.a. Gryllus assimilis"...
Subtitle: ...which is Latin for "suck it, you lose."
Subtitle: (Sheldon:)Hang on.
Subtitle: Voil. "The snowy tree cricket, a.k.a. Oecanthus fultoni"...
Subtitle: ...which is Latin for "I'll suck nothing."

What was Raj doing with his food when Sheldon asked him for his opinion on his argument with Howard?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: See it? "The common field cricket, a.k.a. Gryllus assimilis"...
Subtitle: ...which is Latin for "suck it, you lose."
Subtitle: (Sheldon:)Hang on.
Subtitle: Voil. "The snowy tree cricket, a.k.a. Oecanthus fultoni"...
Subtitle: ...which is Latin for "I'll suck nothing."

What was Raj doing with his food when Sheldon asked him for his opinion on his argument with Howard?<|im_end|>
<|im_start|>assistant
Raj was eating a sandwich. 

text_outputs: Raj was eating a sandwich.
question: <image>
Subtitle: See it? "The common field cricket, a.k.a. Gryllus assimilis"...
Subtitle: ...which is Latin for "suck it, you lose."
Subtitle: (Sheldon:)Hang on.
Subtitle: Voil. "The snowy tree cricket, a.k.a. Oecanthus fultoni"...
Subtitle: ...which is Latin for "I'll suck nothing."
Subtitle: (Sheldon:)Of course, I'm joking, because the Latin for that is "nihil exsorbibo."
Subtitle: (Howard:)That is not Toby, this is Toby.

What was Raj doing with his food when Sheldon asked him for his opinion on his argument with Howard?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: See it? "The common field cricket, a.k.a. Gryllus assimilis"...
Subtitle: ...which is Latin for "suck it, you lose."
Subtitle: (Sheldon:)Hang on.
Subtitle: Voil. "The snowy tree cricket, a.k.a. Oecanthus fultoni"...
Subtitle: ...which is Latin for "I'll suck nothing."
Subtitle: (Sheldon:)Of course, I'm joking, because the Latin for that is "nihil exsorbibo."
Subtitle: (Howard:)That is not Toby, this is Toby.

What was Raj doing with his food when Sheldon asked him for his opinion on his argument with Howard?<|im_end|>
<|im_start|>assistant
Raj was eating a sandwich. Raj was eating a sandwich.

text_outputs: Raj was eating a sandwich.
question: <image>
Subtitle: See it? "The common field cricket, a.k.a. Gryllus assimilis"...
Subtitle: ...which is Latin for "suck it, you lose."
Subtitle: (Sheldon:)Hang on.
Subtitle: Voil. "The snowy tree cricket, a.k.a. Oecanthus fultoni"...
Subtitle: ...which is Latin for "I'll suck nothing."
Subtitle: (Sheldon:)Of course, I'm joking, because the Latin for that is "nihil exsorbibo."
Subtitle: (Howard:)That is not Toby, this is Toby.
Subtitle: (Howard:)Raj, what do you think?

What was Raj doing with his food when Sheldon asked him for his opinion on his argument with Howard?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: See it? "The common field cricket, a.k.a. Gryllus assimilis"...
Subtitle: ...which is Latin for "suck it, you lose."
Subtitle: (Sheldon:)Hang on.
Subtitle: Voil. "The snowy tree cricket, a.k.a. Oecanthus fultoni"...
Subtitle: ...which is Latin for "I'll suck nothing."
Subtitle: (Sheldon:)Of course, I'm joking, because the Latin for that is "nihil exsorbibo."
Subtitle: (Howard:)That is not Toby, this is Toby.
Subtitle: (Howard:)Raj, what do you think?

What was Raj doing with his food when Sheldon asked him for his opinion on his argument with Howard?<|im_end|>
<|im_start|>assistant
Raj was eating a sandwich. Raj was eating a sandwich.Raj was eating a sandwich.

text_outputs: Raj was eating a sandwich.
question: <image>
Subtitle: See it? "The common field cricket, a.k.a. Gryllus assimilis"...
Subtitle: ...which is Latin for "suck it, you lose."
Subtitle: (Sheldon:)Hang on.
Subtitle: Voil. "The snowy tree cricket, a.k.a. Oecanthus fultoni"...
Subtitle: ...which is Latin for "I'll suck nothing."
Subtitle: (Sheldon:)Of course, I'm joking, because the Latin for that is "nihil exsorbibo."
Subtitle: (Howard:)That is not Toby, this is Toby.
Subtitle: (Howard:)Raj, what do you think?
Subtitle: (Raj:)Oh. I really don't care anymore.
Subtitle: [LEONARD SIGHS]
Subtitle: (Leonard:)-God, I had the most horrible night. -What happened?

What was Raj doing with his food when Sheldon asked him for his opinion on his argument with Howard?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: See it? "The common field cricket, a.k.a. Gryllus assimilis"...
Subtitle: ...which is Latin for "suck it, you lose."
Subtitle: (Sheldon:)Hang on.
Subtitle: Voil. "The snowy tree cricket, a.k.a. Oecanthus fultoni"...
Subtitle: ...which is Latin for "I'll suck nothing."
Subtitle: (Sheldon:)Of course, I'm joking, because the Latin for that is "nihil exsorbibo."
Subtitle: (Howard:)That is not Toby, this is Toby.
Subtitle: (Howard:)Raj, what do you think?
Subtitle: (Raj:)Oh. I really don't care anymore.
Subtitle: [LEONARD SIGHS]
Subtitle: (Leonard:)-God, I had the most horrible night. -What happened?

What was Raj doing with his food when Sheldon asked him for his opinion on his argument with Howard?<|im_end|>
<|im_start|>assistant
Raj was eating a sandwich. Raj was eating a sandwich.Raj was eating a sandwich.Raj was eating a sandwich.

text_outputs: Raj was eating a sandwich.
question: <image>
Subtitle: See it? "The common field cricket, a.k.a. Gryllus assimilis"...
Subtitle: ...which is Latin for "suck it, you lose."
Subtitle: (Sheldon:)Hang on.
Subtitle: Voil. "The snowy tree cricket, a.k.a. Oecanthus fultoni"...
Subtitle: ...which is Latin for "I'll suck nothing."
Subtitle: (Sheldon:)Of course, I'm joking, because the Latin for that is "nihil exsorbibo."
Subtitle: (Howard:)That is not Toby, this is Toby.
Subtitle: (Howard:)Raj, what do you think?
Subtitle: (Raj:)Oh. I really don't care anymore.
Subtitle: [LEONARD SIGHS]
Subtitle: (Leonard:)-God, I had the most horrible night. -What happened?
Subtitle: (Sheldon:)Obviously another carnal fiasco with the "shiksi" goddess.

What was Raj doing with his food when Sheldon asked him for his opinion on his argument with Howard?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: See it? "The common field cricket, a.k.a. Gryllus assimilis"...
Subtitle: ...which is Latin for "suck it, you lose."
Subtitle: (Sheldon:)Hang on.
Subtitle: Voil. "The snowy tree cricket, a.k.a. Oecanthus fultoni"...
Subtitle: ...which is Latin for "I'll suck nothing."
Subtitle: (Sheldon:)Of course, I'm joking, because the Latin for that is "nihil exsorbibo."
Subtitle: (Howard:)That is not Toby, this is Toby.
Subtitle: (Howard:)Raj, what do you think?
Subtitle: (Raj:)Oh. I really don't care anymore.
Subtitle: [LEONARD SIGHS]
Subtitle: (Leonard:)-God, I had the most horrible night. -What happened?
Subtitle: (Sheldon:)Obviously another carnal fiasco with the "shiksi" goddess.

What was Raj doing with his food when Sheldon asked him for his opinion on his argument with Howard?<|im_end|>
<|im_start|>assistant
Raj was eating a sandwich. Raj was eating a sandwich.Raj was eating a sandwich.Raj was eating a sandwich.Raj was eating a sandwich.

text_outputs: Raj was eating a sandwich.
  0%|          | 2/500 [00:15<1:06:39,  8.03s/it]question: <image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.

What did Robin do after Simon called her and told her to wait?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.

What did Robin do after Simon called her and told her to wait?<|im_end|>
<|im_start|>assistant

text_outputs: Robin turned around and walked away.
question: <image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.

What did Robin do after Simon called her and told her to wait?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.

What did Robin do after Simon called her and told her to wait?<|im_end|>
<|im_start|>assistant
Robin turned around and walked away.

text_outputs:  
question: <image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.
Subtitle: (Robin:)Swimming's awesome.
Subtitle: (Robin:)We have a sprinkler.

What did Robin do after Simon called her and told her to wait?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.
Subtitle: (Robin:)Swimming's awesome.
Subtitle: (Robin:)We have a sprinkler.

What did Robin do after Simon called her and told her to wait?<|im_end|>
<|im_start|>assistant
Robin turned around and walked away. 

text_outputs: After turning around and walking away, Robin returned to the scene.
question: <image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.
Subtitle: (Robin:)Swimming's awesome.
Subtitle: (Robin:)We have a sprinkler.
Subtitle: (Robin:)Run right through it.
Subtitle: (Robin:)Well, I should go.

What did Robin do after Simon called her and told her to wait?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.
Subtitle: (Robin:)Swimming's awesome.
Subtitle: (Robin:)We have a sprinkler.
Subtitle: (Robin:)Run right through it.
Subtitle: (Robin:)Well, I should go.

What did Robin do after Simon called her and told her to wait?<|im_end|>
<|im_start|>assistant
Robin turned around and walked away. After turning around and walking away, Robin returned to the scene.

text_outputs: After returning to the scene, Robin continued her conversation with Simon.
question: <image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.
Subtitle: (Robin:)Swimming's awesome.
Subtitle: (Robin:)We have a sprinkler.
Subtitle: (Robin:)Run right through it.
Subtitle: (Robin:)Well, I should go.
Subtitle: (Simon:)Uh...
Subtitle: (Simon:)Babe, wait.

What did Robin do after Simon called her and told her to wait?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.
Subtitle: (Robin:)Swimming's awesome.
Subtitle: (Robin:)We have a sprinkler.
Subtitle: (Robin:)Run right through it.
Subtitle: (Robin:)Well, I should go.
Subtitle: (Simon:)Uh...
Subtitle: (Simon:)Babe, wait.

What did Robin do after Simon called her and told her to wait?<|im_end|>
<|im_start|>assistant
Robin turned around and walked away. After turning around and walking away, Robin returned to the scene.After returning to the scene, Robin continued her conversation with Simon.

text_outputs: 
question: <image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.
Subtitle: (Robin:)Swimming's awesome.
Subtitle: (Robin:)We have a sprinkler.
Subtitle: (Robin:)Run right through it.
Subtitle: (Robin:)Well, I should go.
Subtitle: (Simon:)Uh...
Subtitle: (Simon:)Babe, wait.
Subtitle: (Robin:)Yeah, simon?
Subtitle: (Simon:)You forgot to load the drum set.

What did Robin do after Simon called her and told her to wait?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.
Subtitle: (Robin:)Swimming's awesome.
Subtitle: (Robin:)We have a sprinkler.
Subtitle: (Robin:)Run right through it.
Subtitle: (Robin:)Well, I should go.
Subtitle: (Simon:)Uh...
Subtitle: (Simon:)Babe, wait.
Subtitle: (Robin:)Yeah, simon?
Subtitle: (Simon:)You forgot to load the drum set.

What did Robin do after Simon called her and told her to wait?<|im_end|>
<|im_start|>assistant
Robin turned around and walked away. After turning around and walking away, Robin returned to the scene.After returning to the scene, Robin continued her conversation with Simon.

text_outputs: Robin turned around and walked away.
question: <image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.
Subtitle: (Robin:)Swimming's awesome.
Subtitle: (Robin:)We have a sprinkler.
Subtitle: (Robin:)Run right through it.
Subtitle: (Robin:)Well, I should go.
Subtitle: (Simon:)Uh...
Subtitle: (Simon:)Babe, wait.
Subtitle: (Robin:)Yeah, simon?
Subtitle: (Simon:)You forgot to load the drum set.
Subtitle: (Ted:)No!
Subtitle: (Ted:)Tell me you did not load the drum set.

What did Robin do after Simon called her and told her to wait?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.
Subtitle: (Robin:)Swimming's awesome.
Subtitle: (Robin:)We have a sprinkler.
Subtitle: (Robin:)Run right through it.
Subtitle: (Robin:)Well, I should go.
Subtitle: (Simon:)Uh...
Subtitle: (Simon:)Babe, wait.
Subtitle: (Robin:)Yeah, simon?
Subtitle: (Simon:)You forgot to load the drum set.
Subtitle: (Ted:)No!
Subtitle: (Ted:)Tell me you did not load the drum set.

What did Robin do after Simon called her and told her to wait?<|im_end|>
<|im_start|>assistant
Robin turned around and walked away. After turning around and walking away, Robin returned to the scene.After returning to the scene, Robin continued her conversation with Simon.Robin turned around and walked away.

text_outputs: After returning to the scene, Robin continued her conversation with Simon.
  1%|          | 3/500 [00:31<1:35:20, 11.51s/it]question: <image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.

What was Ted doing when Lily explained why Robin wanted to meet the guy?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.

What was Ted doing when Lily explained why Robin wanted to meet the guy?<|im_end|>
<|im_start|>assistant

text_outputs: Ted was listening attentively.
question: <image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.

What was Ted doing when Lily explained why Robin wanted to meet the guy?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.

What was Ted doing when Lily explained why Robin wanted to meet the guy?<|im_end|>
<|im_start|>assistant
Ted was listening attentively.

text_outputs:  
question: <image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.
Subtitle: (Robin:)Swimming's awesome.
Subtitle: (Robin:)We have a sprinkler.

What was Ted doing when Lily explained why Robin wanted to meet the guy?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.
Subtitle: (Robin:)Swimming's awesome.
Subtitle: (Robin:)We have a sprinkler.

What was Ted doing when Lily explained why Robin wanted to meet the guy?<|im_end|>
<|im_start|>assistant
Ted was listening attentively. 

text_outputs: Ted was listening attentively.
question: <image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.
Subtitle: (Robin:)Swimming's awesome.
Subtitle: (Robin:)We have a sprinkler.
Subtitle: (Robin:)Run right through it.
Subtitle: (Robin:)Well, I should go.

What was Ted doing when Lily explained why Robin wanted to meet the guy?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.
Subtitle: (Robin:)Swimming's awesome.
Subtitle: (Robin:)We have a sprinkler.
Subtitle: (Robin:)Run right through it.
Subtitle: (Robin:)Well, I should go.

What was Ted doing when Lily explained why Robin wanted to meet the guy?<|im_end|>
<|im_start|>assistant
Ted was listening attentively. Ted was listening attentively.

text_outputs: 
question: <image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.
Subtitle: (Robin:)Swimming's awesome.
Subtitle: (Robin:)We have a sprinkler.
Subtitle: (Robin:)Run right through it.
Subtitle: (Robin:)Well, I should go.
Subtitle: (Simon:)Uh...
Subtitle: (Simon:)Babe, wait.

What was Ted doing when Lily explained why Robin wanted to meet the guy?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.
Subtitle: (Robin:)Swimming's awesome.
Subtitle: (Robin:)We have a sprinkler.
Subtitle: (Robin:)Run right through it.
Subtitle: (Robin:)Well, I should go.
Subtitle: (Simon:)Uh...
Subtitle: (Simon:)Babe, wait.

What was Ted doing when Lily explained why Robin wanted to meet the guy?<|im_end|>
<|im_start|>assistant
Ted was listening attentively. Ted was listening attentively.

text_outputs: 
question: <image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.
Subtitle: (Robin:)Swimming's awesome.
Subtitle: (Robin:)We have a sprinkler.
Subtitle: (Robin:)Run right through it.
Subtitle: (Robin:)Well, I should go.
Subtitle: (Simon:)Uh...
Subtitle: (Simon:)Babe, wait.
Subtitle: (Robin:)Yeah, simon?
Subtitle: (Simon:)You forgot to load the drum set.

What was Ted doing when Lily explained why Robin wanted to meet the guy?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.
Subtitle: (Robin:)Swimming's awesome.
Subtitle: (Robin:)We have a sprinkler.
Subtitle: (Robin:)Run right through it.
Subtitle: (Robin:)Well, I should go.
Subtitle: (Simon:)Uh...
Subtitle: (Simon:)Babe, wait.
Subtitle: (Robin:)Yeah, simon?
Subtitle: (Simon:)You forgot to load the drum set.

What was Ted doing when Lily explained why Robin wanted to meet the guy?<|im_end|>
<|im_start|>assistant
Ted was listening attentively. Ted was listening attentively.

text_outputs: 
question: <image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.
Subtitle: (Robin:)Swimming's awesome.
Subtitle: (Robin:)We have a sprinkler.
Subtitle: (Robin:)Run right through it.
Subtitle: (Robin:)Well, I should go.
Subtitle: (Simon:)Uh...
Subtitle: (Simon:)Babe, wait.
Subtitle: (Robin:)Yeah, simon?
Subtitle: (Simon:)You forgot to load the drum set.
Subtitle: (Ted:)No!
Subtitle: (Ted:)Tell me you did not load the drum set.

What was Ted doing when Lily explained why Robin wanted to meet the guy?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.
Subtitle: (Robin:)Swimming's awesome.
Subtitle: (Robin:)We have a sprinkler.
Subtitle: (Robin:)Run right through it.
Subtitle: (Robin:)Well, I should go.
Subtitle: (Simon:)Uh...
Subtitle: (Simon:)Babe, wait.
Subtitle: (Robin:)Yeah, simon?
Subtitle: (Simon:)You forgot to load the drum set.
Subtitle: (Ted:)No!
Subtitle: (Ted:)Tell me you did not load the drum set.

What was Ted doing when Lily explained why Robin wanted to meet the guy?<|im_end|>
<|im_start|>assistant
Ted was listening attentively. Ted was listening attentively.

text_outputs: 
question: <image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.
Subtitle: (Robin:)Swimming's awesome.
Subtitle: (Robin:)We have a sprinkler.
Subtitle: (Robin:)Run right through it.
Subtitle: (Robin:)Well, I should go.
Subtitle: (Simon:)Uh...
Subtitle: (Simon:)Babe, wait.
Subtitle: (Robin:)Yeah, simon?
Subtitle: (Simon:)You forgot to load the drum set.
Subtitle: (Ted:)No!
Subtitle: (Ted:)Tell me you did not load the drum set.
Subtitle: (Robin:)I did.
Subtitle: (Marshall:)And now he's coming here?

What was Ted doing when Lily explained why Robin wanted to meet the guy?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.
Subtitle: (Robin:)Swimming's awesome.
Subtitle: (Robin:)We have a sprinkler.
Subtitle: (Robin:)Run right through it.
Subtitle: (Robin:)Well, I should go.
Subtitle: (Simon:)Uh...
Subtitle: (Simon:)Babe, wait.
Subtitle: (Robin:)Yeah, simon?
Subtitle: (Simon:)You forgot to load the drum set.
Subtitle: (Ted:)No!
Subtitle: (Ted:)Tell me you did not load the drum set.
Subtitle: (Robin:)I did.
Subtitle: (Marshall:)And now he's coming here?

What was Ted doing when Lily explained why Robin wanted to meet the guy?<|im_end|>
<|im_start|>assistant
Ted was listening attentively. Ted was listening attentively.

text_outputs: 
question: <image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.
Subtitle: (Robin:)Swimming's awesome.
Subtitle: (Robin:)We have a sprinkler.
Subtitle: (Robin:)Run right through it.
Subtitle: (Robin:)Well, I should go.
Subtitle: (Simon:)Uh...
Subtitle: (Simon:)Babe, wait.
Subtitle: (Robin:)Yeah, simon?
Subtitle: (Simon:)You forgot to load the drum set.
Subtitle: (Ted:)No!
Subtitle: (Ted:)Tell me you did not load the drum set.
Subtitle: (Robin:)I did.
Subtitle: (Marshall:)And now he's coming here?
Subtitle: (Marshall:)Robin, why do you want to see this guy?
Subtitle: (Lily:)Oh! I know why.

What was Ted doing when Lily explained why Robin wanted to meet the guy?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.
Subtitle: (Robin:)Swimming's awesome.
Subtitle: (Robin:)We have a sprinkler.
Subtitle: (Robin:)Run right through it.
Subtitle: (Robin:)Well, I should go.
Subtitle: (Simon:)Uh...
Subtitle: (Simon:)Babe, wait.
Subtitle: (Robin:)Yeah, simon?
Subtitle: (Simon:)You forgot to load the drum set.
Subtitle: (Ted:)No!
Subtitle: (Ted:)Tell me you did not load the drum set.
Subtitle: (Robin:)I did.
Subtitle: (Marshall:)And now he's coming here?
Subtitle: (Marshall:)Robin, why do you want to see this guy?
Subtitle: (Lily:)Oh! I know why.

What was Ted doing when Lily explained why Robin wanted to meet the guy?<|im_end|>
<|im_start|>assistant
Ted was listening attentively. Ted was listening attentively.

text_outputs: 
question: <image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.
Subtitle: (Robin:)Swimming's awesome.
Subtitle: (Robin:)We have a sprinkler.
Subtitle: (Robin:)Run right through it.
Subtitle: (Robin:)Well, I should go.
Subtitle: (Simon:)Uh...
Subtitle: (Simon:)Babe, wait.
Subtitle: (Robin:)Yeah, simon?
Subtitle: (Simon:)You forgot to load the drum set.
Subtitle: (Ted:)No!
Subtitle: (Ted:)Tell me you did not load the drum set.
Subtitle: (Robin:)I did.
Subtitle: (Marshall:)And now he's coming here?
Subtitle: (Marshall:)Robin, why do you want to see this guy?
Subtitle: (Lily:)Oh! I know why.
Subtitle: (Lily:)You're gonna win, aren't you?
Subtitle: (Robin:)I have no idea what you're talking about.
Subtitle: (Ted:)Yes, you do.

What was Ted doing when Lily explained why Robin wanted to meet the guy?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.
Subtitle: (Robin:)Swimming's awesome.
Subtitle: (Robin:)We have a sprinkler.
Subtitle: (Robin:)Run right through it.
Subtitle: (Robin:)Well, I should go.
Subtitle: (Simon:)Uh...
Subtitle: (Simon:)Babe, wait.
Subtitle: (Robin:)Yeah, simon?
Subtitle: (Simon:)You forgot to load the drum set.
Subtitle: (Ted:)No!
Subtitle: (Ted:)Tell me you did not load the drum set.
Subtitle: (Robin:)I did.
Subtitle: (Marshall:)And now he's coming here?
Subtitle: (Marshall:)Robin, why do you want to see this guy?
Subtitle: (Lily:)Oh! I know why.
Subtitle: (Lily:)You're gonna win, aren't you?
Subtitle: (Robin:)I have no idea what you're talking about.
Subtitle: (Ted:)Yes, you do.

What was Ted doing when Lily explained why Robin wanted to meet the guy?<|im_end|>
<|im_start|>assistant
Ted was listening attentively. Ted was listening attentively.

text_outputs: 
question: <image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.
Subtitle: (Robin:)Swimming's awesome.
Subtitle: (Robin:)We have a sprinkler.
Subtitle: (Robin:)Run right through it.
Subtitle: (Robin:)Well, I should go.
Subtitle: (Simon:)Uh...
Subtitle: (Simon:)Babe, wait.
Subtitle: (Robin:)Yeah, simon?
Subtitle: (Simon:)You forgot to load the drum set.
Subtitle: (Ted:)No!
Subtitle: (Ted:)Tell me you did not load the drum set.
Subtitle: (Robin:)I did.
Subtitle: (Marshall:)And now he's coming here?
Subtitle: (Marshall:)Robin, why do you want to see this guy?
Subtitle: (Lily:)Oh! I know why.
Subtitle: (Lily:)You're gonna win, aren't you?
Subtitle: (Robin:)I have no idea what you're talking about.
Subtitle: (Ted:)Yes, you do.
Subtitle: (Ted:)Whenever you haven't seen someone for a long time,
Subtitle: (Ted:)No matter how much you want to deny it,
Subtitle: (Ted:)There's always a clear winner and a clear loser.

What was Ted doing when Lily explained why Robin wanted to meet the guy?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: (Simon:)Well, the thing is...
Subtitle: (Simon:)Her folks just put in a pool.
Subtitle: (Robin:)Oh... So...
Subtitle: (Robin:)Totally... I get it.
Subtitle: (Robin:)Pools are great.
Subtitle: (Robin:)Swimming's awesome.
Subtitle: (Robin:)We have a sprinkler.
Subtitle: (Robin:)Run right through it.
Subtitle: (Robin:)Well, I should go.
Subtitle: (Simon:)Uh...
Subtitle: (Simon:)Babe, wait.
Subtitle: (Robin:)Yeah, simon?
Subtitle: (Simon:)You forgot to load the drum set.
Subtitle: (Ted:)No!
Subtitle: (Ted:)Tell me you did not load the drum set.
Subtitle: (Robin:)I did.
Subtitle: (Marshall:)And now he's coming here?
Subtitle: (Marshall:)Robin, why do you want to see this guy?
Subtitle: (Lily:)Oh! I know why.
Subtitle: (Lily:)You're gonna win, aren't you?
Subtitle: (Robin:)I have no idea what you're talking about.
Subtitle: (Ted:)Yes, you do.
Subtitle: (Ted:)Whenever you haven't seen someone for a long time,
Subtitle: (Ted:)No matter how much you want to deny it,
Subtitle: (Ted:)There's always a clear winner and a clear loser.

What was Ted doing when Lily explained why Robin wanted to meet the guy?<|im_end|>
<|im_start|>assistant
Ted was listening attentively. Ted was listening attentively.

text_outputs: 
  1%|          | 4/500 [01:00<2:33:54, 18.62s/it]question: <image>
Subtitle: (Castle:)The killer took it.
Subtitle: (Ryan:)Why? I mean it doesn't look like it was worth very much.
Subtitle: (Ryan:)If it was about money, how come he didn't take her purse?

What did Ryan took from Beckett hands when he was talking to Castle?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: (Castle:)The killer took it.
Subtitle: (Ryan:)Why? I mean it doesn't look like it was worth very much.
Subtitle: (Ryan:)If it was about money, how come he didn't take her purse?

What did Ryan took from Beckett hands when he was talking to Castle?<|im_end|>
<|im_start|>assistant

text_outputs: Ryan took a piece of paper from Beckett's hands.
question: <image>
Subtitle: (Castle:)The killer took it.
Subtitle: (Ryan:)Why? I mean it doesn't look like it was worth very much.
Subtitle: (Ryan:)If it was about money, how come he didn't take her purse?
Subtitle: (Castle:)Because it wasn't about money. It was about passion.
Subtitle: (Castle:)Do you recognize that bracelet?

What did Ryan took from Beckett hands when he was talking to Castle?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: (Castle:)The killer took it.
Subtitle: (Ryan:)Why? I mean it doesn't look like it was worth very much.
Subtitle: (Ryan:)If it was about money, how come he didn't take her purse?
Subtitle: (Castle:)Because it wasn't about money. It was about passion.
Subtitle: (Castle:)Do you recognize that bracelet?

What did Ryan took from Beckett hands when he was talking to Castle?<|im_end|>
<|im_start|>assistant
Ryan took a piece of paper from Beckett's hands.

text_outputs:  
question: <image>
Subtitle: (Castle:)The killer took it.
Subtitle: (Ryan:)Why? I mean it doesn't look like it was worth very much.
Subtitle: (Ryan:)If it was about money, how come he didn't take her purse?
Subtitle: (Castle:)Because it wasn't about money. It was about passion.
Subtitle: (Castle:)Do you recognize that bracelet?
Subtitle: (Beckett:)I've seen that design before.

What did Ryan took from Beckett hands when he was talking to Castle?
model_input: <|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<image>
Subtitle: (Castle:)The killer took it.
Subtitle: (Ryan:)Why? I mean it doesn't look like it was worth very much.
Subtitle: (Ryan:)If it was about money, how come he didn't take her purse?
Subtitle: (Castle:)Because it wasn't about money. It was about passion.
Subtitle: (Castle:)Do you recognize that bracelet?
Subtitle: (Beckett:)I've seen that design before.

What did Ryan took from Beckett hands when he was talking to Castle?<|im_end|>
<|im_start|>assistant
Ryan took a piece of paper from Beckett's hands. 

text_outputs: Ryan took a piece of paper from Beckett's hands.
  1%|          | 5/500 [01:02<1:44:36, 12.68s/it]  1%|          | 6/500 [01:34<2:37:59, 19.19s/it]  1%|         | 7/500 [01:45<2:14:49, 16.41s/it]  2%|         | 8/500 [02:34<3:40:00, 26.83s/it]  2%|         | 9/500 [02:39<2:43:39, 20.00s/it]  2%|         | 10/500 [04:08<5:37:38, 41.34s/it]  2%|         | 11/500 [05:08<6:22:31, 46.94s/it]  2%|         | 12/500 [05:55<6:23:02, 47.09s/it]  3%|         | 13/500 [05:57<4:31:24, 33.44s/it]  3%|         | 14/500 [05:59<3:14:13, 23.98s/it]  3%|         | 15/500 [06:21<3:08:29, 23.32s/it]  3%|         | 16/500 [07:01<3:47:58, 28.26s/it]  3%|         | 17/500 [07:03<2:43:53, 20.36s/it]  4%|         | 18/500 [07:13<2:18:13, 17.21s/it]  4%|         | 19/500 [07:38<2:36:57, 19.58s/it]  4%|         | 20/500 [08:20<3:30:02, 26.26s/it]  4%|         | 21/500 [09:22<4:55:50, 37.06s/it]  4%|         | 22/500 [09:30<3:45:59, 28.37s/it]  5%|         | 23/500 [09:42<3:07:09, 23.54s/it]  5%|         | 24/500 [10:09<3:14:12, 24.48s/it]  5%|         | 25/500 [11:00<4:17:19, 32.50s/it]  5%|         | 26/500 [11:28<4:04:49, 30.99s/it]  5%|         | 27/500 [11:50<3:44:49, 28.52s/it]  6%|         | 28/500 [11:52<2:40:22, 20.39s/it]  6%|         | 29/500 [11:54<1:56:47, 14.88s/it]  6%|         | 30/500 [11:59<1:34:44, 12.10s/it]  6%|         | 31/500 [12:06<1:21:54, 10.48s/it]  6%|         | 32/500 [12:31<1:55:48, 14.85s/it]  7%|         | 33/500 [12:53<2:11:59, 16.96s/it]  7%|         | 34/500 [13:17<2:27:55, 19.05s/it]  7%|         | 35/500 [13:39<2:33:24, 19.79s/it]  7%|         | 36/500 [14:05<2:48:35, 21.80s/it]  7%|         | 37/500 [14:27<2:48:33, 21.84s/it]  8%|         | 38/500 [14:31<2:06:24, 16.42s/it]  8%|         | 39/500 [14:55<2:24:28, 18.80s/it]  8%|         | 40/500 [15:34<3:09:18, 24.69s/it]  8%|         | 41/500 [15:37<2:20:01, 18.30s/it]  8%|         | 42/500 [15:40<1:45:43, 13.85s/it]  9%|         | 43/500 [15:45<1:25:18, 11.20s/it]  9%|         | 44/500 [16:08<1:50:27, 14.53s/it]  9%|         | 45/500 [16:12<1:27:23, 11.53s/it]  9%|         | 46/500 [17:06<3:02:45, 24.15s/it]  9%|         | 47/500 [17:24<2:48:29, 22.32s/it] 10%|         | 48/500 [17:58<3:15:25, 25.94s/it] 10%|         | 49/500 [18:23<3:12:47, 25.65s/it] 10%|         | 50/500 [18:25<2:19:24, 18.59s/it] 10%|         | 51/500 [18:27<1:41:51, 13.61s/it] 10%|         | 52/500 [18:31<1:20:01, 10.72s/it] 11%|         | 53/500 [18:33<1:00:29,  8.12s/it] 11%|         | 54/500 [18:37<49:52,  6.71s/it]   11%|         | 55/500 [18:42<46:20,  6.25s/it] 11%|         | 56/500 [18:51<53:03,  7.17s/it] 11%|        | 57/500 [18:52<39:41,  5.38s/it] 12%|        | 58/500 [20:12<3:24:00, 27.69s/it] 12%|        | 59/500 [20:15<2:28:36, 20.22s/it] 12%|        | 60/500 [21:19<4:03:39, 33.23s/it] 12%|        | 61/500 [21:46<3:49:37, 31.38s/it] 12%|        | 62/500 [21:48<2:45:02, 22.61s/it] 13%|        | 63/500 [21:51<2:02:29, 16.82s/it] 13%|        | 64/500 [21:55<1:34:55, 13.06s/it] 13%|        | 65/500 [22:04<1:24:31, 11.66s/it] 13%|        | 66/500 [22:15<1:23:04, 11.49s/it] 13%|        | 67/500 [22:57<2:29:10, 20.67s/it] 14%|        | 68/500 [23:01<1:53:44, 15.80s/it] 14%|        | 69/500 [23:14<1:46:15, 14.79s/it] 14%|        | 70/500 [23:39<2:08:11, 17.89s/it] 14%|        | 71/500 [23:40<1:32:14, 12.90s/it] 14%|        | 72/500 [23:55<1:34:53, 13.30s/it] 15%|        | 73/500 [24:17<1:54:14, 16.05s/it] 15%|        | 74/500 [24:20<1:26:42, 12.21s/it] 15%|        | 75/500 [24:23<1:07:06,  9.47s/it] 15%|        | 76/500 [24:27<54:02,  7.65s/it]   15%|        | 77/500 [24:33<50:22,  7.15s/it] 16%|        | 78/500 [25:11<1:56:03, 16.50s/it] 16%|        | 79/500 [26:16<3:38:11, 31.10s/it] 16%|        | 80/500 [27:11<4:27:01, 38.15s/it] 16%|        | 81/500 [27:29<3:44:39, 32.17s/it] 16%|        | 82/500 [27:55<3:31:12, 30.32s/it] 17%|        | 83/500 [28:24<3:28:02, 29.93s/it] 17%|        | 84/500 [28:58<3:34:59, 31.01s/it] 17%|        | 85/500 [29:03<2:40:57, 23.27s/it] 17%|        | 86/500 [29:11<2:09:58, 18.84s/it] 17%|        | 87/500 [29:15<1:38:47, 14.35s/it] 18%|        | 88/500 [29:20<1:18:00, 11.36s/it] 18%|        | 89/500 [29:31<1:17:31, 11.32s/it] 18%|        | 90/500 [29:59<1:51:49, 16.37s/it] 18%|        | 91/500 [30:28<2:16:48, 20.07s/it] 18%|        | 92/500 [30:33<1:46:16, 15.63s/it] 19%|        | 93/500 [30:43<1:35:35, 14.09s/it] 19%|        | 94/500 [31:46<3:13:47, 28.64s/it] 19%|        | 95/500 [33:45<6:16:06, 55.72s/it] 19%|        | 96/500 [33:46<4:25:07, 39.37s/it] 19%|        | 97/500 [33:55<3:22:17, 30.12s/it] 20%|        | 98/500 [34:14<2:59:58, 26.86s/it] 20%|        | 99/500 [34:36<2:49:30, 25.36s/it] 20%|        | 100/500 [35:01<2:49:27, 25.42s/it] 20%|        | 101/500 [35:07<2:09:38, 19.50s/it] 20%|        | 102/500 [35:23<2:01:48, 18.36s/it] 21%|        | 103/500 [35:28<1:34:41, 14.31s/it] 21%|        | 104/500 [35:34<1:18:21, 11.87s/it] 21%|        | 105/500 [35:36<59:06,  8.98s/it]   21%|        | 106/500 [35:37<43:24,  6.61s/it] 21%|       | 107/500 [35:48<51:09,  7.81s/it] 22%|       | 108/500 [36:38<2:13:42, 20.47s/it] 22%|       | 109/500 [37:31<3:16:44, 30.19s/it] 22%|       | 110/500 [37:33<2:21:54, 21.83s/it] 22%|       | 111/500 [37:39<1:50:26, 17.03s/it] 22%|       | 112/500 [37:47<1:34:06, 14.55s/it] 23%|       | 113/500 [38:44<2:55:19, 27.18s/it] 23%|       | 114/500 [39:38<3:47:09, 35.31s/it] 23%|       | 115/500 [39:43<2:47:54, 26.17s/it] 23%|       | 116/500 [40:27<3:20:37, 31.35s/it] 23%|       | 117/500 [40:37<2:39:52, 25.05s/it] 24%|       | 118/500 [40:48<2:13:07, 20.91s/it] 24%|       | 119/500 [40:54<1:44:45, 16.50s/it] 24%|       | 120/500 [41:02<1:27:59, 13.89s/it] 24%|       | 121/500 [41:15<1:25:43, 13.57s/it] 24%|       | 122/500 [41:46<1:57:35, 18.66s/it] 25%|       | 123/500 [41:52<1:33:41, 14.91s/it] 25%|       | 124/500 [42:02<1:25:21, 13.62s/it] 25%|       | 125/500 [42:15<1:22:57, 13.27s/it] 25%|       | 126/500 [42:19<1:06:08, 10.61s/it] 25%|       | 127/500 [42:35<1:16:18, 12.28s/it] 26%|       | 128/500 [42:53<1:26:28, 13.95s/it] 26%|       | 129/500 [42:57<1:07:54, 10.98s/it] 26%|       | 130/500 [43:19<1:27:52, 14.25s/it] 26%|       | 131/500 [43:26<1:14:48, 12.16s/it] 26%|       | 132/500 [43:50<1:36:09, 15.68s/it] 27%|       | 133/500 [44:35<2:29:35, 24.46s/it] 27%|       | 134/500 [44:37<1:47:40, 17.65s/it] 27%|       | 135/500 [44:40<1:20:35, 13.25s/it] 27%|       | 136/500 [44:43<1:02:17, 10.27s/it] 27%|       | 137/500 [44:57<1:07:26, 11.15s/it] 28%|       | 138/500 [45:13<1:16:55, 12.75s/it] 28%|       | 139/500 [45:31<1:25:49, 14.26s/it] 28%|       | 140/500 [45:52<1:38:25, 16.40s/it] 28%|       | 141/500 [46:22<2:01:33, 20.32s/it] 28%|       | 142/500 [46:24<1:28:19, 14.80s/it] 29%|       | 143/500 [46:32<1:17:01, 12.95s/it] 29%|       | 144/500 [46:57<1:38:06, 16.53s/it] 29%|       | 145/500 [47:13<1:36:56, 16.39s/it] 29%|       | 146/500 [47:20<1:20:05, 13.57s/it] 29%|       | 147/500 [47:54<1:56:18, 19.77s/it] 30%|       | 148/500 [48:36<2:35:10, 26.45s/it] 30%|       | 149/500 [48:49<2:11:10, 22.42s/it] 30%|       | 150/500 [48:51<1:33:52, 16.09s/it] 30%|       | 151/500 [49:12<1:42:51, 17.68s/it] 30%|       | 152/500 [49:23<1:29:51, 15.49s/it] 31%|       | 153/500 [50:06<2:17:30, 23.78s/it] 31%|       | 154/500 [50:16<1:53:03, 19.60s/it] 31%|       | 155/500 [50:45<2:09:03, 22.44s/it] 31%|       | 156/500 [51:27<2:43:12, 28.47s/it] 31%|      | 157/500 [52:47<4:11:25, 43.98s/it] 32%|      | 158/500 [54:48<6:21:00, 66.84s/it] 32%|      | 159/500 [55:35<5:47:10, 61.09s/it] 32%|      | 160/500 [55:40<4:09:41, 44.06s/it] 32%|      | 161/500 [55:58<3:25:06, 36.30s/it] 32%|      | 162/500 [56:37<3:29:01, 37.11s/it] 33%|      | 163/500 [57:25<3:47:36, 40.52s/it] 33%|      | 164/500 [57:32<2:49:56, 30.35s/it] 33%|      | 165/500 [57:58<2:41:42, 28.96s/it] 33%|      | 166/500 [58:06<2:06:15, 22.68s/it] 33%|      | 167/500 [58:12<1:38:58, 17.83s/it] 34%|      | 168/500 [58:21<1:23:33, 15.10s/it] 34%|      | 169/500 [58:28<1:10:50, 12.84s/it] 34%|      | 170/500 [58:41<1:10:48, 12.87s/it] 34%|      | 171/500 [59:03<1:25:42, 15.63s/it] 34%|      | 172/500 [59:30<1:43:28, 18.93s/it] 35%|      | 173/500 [59:57<1:55:56, 21.27s/it] 35%|      | 174/500 [1:00:05<1:34:19, 17.36s/it] 35%|      | 175/500 [1:00:28<1:42:27, 18.92s/it] 35%|      | 176/500 [1:00:53<1:53:11, 20.96s/it] 35%|      | 177/500 [1:01:25<2:09:31, 24.06s/it] 36%|      | 178/500 [1:01:30<1:38:54, 18.43s/it] 36%|      | 179/500 [1:01:45<1:33:44, 17.52s/it] 36%|      | 180/500 [1:01:51<1:15:02, 14.07s/it] 36%|      | 181/500 [1:02:04<1:12:52, 13.71s/it] 36%|      | 182/500 [1:02:22<1:19:54, 15.08s/it] 37%|      | 183/500 [1:02:56<1:48:21, 20.51s/it] 37%|      | 184/500 [1:02:57<1:17:25, 14.70s/it] 37%|      | 185/500 [1:02:59<58:11, 11.08s/it]   37%|      | 186/500 [1:03:04<48:01,  9.18s/it] 37%|      | 187/500 [1:03:11<44:04,  8.45s/it] 38%|      | 188/500 [1:03:17<40:24,  7.77s/it] 38%|      | 189/500 [1:03:26<41:53,  8.08s/it] 38%|      | 190/500 [1:03:40<51:29,  9.97s/it] 38%|      | 191/500 [1:03:56<59:59, 11.65s/it] 38%|      | 192/500 [1:04:29<1:32:33, 18.03s/it] 39%|      | 193/500 [1:04:41<1:22:43, 16.17s/it] 39%|      | 194/500 [1:04:52<1:14:46, 14.66s/it] 39%|      | 195/500 [1:05:17<1:31:25, 17.99s/it] 39%|      | 196/500 [1:06:13<2:28:37, 29.33s/it] 39%|      | 197/500 [1:06:16<1:47:51, 21.36s/it] 40%|      | 198/500 [1:06:42<1:55:02, 22.86s/it] 40%|      | 199/500 [1:07:08<1:58:52, 23.70s/it] 40%|      | 200/500 [1:07:30<1:55:56, 23.19s/it] 40%|      | 201/500 [1:07:37<1:30:47, 18.22s/it] 40%|      | 202/500 [1:08:03<1:42:53, 20.72s/it] 41%|      | 203/500 [1:08:07<1:17:58, 15.75s/it] 41%|      | 204/500 [1:09:16<2:35:54, 31.60s/it] 41%|      | 205/500 [1:10:03<2:58:43, 36.35s/it] 41%|      | 206/500 [1:10:14<2:19:41, 28.51s/it] 41%|     | 207/500 [1:10:27<1:57:35, 24.08s/it] 42%|     | 208/500 [1:10:49<1:53:03, 23.23s/it] 42%|     | 209/500 [1:10:52<1:23:12, 17.16s/it] 42%|     | 210/500 [1:11:02<1:12:35, 15.02s/it] 42%|     | 211/500 [1:11:06<57:07, 11.86s/it]   42%|     | 212/500 [1:11:34<1:20:04, 16.68s/it] 43%|     | 213/500 [1:11:59<1:32:27, 19.33s/it] 43%|     | 214/500 [1:12:21<1:35:37, 20.06s/it] 43%|     | 215/500 [1:12:29<1:17:10, 16.25s/it] 43%|     | 216/500 [1:12:56<1:33:17, 19.71s/it] 43%|     | 217/500 [1:13:18<1:36:08, 20.38s/it] 44%|     | 218/500 [1:13:25<1:16:46, 16.34s/it] 44%|     | 219/500 [1:13:33<1:04:10, 13.70s/it] 44%|     | 220/500 [1:13:49<1:07:33, 14.48s/it] 44%|     | 221/500 [1:14:58<2:23:56, 30.96s/it] 44%|     | 222/500 [1:15:04<1:48:17, 23.37s/it] 45%|     | 223/500 [1:15:15<1:31:03, 19.72s/it] 45%|     | 224/500 [1:15:25<1:17:22, 16.82s/it] 45%|     | 225/500 [1:15:38<1:11:02, 15.50s/it] 45%|     | 226/500 [1:16:20<1:47:37, 23.57s/it] 45%|     | 227/500 [1:16:27<1:24:08, 18.49s/it] 46%|     | 228/500 [1:16:30<1:03:33, 14.02s/it] 46%|     | 229/500 [1:16:42<59:42, 13.22s/it]   46%|     | 230/500 [1:16:58<1:03:19, 14.07s/it] 46%|     | 231/500 [1:17:31<1:29:00, 19.85s/it] 46%|     | 232/500 [1:18:19<2:05:53, 28.18s/it] 47%|     | 233/500 [1:18:27<1:39:11, 22.29s/it] 47%|     | 234/500 [1:19:00<1:52:38, 25.41s/it] 47%|     | 235/500 [1:19:33<2:02:07, 27.65s/it] 47%|     | 236/500 [1:19:38<1:31:41, 20.84s/it] 47%|     | 237/500 [1:19:59<1:31:49, 20.95s/it] 48%|     | 238/500 [1:20:03<1:08:30, 15.69s/it] 48%|     | 239/500 [1:20:50<1:49:32, 25.18s/it] 48%|     | 240/500 [1:20:53<1:19:49, 18.42s/it] 48%|     | 241/500 [1:21:01<1:06:25, 15.39s/it] 48%|     | 242/500 [1:21:14<1:02:42, 14.58s/it] 49%|     | 243/500 [1:21:32<1:07:16, 15.71s/it] 49%|     | 244/500 [1:21:53<1:14:32, 17.47s/it] 49%|     | 245/500 [1:22:15<1:19:50, 18.79s/it] 49%|     | 246/500 [1:22:20<1:01:59, 14.64s/it] 49%|     | 247/500 [1:22:43<1:11:37, 16.99s/it] 50%|     | 248/500 [1:22:45<52:45, 12.56s/it]   50%|     | 249/500 [1:22:48<40:04,  9.58s/it] 50%|     | 250/500 [1:22:56<38:30,  9.24s/it] 50%|     | 251/500 [1:23:12<46:39, 11.24s/it] 50%|     | 252/500 [1:23:35<1:01:32, 14.89s/it] 51%|     | 253/500 [1:24:26<1:45:09, 25.55s/it] 51%|     | 254/500 [1:24:29<1:17:40, 18.94s/it] 51%|     | 255/500 [1:24:33<58:25, 14.31s/it]   51%|     | 256/500 [1:24:37<46:19, 11.39s/it] 51%|    | 257/500 [1:24:47<43:31, 10.75s/it] 52%|    | 258/500 [1:24:54<39:45,  9.86s/it] 52%|    | 259/500 [1:25:15<52:44, 13.13s/it] 52%|    | 260/500 [1:25:19<41:32, 10.38s/it] 52%|    | 261/500 [1:25:25<36:28,  9.16s/it] 52%|    | 262/500 [1:25:32<33:30,  8.45s/it] 53%|    | 263/500 [1:26:16<1:14:41, 18.91s/it] 53%|    | 264/500 [1:27:04<1:49:00, 27.71s/it] 53%|    | 265/500 [1:27:11<1:24:29, 21.57s/it] 53%|    | 266/500 [1:27:33<1:24:46, 21.74s/it] 53%|    | 267/500 [1:27:34<1:00:17, 15.53s/it] 54%|    | 268/500 [1:27:36<44:38, 11.55s/it]   54%|    | 269/500 [1:27:41<35:55,  9.33s/it] 54%|    | 270/500 [1:27:50<35:24,  9.24s/it] 54%|    | 271/500 [1:28:15<54:09, 14.19s/it] 54%|    | 272/500 [1:28:20<42:51, 11.28s/it] 55%|    | 273/500 [1:28:39<51:31, 13.62s/it] 55%|    | 274/500 [1:28:57<56:51, 15.09s/it] 55%|    | 275/500 [1:29:22<1:07:37, 18.03s/it] 55%|    | 276/500 [1:29:48<1:15:42, 20.28s/it] 55%|    | 277/500 [1:30:01<1:07:39, 18.21s/it] 56%|    | 278/500 [1:30:06<52:26, 14.17s/it]   56%|    | 279/500 [1:30:13<44:08, 11.98s/it] 56%|    | 280/500 [1:30:21<40:06, 10.94s/it] 56%|    | 281/500 [1:30:43<51:12, 14.03s/it] 56%|    | 282/500 [1:31:17<1:13:09, 20.13s/it] 57%|    | 283/500 [1:31:21<55:01, 15.22s/it]   57%|    | 284/500 [1:31:32<50:11, 13.94s/it] 57%|    | 285/500 [1:31:53<58:10, 16.24s/it] 57%|    | 286/500 [1:31:59<47:06, 13.21s/it] 57%|    | 287/500 [1:32:09<42:44, 12.04s/it] 58%|    | 288/500 [1:32:27<49:04, 13.89s/it] 58%|    | 289/500 [1:32:40<47:41, 13.56s/it] 58%|    | 290/500 [1:33:01<55:23, 15.83s/it] 58%|    | 291/500 [1:33:39<1:18:14, 22.46s/it] 58%|    | 292/500 [1:33:44<59:49, 17.26s/it]   59%|    | 293/500 [1:34:07<1:05:34, 19.01s/it] 59%|    | 294/500 [1:34:16<54:32, 15.89s/it]   59%|    | 295/500 [1:34:40<1:03:00, 18.44s/it] 59%|    | 296/500 [1:35:19<1:23:52, 24.67s/it] 59%|    | 297/500 [1:35:30<1:09:30, 20.54s/it] 60%|    | 298/500 [1:35:52<1:10:15, 20.87s/it] 60%|    | 299/500 [1:36:04<1:00:56, 18.19s/it] 60%|    | 300/500 [1:36:07<45:37, 13.69s/it]   60%|    | 301/500 [1:36:12<36:47, 11.09s/it] 60%|    | 302/500 [1:36:20<33:14, 10.07s/it] 61%|    | 303/500 [1:36:23<26:42,  8.13s/it] 61%|    | 304/500 [1:36:41<36:22, 11.14s/it] 61%|    | 305/500 [1:37:15<58:06, 17.88s/it] 61%|    | 306/500 [1:37:22<47:42, 14.76s/it] 61%|   | 307/500 [1:37:46<55:25, 17.23s/it] 62%|   | 308/500 [1:38:29<1:20:09, 25.05s/it] 62%|   | 309/500 [1:39:13<1:37:39, 30.68s/it] 62%|   | 310/500 [1:40:07<1:59:58, 37.89s/it] 62%|   | 311/500 [1:41:01<2:13:54, 42.51s/it] 62%|   | 312/500 [1:41:05<1:36:55, 30.93s/it] 63%|   | 313/500 [1:41:06<1:09:09, 22.19s/it] 63%|   | 314/500 [1:41:21<1:02:00, 20.00s/it] 63%|   | 315/500 [1:42:05<1:23:14, 27.00s/it] 63%|   | 316/500 [1:42:16<1:08:12, 22.24s/it] 63%|   | 317/500 [1:42:19<50:28, 16.55s/it]   64%|   | 318/500 [1:42:26<41:38, 13.73s/it] 64%|   | 319/500 [1:42:37<39:09, 12.98s/it] 64%|   | 320/500 [1:42:57<45:12, 15.07s/it] 64%|   | 321/500 [1:43:13<45:51, 15.37s/it] 64%|   | 322/500 [1:43:36<51:39, 17.41s/it] 65%|   | 323/500 [1:43:54<52:22, 17.75s/it] 65%|   | 324/500 [1:44:07<47:34, 16.22s/it] 65%|   | 325/500 [1:44:12<37:56, 13.01s/it] 65%|   | 326/500 [1:44:51<1:00:07, 20.73s/it] 65%|   | 327/500 [1:44:54<44:50, 15.55s/it]   66%|   | 328/500 [1:44:56<32:14, 11.25s/it] 66%|   | 329/500 [1:45:11<35:51, 12.58s/it] 66%|   | 330/500 [1:45:24<35:59, 12.71s/it] 66%|   | 331/500 [1:45:53<49:38, 17.63s/it] 66%|   | 332/500 [1:45:57<37:49, 13.51s/it] 67%|   | 333/500 [1:46:03<30:37, 11.00s/it] 67%|   | 334/500 [1:46:16<32:18, 11.67s/it] 67%|   | 335/500 [1:46:17<23:24,  8.51s/it] 67%|   | 336/500 [1:46:18<17:09,  6.28s/it] 67%|   | 337/500 [1:46:21<14:13,  5.24s/it] 68%|   | 338/500 [1:46:25<13:27,  4.99s/it] 68%|   | 339/500 [1:46:36<18:10,  6.77s/it] 68%|   | 340/500 [1:46:47<21:23,  8.02s/it] 68%|   | 341/500 [1:46:57<22:30,  8.50s/it] 68%|   | 342/500 [1:47:10<26:00,  9.88s/it] 69%|   | 343/500 [1:47:36<38:35, 14.75s/it] 69%|   | 344/500 [1:47:37<27:53, 10.73s/it] 69%|   | 345/500 [1:47:44<24:25,  9.45s/it] 69%|   | 346/500 [1:48:17<42:57, 16.73s/it] 69%|   | 347/500 [1:48:31<39:58, 15.68s/it] 70%|   | 348/500 [1:48:34<30:04, 11.87s/it] 70%|   | 349/500 [1:48:39<25:10, 10.01s/it] 70%|   | 350/500 [1:48:49<25:08, 10.05s/it] 70%|   | 351/500 [1:49:05<28:58, 11.67s/it] 70%|   | 352/500 [1:49:32<40:32, 16.43s/it] 71%|   | 353/500 [1:50:02<50:00, 20.41s/it] 71%|   | 354/500 [1:50:31<55:51, 22.95s/it] 71%|   | 355/500 [1:51:02<1:01:05, 25.28s/it] 71%|   | 356/500 [1:51:33<1:05:18, 27.21s/it] 71%|  | 357/500 [1:51:41<50:38, 21.25s/it]   72%|  | 358/500 [1:52:24<1:05:52, 27.83s/it] 72%|  | 359/500 [1:52:26<47:33, 20.24s/it]   72%|  | 360/500 [1:52:37<40:37, 17.41s/it] 72%|  | 361/500 [1:52:40<29:58, 12.94s/it] 72%|  | 362/500 [1:53:02<36:16, 15.77s/it] 73%|  | 363/500 [1:53:32<45:45, 20.04s/it] 73%|  | 364/500 [1:53:34<33:02, 14.58s/it] 73%|  | 365/500 [1:53:44<29:41, 13.20s/it] 73%|  | 366/500 [1:53:57<29:41, 13.29s/it] 73%|  | 367/500 [1:54:00<22:13, 10.03s/it] 74%|  | 368/500 [1:54:30<35:31, 16.15s/it] 74%|  | 369/500 [1:54:49<36:40, 16.80s/it] 74%|  | 370/500 [1:54:51<26:54, 12.42s/it] 74%|  | 371/500 [1:54:56<21:47, 10.14s/it] 74%|  | 372/500 [1:55:00<17:57,  8.42s/it] 75%|  | 373/500 [1:55:09<17:57,  8.48s/it] 75%|  | 374/500 [1:55:30<26:11, 12.47s/it] 75%|  | 375/500 [1:55:34<20:23,  9.79s/it] 75%|  | 376/500 [1:55:54<26:49, 12.98s/it] 75%|  | 377/500 [1:56:42<47:36, 23.22s/it] 76%|  | 378/500 [1:57:20<56:45, 27.92s/it] 76%|  | 379/500 [1:58:11<1:09:55, 34.67s/it] 76%|  | 380/500 [1:58:17<52:17, 26.14s/it]   76%|  | 381/500 [1:58:38<48:44, 24.58s/it] 76%|  | 382/500 [2:00:10<1:27:53, 44.69s/it] 77%|  | 383/500 [2:01:12<1:37:31, 50.01s/it] 77%|  | 384/500 [2:01:48<1:28:15, 45.65s/it] 77%|  | 385/500 [2:01:50<1:02:47, 32.76s/it] 77%|  | 386/500 [2:01:54<45:53, 24.15s/it]   77%|  | 387/500 [2:02:01<35:22, 18.78s/it] 78%|  | 388/500 [2:02:09<29:10, 15.63s/it] 78%|  | 389/500 [2:02:30<32:14, 17.43s/it] 78%|  | 390/500 [2:02:33<23:32, 12.84s/it] 78%|  | 391/500 [2:02:35<17:26,  9.60s/it] 78%|  | 392/500 [2:02:48<19:16, 10.71s/it] 79%|  | 393/500 [2:03:08<23:53, 13.40s/it] 79%|  | 394/500 [2:03:38<32:26, 18.36s/it] 79%|  | 395/500 [2:03:44<26:00, 14.87s/it] 79%|  | 396/500 [2:03:54<23:16, 13.43s/it] 79%|  | 397/500 [2:04:04<21:13, 12.37s/it] 80%|  | 398/500 [2:04:23<24:24, 14.36s/it] 80%|  | 399/500 [2:04:54<32:33, 19.34s/it] 80%|  | 400/500 [2:05:02<26:21, 15.82s/it] 80%|  | 401/500 [2:05:08<21:27, 13.00s/it] 80%|  | 402/500 [2:05:33<27:10, 16.64s/it] 81%|  | 403/500 [2:05:59<31:30, 19.49s/it] 81%|  | 404/500 [2:06:18<30:33, 19.09s/it] 81%|  | 405/500 [2:06:53<37:49, 23.89s/it] 81%|  | 406/500 [2:08:02<58:53, 37.59s/it] 81%| | 407/500 [2:08:17<47:47, 30.83s/it] 82%| | 408/500 [2:08:36<41:43, 27.22s/it] 82%| | 409/500 [2:10:39<1:24:51, 55.95s/it] 82%| | 410/500 [2:11:45<1:28:15, 58.84s/it] 82%| | 411/500 [2:12:00<1:07:57, 45.82s/it] 82%| | 412/500 [2:12:02<48:03, 32.77s/it]   83%| | 413/500 [2:12:06<34:56, 24.09s/it] 83%| | 414/500 [2:12:13<27:07, 18.92s/it] 83%| | 415/500 [2:12:30<25:44, 18.17s/it] 83%| | 416/500 [2:12:56<28:50, 20.60s/it] 83%| | 417/500 [2:12:59<21:20, 15.43s/it] 84%| | 418/500 [2:13:05<17:06, 12.51s/it] 84%| | 419/500 [2:13:13<15:05, 11.17s/it] 84%| | 420/500 [2:13:30<17:05, 12.82s/it] 84%| | 421/500 [2:13:45<17:57, 13.64s/it] 84%| | 422/500 [2:13:57<16:58, 13.06s/it] 85%| | 423/500 [2:14:16<19:02, 14.84s/it] 85%| | 424/500 [2:14:42<22:55, 18.10s/it] 85%| | 425/500 [2:14:43<16:14, 13.00s/it] 85%| | 426/500 [2:14:45<12:04,  9.79s/it] 85%| | 427/500 [2:14:49<09:38,  7.92s/it] 86%| | 428/500 [2:14:55<08:54,  7.43s/it] 86%| | 429/500 [2:14:58<07:10,  6.06s/it] 86%| | 430/500 [2:15:05<07:24,  6.35s/it] 86%| | 431/500 [2:15:09<06:34,  5.72s/it] 86%| | 432/500 [2:15:16<06:52,  6.06s/it] 87%| | 433/500 [2:15:35<11:01,  9.88s/it] 87%| | 434/500 [2:16:01<16:19, 14.84s/it] 87%| | 435/500 [2:16:27<19:47, 18.26s/it] 87%| | 436/500 [2:16:35<16:03, 15.05s/it] 87%| | 437/500 [2:16:57<18:00, 17.14s/it] 88%| | 438/500 [2:16:59<13:08, 12.71s/it] 88%| | 439/500 [2:17:01<09:35,  9.43s/it] 88%| | 440/500 [2:17:03<07:14,  7.23s/it] 88%| | 441/500 [2:17:06<05:56,  6.05s/it] 88%| | 442/500 [2:17:22<08:29,  8.79s/it] 89%| | 443/500 [2:18:05<18:07, 19.08s/it] 89%| | 444/500 [2:18:09<13:32, 14.51s/it] 89%| | 445/500 [2:18:17<11:44, 12.81s/it] 89%| | 446/500 [2:18:41<14:19, 15.92s/it] 89%| | 447/500 [2:18:43<10:22, 11.74s/it] 90%| | 448/500 [2:18:45<07:38,  8.81s/it] 90%| | 449/500 [2:19:00<09:16, 10.91s/it] 90%| | 450/500 [2:20:07<22:54, 27.49s/it] 90%| | 451/500 [2:20:28<21:03, 25.79s/it] 90%| | 452/500 [2:20:31<14:57, 18.71s/it] 91%| | 453/500 [2:20:36<11:28, 14.64s/it] 91%| | 454/500 [2:21:06<14:45, 19.26s/it] 91%| | 455/500 [2:22:02<22:42, 30.29s/it] 91%| | 456/500 [2:22:20<19:39, 26.80s/it] 91%|| 457/500 [2:22:33<16:08, 22.52s/it] 92%|| 458/500 [2:22:54<15:30, 22.16s/it] 92%|| 459/500 [2:23:20<15:55, 23.30s/it] 92%|| 460/500 [2:23:25<11:46, 17.66s/it] 92%|| 461/500 [2:23:27<08:30, 13.09s/it] 92%|| 462/500 [2:23:30<06:17,  9.94s/it] 93%|| 463/500 [2:23:38<05:50,  9.46s/it] 93%|| 464/500 [2:23:56<07:13, 12.04s/it] 93%|| 465/500 [2:24:25<09:57, 17.08s/it] 93%|| 466/500 [2:24:31<07:50, 13.84s/it] 93%|| 467/500 [2:24:56<09:27, 17.20s/it] 94%|| 468/500 [2:27:12<28:05, 52.68s/it] 94%|| 469/500 [2:28:02<26:48, 51.89s/it] 94%|| 470/500 [2:28:05<18:34, 37.15s/it] 94%|| 471/500 [2:28:09<13:13, 27.35s/it] 94%|| 472/500 [2:28:18<10:07, 21.71s/it] 95%|| 473/500 [2:28:30<08:30, 18.91s/it] 95%|| 474/500 [2:28:53<08:45, 20.22s/it] 95%|| 475/500 [2:29:19<09:09, 21.96s/it] 95%|| 476/500 [2:29:24<06:41, 16.74s/it] 95%|| 477/500 [2:30:01<08:47, 22.93s/it] 96%|| 478/500 [2:30:58<12:09, 33.17s/it] 96%|| 479/500 [2:31:58<14:23, 41.10s/it] 96%|| 480/500 [2:32:02<10:00, 30.02s/it] 96%|| 481/500 [2:32:15<07:54, 24.98s/it] 96%|| 482/500 [2:32:19<05:32, 18.48s/it] 97%|| 483/500 [2:32:25<04:11, 14.82s/it] 97%|| 484/500 [2:32:34<03:29, 13.09s/it] 97%|| 485/500 [2:32:58<04:07, 16.47s/it] 97%|| 486/500 [2:33:00<02:50, 12.16s/it] 97%|| 487/500 [2:33:06<02:11, 10.12s/it] 98%|| 488/500 [2:33:12<01:48,  9.04s/it] 98%|| 489/500 [2:33:21<01:38,  8.93s/it] 98%|| 490/500 [2:33:45<02:14, 13.47s/it] 98%|| 491/500 [2:33:47<01:29,  9.98s/it] 98%|| 492/500 [2:34:20<02:16, 17.08s/it] 99%|| 493/500 [2:34:58<02:42, 23.18s/it] 99%|| 494/500 [2:35:25<02:25, 24.28s/it] 99%|| 495/500 [2:35:48<01:59, 23.91s/it] 99%|| 496/500 [2:36:26<01:52, 28.23s/it] 99%|| 497/500 [2:36:29<01:01, 20.66s/it]100%|| 498/500 [2:36:36<00:32, 16.49s/it]100%|| 499/500 [2:36:41<00:13, 13.06s/it]100%|| 500/500 [2:36:48<00:00, 11.16s/it]100%|| 500/500 [2:36:48<00:00, 18.82s/it]
