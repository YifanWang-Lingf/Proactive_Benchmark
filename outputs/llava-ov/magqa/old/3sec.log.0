/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
ClipTestArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
attn_implementation=flash_attention_2,
augmentation=False,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
consecutive_n_frames_threshold=1,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
dataset_config=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
embed_mark=2fps_384_1+3x3,
end_idx=1000,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=no,
eval_use_gather_object=False,
evaluation_strategy=None,
finetune_modules=['connector', 'mm_projector', 'response_head', 'related_head'],
first_n_frames_no_generate=0,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
frame_fps=2.0,
frame_num_tokens=49,
frame_resolution=384,
frame_token_cls=False,
frame_token_pooled=[7, 7],
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
grounding_mode=False,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
input_dir=/share2/wangyq/projects/MMDuet/datasets/shot2story/videos,
is_online_model=True,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
live_version=test,
llm_pretrained=lmms-lab/llava-onevision-qwen2-7b-ov,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=outputs/debug/runs/Apr19_12-55-39_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lora_alpha=32,
lora_modules=model\.layers.*(q_proj|k_proj|v_proj|o_proj|gate_proj|up_proj|down_proj)$,
lora_pretrained=None,
lora_r=16,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_num_frames=400,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
no_output_before_user_input=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=outputs/debug,
output_fname=outputs/llava-ov/magqa/3sec.jsonl.0,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_assistant_turns=False,
remove_unused_columns=True,
repetition_penalty=None,
report_to=['tensorboard', 'wandb'],
response_min_interval_frames=None,
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=outputs/debug,
running_list_length=20,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
score_heads=informative_score,
seed=42,
skip_memory_metrics=True,
split_batches=None,
start_idx=0,
stream_end_prob_threshold=None,
stream_end_score_sum_threshold=None,
stream_loss_weight=1.0,
system_prompt=A multimodal AI assistant is helping users with some activities. Below is their conversation, interleaved with the list of video frames received by the assistant.,
test_fname=/share2/wangyq/projects/MMDuet/datasets/shot2story/annotations/magqa_test.json,
tf32=None,
threshold_z=None,
time_instruction_format=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
v_placeholder=<image>,
video_chunk_sec=3,
video_pooling_stride=4,
vision_pretrained=google/siglip-large-patch16-384,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Loaded LLaVA model: lmms-lab/llava-onevision-qwen2-7b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 237.35it/s]
Loading vision tower: google/siglip-so400m-patch14-384
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.patch_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.patch_embedding.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.position_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.probe: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.attention.in_proj_weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.attention.in_proj_bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.attention.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.attention.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:21<01:04, 21.61s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:43<00:44, 22.06s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:06<00:22, 22.35s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:12<00:00, 15.98s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:12<00:00, 18.23s/it]
Model Class: LlavaQwenForCausalLM
  0%|          | 0/1000 [00:00<?, ?it/s]/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:612: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  0%|          | 1/1000 [00:15<4:12:28, 15.16s/it]  0%|          | 2/1000 [00:19<2:24:21,  8.68s/it]  0%|          | 3/1000 [00:23<1:49:34,  6.59s/it]  0%|          | 4/1000 [00:26<1:26:47,  5.23s/it]  0%|          | 5/1000 [00:30<1:18:51,  4.76s/it]  1%|          | 6/1000 [00:38<1:36:54,  5.85s/it]  1%|          | 7/1000 [00:48<1:59:18,  7.21s/it]  1%|          | 8/1000 [00:50<1:34:20,  5.71s/it]  1%|          | 9/1000 [00:55<1:27:36,  5.30s/it]  1%|          | 10/1000 [01:00<1:25:18,  5.17s/it]  1%|          | 11/1000 [01:05<1:27:46,  5.32s/it]  1%|          | 12/1000 [01:09<1:20:46,  4.91s/it]  1%|▏         | 13/1000 [01:22<1:58:11,  7.19s/it]  1%|▏         | 14/1000 [01:28<1:51:48,  6.80s/it]  2%|▏         | 15/1000 [01:37<2:02:08,  7.44s/it]  2%|▏         | 16/1000 [01:42<1:50:43,  6.75s/it]  2%|▏         | 17/1000 [01:47<1:43:51,  6.34s/it]  2%|▏         | 18/1000 [01:53<1:42:54,  6.29s/it]  2%|▏         | 19/1000 [01:58<1:36:10,  5.88s/it]  2%|▏         | 20/1000 [02:09<2:01:38,  7.45s/it]  2%|▏         | 21/1000 [02:15<1:52:17,  6.88s/it]  2%|▏         | 22/1000 [02:19<1:38:33,  6.05s/it]  2%|▏         | 23/1000 [02:25<1:38:11,  6.03s/it]  2%|▏         | 24/1000 [02:30<1:30:41,  5.58s/it]  2%|▎         | 25/1000 [02:34<1:27:02,  5.36s/it]  3%|▎         | 26/1000 [02:41<1:32:14,  5.68s/it]  3%|▎         | 27/1000 [02:55<2:13:24,  8.23s/it]  3%|▎         | 28/1000 [03:02<2:08:37,  7.94s/it]  3%|▎         | 29/1000 [03:04<1:39:25,  6.14s/it]  3%|▎         | 30/1000 [03:09<1:35:09,  5.89s/it]  3%|▎         | 31/1000 [03:15<1:31:12,  5.65s/it]  3%|▎         | 32/1000 [03:16<1:12:31,  4.50s/it]  3%|▎         | 33/1000 [03:20<1:10:07,  4.35s/it]  3%|▎         | 34/1000 [03:24<1:07:40,  4.20s/it]  4%|▎         | 35/1000 [03:28<1:03:53,  3.97s/it]  4%|▎         | 36/1000 [03:31<58:38,  3.65s/it]    4%|▎         | 37/1000 [03:34<58:12,  3.63s/it]  4%|▍         | 38/1000 [03:43<1:22:54,  5.17s/it]  4%|▍         | 39/1000 [03:48<1:21:57,  5.12s/it]  4%|▍         | 40/1000 [03:59<1:48:12,  6.76s/it]  4%|▍         | 41/1000 [04:11<2:15:23,  8.47s/it]  4%|▍         | 42/1000 [04:17<2:01:46,  7.63s/it]  4%|▍         | 43/1000 [04:26<2:08:07,  8.03s/it]  4%|▍         | 44/1000 [04:31<1:53:37,  7.13s/it]  4%|▍         | 45/1000 [04:34<1:36:48,  6.08s/it]  5%|▍         | 46/1000 [04:38<1:25:00,  5.35s/it]  5%|▍         | 47/1000 [04:43<1:22:18,  5.18s/it]  5%|▍         | 48/1000 [04:46<1:14:32,  4.70s/it]  5%|▍         | 49/1000 [04:51<1:13:18,  4.63s/it]  5%|▌         | 50/1000 [04:55<1:09:17,  4.38s/it]  5%|▌         | 51/1000 [05:05<1:37:08,  6.14s/it]  5%|▌         | 52/1000 [05:09<1:28:05,  5.58s/it]  5%|▌         | 53/1000 [05:12<1:14:04,  4.69s/it]  5%|▌         | 54/1000 [05:16<1:10:35,  4.48s/it]  6%|▌         | 55/1000 [05:22<1:19:07,  5.02s/it]  6%|▌         | 56/1000 [05:25<1:08:20,  4.34s/it]  6%|▌         | 57/1000 [05:29<1:10:13,  4.47s/it]  6%|▌         | 58/1000 [05:34<1:08:27,  4.36s/it]  6%|▌         | 59/1000 [05:41<1:21:52,  5.22s/it]  6%|▌         | 60/1000 [05:48<1:29:07,  5.69s/it]  6%|▌         | 61/1000 [05:52<1:23:21,  5.33s/it]  6%|▌         | 62/1000 [06:02<1:47:06,  6.85s/it]  6%|▋         | 63/1000 [06:09<1:43:52,  6.65s/it]  6%|▋         | 64/1000 [06:16<1:46:24,  6.82s/it]  6%|▋         | 65/1000 [06:22<1:44:37,  6.71s/it]  7%|▋         | 66/1000 [06:27<1:32:53,  5.97s/it]  7%|▋         | 67/1000 [06:34<1:40:48,  6.48s/it]  7%|▋         | 68/1000 [06:38<1:28:21,  5.69s/it]  7%|▋         | 69/1000 [06:44<1:28:46,  5.72s/it]  7%|▋         | 70/1000 [06:48<1:22:44,  5.34s/it]  7%|▋         | 71/1000 [06:53<1:21:37,  5.27s/it]  7%|▋         | 72/1000 [06:57<1:14:18,  4.80s/it]  7%|▋         | 73/1000 [07:01<1:10:26,  4.56s/it]  7%|▋         | 74/1000 [07:07<1:15:48,  4.91s/it]  8%|▊         | 75/1000 [07:10<1:09:00,  4.48s/it]  8%|▊         | 76/1000 [07:14<1:07:02,  4.35s/it]  8%|▊         | 77/1000 [07:24<1:33:12,  6.06s/it]  8%|▊         | 78/1000 [07:29<1:28:23,  5.75s/it]  8%|▊         | 79/1000 [07:37<1:38:13,  6.40s/it]  8%|▊         | 80/1000 [07:42<1:28:56,  5.80s/it]  8%|▊         | 81/1000 [07:47<1:28:10,  5.76s/it]  8%|▊         | 82/1000 [07:53<1:25:51,  5.61s/it]  8%|▊         | 83/1000 [07:57<1:19:01,  5.17s/it]  8%|▊         | 84/1000 [08:04<1:27:54,  5.76s/it]  8%|▊         | 85/1000 [08:10<1:29:57,  5.90s/it]  9%|▊         | 86/1000 [08:14<1:22:20,  5.41s/it]  9%|▊         | 87/1000 [08:22<1:30:09,  5.93s/it]  9%|▉         | 88/1000 [08:27<1:28:04,  5.79s/it]  9%|▉         | 89/1000 [08:31<1:19:42,  5.25s/it]  9%|▉         | 90/1000 [08:35<1:11:59,  4.75s/it]  9%|▉         | 91/1000 [08:38<1:06:37,  4.40s/it]  9%|▉         | 92/1000 [08:42<1:05:10,  4.31s/it]  9%|▉         | 93/1000 [08:49<1:17:13,  5.11s/it]  9%|▉         | 94/1000 [08:53<1:09:47,  4.62s/it] 10%|▉         | 95/1000 [08:59<1:15:19,  4.99s/it] 10%|▉         | 96/1000 [09:02<1:08:30,  4.55s/it] 10%|▉         | 97/1000 [09:08<1:12:40,  4.83s/it] 10%|▉         | 98/1000 [09:14<1:21:31,  5.42s/it] 10%|▉         | 99/1000 [09:19<1:17:15,  5.14s/it] 10%|█         | 100/1000 [09:24<1:18:14,  5.22s/it] 10%|█         | 101/1000 [09:30<1:21:00,  5.41s/it] 10%|█         | 102/1000 [09:39<1:35:27,  6.38s/it] 10%|█         | 103/1000 [09:42<1:18:59,  5.28s/it] 10%|█         | 104/1000 [09:47<1:19:08,  5.30s/it] 10%|█         | 105/1000 [09:55<1:33:30,  6.27s/it] 11%|█         | 106/1000 [10:07<1:58:53,  7.98s/it] 11%|█         | 107/1000 [10:15<1:57:35,  7.90s/it] 11%|█         | 108/1000 [10:18<1:35:29,  6.42s/it] 11%|█         | 109/1000 [10:22<1:26:17,  5.81s/it] 11%|█         | 110/1000 [10:26<1:15:45,  5.11s/it] 11%|█         | 111/1000 [10:30<1:12:27,  4.89s/it] 11%|█         | 112/1000 [10:34<1:07:53,  4.59s/it] 11%|█▏        | 113/1000 [10:38<1:03:47,  4.31s/it] 11%|█▏        | 114/1000 [10:41<59:10,  4.01s/it]   12%|█▏        | 115/1000 [10:51<1:24:16,  5.71s/it] 12%|█▏        | 116/1000 [11:01<1:42:06,  6.93s/it] 12%|█▏        | 117/1000 [11:07<1:39:36,  6.77s/it] 12%|█▏        | 118/1000 [11:23<2:21:24,  9.62s/it] 12%|█▏        | 119/1000 [11:39<2:50:06, 11.58s/it] 12%|█▏        | 120/1000 [11:47<2:34:05, 10.51s/it] 12%|█▏        | 121/1000 [11:55<2:21:38,  9.67s/it] 12%|█▏        | 122/1000 [11:59<1:55:16,  7.88s/it] 12%|█▏        | 123/1000 [12:03<1:37:14,  6.65s/it] 12%|█▏        | 124/1000 [12:07<1:28:39,  6.07s/it] 12%|█▎        | 125/1000 [12:12<1:22:06,  5.63s/it] 13%|█▎        | 126/1000 [12:16<1:15:18,  5.17s/it] 13%|█▎        | 127/1000 [12:22<1:17:21,  5.32s/it] 13%|█▎        | 128/1000 [12:28<1:19:19,  5.46s/it] 13%|█▎        | 129/1000 [12:33<1:20:30,  5.55s/it] 13%|█▎        | 130/1000 [12:37<1:13:54,  5.10s/it] 13%|█▎        | 131/1000 [12:44<1:22:34,  5.70s/it] 13%|█▎        | 132/1000 [12:53<1:33:49,  6.49s/it] 13%|█▎        | 133/1000 [12:58<1:28:42,  6.14s/it] 13%|█▎        | 134/1000 [13:03<1:24:06,  5.83s/it] 14%|█▎        | 135/1000 [13:07<1:13:20,  5.09s/it] 14%|█▎        | 136/1000 [13:13<1:17:35,  5.39s/it] 14%|█▎        | 137/1000 [13:19<1:19:48,  5.55s/it] 14%|█▍        | 138/1000 [13:20<1:03:16,  4.40s/it] 14%|█▍        | 139/1000 [13:24<1:01:53,  4.31s/it] 14%|█▍        | 140/1000 [13:28<58:15,  4.06s/it]   14%|█▍        | 141/1000 [13:31<52:45,  3.69s/it] 14%|█▍        | 142/1000 [13:33<45:55,  3.21s/it] 14%|█▍        | 143/1000 [13:36<47:57,  3.36s/it] 14%|█▍        | 144/1000 [13:43<1:00:04,  4.21s/it] 14%|█▍        | 145/1000 [13:49<1:09:02,  4.85s/it] 15%|█▍        | 146/1000 [13:56<1:16:13,  5.36s/it] 15%|█▍        | 147/1000 [13:59<1:08:12,  4.80s/it] 15%|█▍        | 148/1000 [14:03<1:05:27,  4.61s/it] 15%|█▍        | 149/1000 [14:10<1:14:34,  5.26s/it] 15%|█▌        | 150/1000 [14:17<1:22:04,  5.79s/it] 15%|█▌        | 151/1000 [14:22<1:19:34,  5.62s/it] 15%|█▌        | 152/1000 [14:29<1:23:11,  5.89s/it] 15%|█▌        | 153/1000 [14:34<1:22:02,  5.81s/it] 15%|█▌        | 154/1000 [14:38<1:10:51,  5.03s/it] 16%|█▌        | 155/1000 [14:41<1:04:41,  4.59s/it] 16%|█▌        | 156/1000 [14:44<59:05,  4.20s/it]   16%|█▌        | 157/1000 [14:51<1:08:40,  4.89s/it] 16%|█▌        | 158/1000 [14:53<56:42,  4.04s/it]   16%|█▌        | 159/1000 [14:59<1:05:12,  4.65s/it] 16%|█▌        | 160/1000 [15:05<1:09:05,  4.93s/it] 16%|█▌        | 161/1000 [15:10<1:10:08,  5.02s/it] 16%|█▌        | 162/1000 [15:14<1:04:45,  4.64s/it] 16%|█▋        | 163/1000 [15:20<1:13:16,  5.25s/it] 16%|█▋        | 164/1000 [15:24<1:06:47,  4.79s/it] 16%|█▋        | 165/1000 [15:28<1:05:07,  4.68s/it] 17%|█▋        | 166/1000 [15:33<1:04:35,  4.65s/it] 17%|█▋        | 167/1000 [15:37<1:01:45,  4.45s/it] 17%|█▋        | 168/1000 [15:42<1:05:00,  4.69s/it] 17%|█▋        | 169/1000 [15:51<1:21:02,  5.85s/it] 17%|█▋        | 170/1000 [16:00<1:35:57,  6.94s/it] 17%|█▋        | 171/1000 [16:08<1:38:37,  7.14s/it] 17%|█▋        | 172/1000 [16:12<1:24:44,  6.14s/it] 17%|█▋        | 173/1000 [16:20<1:32:45,  6.73s/it] 17%|█▋        | 174/1000 [16:26<1:29:41,  6.52s/it] 18%|█▊        | 175/1000 [16:30<1:20:33,  5.86s/it] 18%|█▊        | 176/1000 [16:36<1:18:58,  5.75s/it] 18%|█▊        | 177/1000 [16:39<1:10:11,  5.12s/it] 18%|█▊        | 178/1000 [16:45<1:10:44,  5.16s/it] 18%|█▊        | 179/1000 [16:49<1:05:54,  4.82s/it] 18%|█▊        | 180/1000 [16:55<1:11:09,  5.21s/it] 18%|█▊        | 181/1000 [17:02<1:20:25,  5.89s/it] 18%|█▊        | 182/1000 [17:08<1:20:59,  5.94s/it] 18%|█▊        | 183/1000 [17:13<1:16:19,  5.61s/it] 18%|█▊        | 184/1000 [17:17<1:11:10,  5.23s/it] 18%|█▊        | 185/1000 [17:27<1:28:10,  6.49s/it] 19%|█▊        | 186/1000 [17:33<1:27:55,  6.48s/it] 19%|█▊        | 187/1000 [17:40<1:28:45,  6.55s/it] 19%|█▉        | 188/1000 [17:49<1:36:32,  7.13s/it] 19%|█▉        | 189/1000 [17:54<1:31:36,  6.78s/it] 19%|█▉        | 190/1000 [18:01<1:32:06,  6.82s/it] 19%|█▉        | 191/1000 [18:06<1:23:19,  6.18s/it] 19%|█▉        | 192/1000 [18:13<1:27:44,  6.51s/it] 19%|█▉        | 193/1000 [18:18<1:21:05,  6.03s/it] 19%|█▉        | 194/1000 [18:22<1:11:54,  5.35s/it] 20%|█▉        | 195/1000 [18:28<1:12:48,  5.43s/it] 20%|█▉        | 196/1000 [18:33<1:12:28,  5.41s/it] 20%|█▉        | 197/1000 [18:35<1:00:24,  4.51s/it] 20%|█▉        | 198/1000 [18:40<1:00:49,  4.55s/it] 20%|█▉        | 199/1000 [18:43<55:56,  4.19s/it]   20%|██        | 200/1000 [18:46<47:54,  3.59s/it] 20%|██        | 201/1000 [18:50<50:58,  3.83s/it] 20%|██        | 202/1000 [18:54<49:37,  3.73s/it] 20%|██        | 203/1000 [18:57<46:48,  3.52s/it] 20%|██        | 204/1000 [19:10<1:24:21,  6.36s/it] 20%|██        | 205/1000 [19:16<1:24:17,  6.36s/it] 21%|██        | 206/1000 [19:29<1:49:17,  8.26s/it] 21%|██        | 207/1000 [19:41<2:05:55,  9.53s/it] 21%|██        | 208/1000 [19:49<1:58:08,  8.95s/it] 21%|██        | 209/1000 [19:56<1:53:32,  8.61s/it] 21%|██        | 210/1000 [20:20<2:50:27, 12.95s/it] 21%|██        | 211/1000 [20:34<2:54:19, 13.26s/it] 21%|██        | 212/1000 [20:41<2:30:17, 11.44s/it] 21%|██▏       | 213/1000 [20:44<1:58:31,  9.04s/it] 21%|██▏       | 214/1000 [20:50<1:45:51,  8.08s/it] 22%|██▏       | 215/1000 [21:01<1:57:45,  9.00s/it] 22%|██▏       | 216/1000 [21:06<1:41:43,  7.79s/it] 22%|██▏       | 217/1000 [21:08<1:20:17,  6.15s/it] 22%|██▏       | 218/1000 [21:14<1:17:30,  5.95s/it] 22%|██▏       | 219/1000 [21:21<1:23:17,  6.40s/it] 22%|██▏       | 220/1000 [21:27<1:18:31,  6.04s/it] 22%|██▏       | 221/1000 [21:34<1:21:52,  6.31s/it] 22%|██▏       | 222/1000 [21:44<1:37:58,  7.56s/it] 22%|██▏       | 223/1000 [21:47<1:22:00,  6.33s/it] 22%|██▏       | 224/1000 [21:50<1:08:17,  5.28s/it] 22%|██▎       | 225/1000 [21:54<1:02:39,  4.85s/it] 23%|██▎       | 226/1000 [21:58<57:18,  4.44s/it]   23%|██▎       | 227/1000 [22:01<53:56,  4.19s/it] 23%|██▎       | 228/1000 [22:04<48:25,  3.76s/it] 23%|██▎       | 229/1000 [22:07<46:57,  3.65s/it] 23%|██▎       | 230/1000 [22:13<54:51,  4.27s/it] 23%|██▎       | 231/1000 [22:19<1:00:17,  4.70s/it] 23%|██▎       | 232/1000 [22:26<1:08:44,  5.37s/it] 23%|██▎       | 233/1000 [22:34<1:21:27,  6.37s/it] 23%|██▎       | 234/1000 [22:39<1:13:39,  5.77s/it] 24%|██▎       | 235/1000 [22:46<1:19:43,  6.25s/it] 24%|██▎       | 236/1000 [22:54<1:24:17,  6.62s/it] 24%|██▎       | 237/1000 [22:58<1:14:03,  5.82s/it] 24%|██▍       | 238/1000 [23:06<1:22:23,  6.49s/it] 24%|██▍       | 239/1000 [23:12<1:23:20,  6.57s/it] 24%|██▍       | 240/1000 [23:17<1:17:08,  6.09s/it] 24%|██▍       | 241/1000 [23:27<1:31:44,  7.25s/it] 24%|██▍       | 242/1000 [23:31<1:18:17,  6.20s/it] 24%|██▍       | 243/1000 [23:34<1:07:04,  5.32s/it] 24%|██▍       | 244/1000 [23:37<57:02,  4.53s/it]   24%|██▍       | 245/1000 [23:41<54:23,  4.32s/it] 25%|██▍       | 246/1000 [23:44<49:54,  3.97s/it] 25%|██▍       | 247/1000 [23:47<47:14,  3.76s/it] 25%|██▍       | 248/1000 [23:51<46:38,  3.72s/it] 25%|██▍       | 249/1000 [23:56<51:42,  4.13s/it] 25%|██▌       | 250/1000 [23:59<47:28,  3.80s/it] 25%|██▌       | 251/1000 [24:04<50:38,  4.06s/it] 25%|██▌       | 252/1000 [24:09<55:43,  4.47s/it] 25%|██▌       | 253/1000 [24:18<1:11:19,  5.73s/it] 25%|██▌       | 254/1000 [24:26<1:19:04,  6.36s/it] 26%|██▌       | 255/1000 [24:32<1:19:36,  6.41s/it] 26%|██▌       | 256/1000 [24:37<1:12:41,  5.86s/it] 26%|██▌       | 257/1000 [24:41<1:06:11,  5.34s/it] 26%|██▌       | 258/1000 [24:44<58:56,  4.77s/it]   26%|██▌       | 259/1000 [24:51<1:04:53,  5.25s/it] 26%|██▌       | 260/1000 [24:56<1:05:14,  5.29s/it] 26%|██▌       | 261/1000 [25:00<1:00:42,  4.93s/it] 26%|██▌       | 262/1000 [25:03<53:22,  4.34s/it]   26%|██▋       | 263/1000 [25:09<1:00:10,  4.90s/it] 26%|██▋       | 264/1000 [25:18<1:13:45,  6.01s/it] 26%|██▋       | 265/1000 [25:24<1:15:29,  6.16s/it] 27%|██▋       | 266/1000 [25:27<1:03:17,  5.17s/it] 27%|██▋       | 267/1000 [25:31<58:11,  4.76s/it]   27%|██▋       | 268/1000 [25:36<59:09,  4.85s/it] 27%|██▋       | 269/1000 [25:39<51:35,  4.24s/it] 27%|██▋       | 270/1000 [25:43<49:47,  4.09s/it] 27%|██▋       | 271/1000 [25:46<46:03,  3.79s/it] 27%|██▋       | 272/1000 [25:50<45:37,  3.76s/it] 27%|██▋       | 273/1000 [25:53<43:14,  3.57s/it] 27%|██▋       | 274/1000 [26:01<1:00:52,  5.03s/it] 28%|██▊       | 275/1000 [26:11<1:16:45,  6.35s/it] 28%|██▊       | 276/1000 [26:16<1:13:27,  6.09s/it] 28%|██▊       | 277/1000 [26:21<1:07:40,  5.62s/it] 28%|██▊       | 278/1000 [26:27<1:10:07,  5.83s/it] 28%|██▊       | 279/1000 [26:31<1:02:59,  5.24s/it] 28%|██▊       | 280/1000 [26:38<1:09:01,  5.75s/it] 28%|██▊       | 281/1000 [26:45<1:14:33,  6.22s/it] 28%|██▊       | 282/1000 [26:49<1:06:29,  5.56s/it] 28%|██▊       | 283/1000 [26:58<1:20:20,  6.72s/it] 28%|██▊       | 284/1000 [27:07<1:27:21,  7.32s/it] 28%|██▊       | 285/1000 [27:10<1:11:22,  5.99s/it] 29%|██▊       | 286/1000 [27:15<1:08:44,  5.78s/it] 29%|██▊       | 287/1000 [27:23<1:13:56,  6.22s/it] 29%|██▉       | 288/1000 [27:26<1:04:53,  5.47s/it] 29%|██▉       | 289/1000 [27:30<58:07,  4.91s/it]   29%|██▉       | 290/1000 [27:35<57:31,  4.86s/it] 29%|██▉       | 291/1000 [27:39<57:11,  4.84s/it] 29%|██▉       | 292/1000 [27:42<50:20,  4.27s/it] 29%|██▉       | 293/1000 [27:49<1:00:16,  5.12s/it] 29%|██▉       | 294/1000 [27:54<59:04,  5.02s/it]   30%|██▉       | 295/1000 [27:58<54:40,  4.65s/it] 30%|██▉       | 296/1000 [28:03<55:38,  4.74s/it] 30%|██▉       | 297/1000 [28:09<59:43,  5.10s/it] 30%|██▉       | 298/1000 [28:15<1:01:49,  5.28s/it] 30%|██▉       | 299/1000 [28:23<1:13:56,  6.33s/it] 30%|███       | 300/1000 [28:32<1:22:52,  7.10s/it] 30%|███       | 301/1000 [28:39<1:22:25,  7.07s/it] 30%|███       | 302/1000 [28:46<1:20:46,  6.94s/it] 30%|███       | 303/1000 [28:50<1:10:14,  6.05s/it] 30%|███       | 304/1000 [28:54<1:03:20,  5.46s/it] 30%|███       | 305/1000 [28:58<56:34,  4.88s/it]   31%|███       | 306/1000 [29:03<59:10,  5.12s/it] 31%|███       | 307/1000 [29:06<49:22,  4.27s/it] 31%|███       | 308/1000 [29:11<54:32,  4.73s/it] 31%|███       | 309/1000 [29:14<47:56,  4.16s/it] 31%|███       | 310/1000 [29:21<57:52,  5.03s/it] 31%|███       | 311/1000 [29:27<59:20,  5.17s/it] 31%|███       | 312/1000 [29:30<54:13,  4.73s/it] 31%|███▏      | 313/1000 [29:35<52:12,  4.56s/it] 31%|███▏      | 314/1000 [29:37<45:27,  3.98s/it] 32%|███▏      | 315/1000 [29:41<44:20,  3.88s/it] 32%|███▏      | 316/1000 [29:47<52:43,  4.63s/it] 32%|███▏      | 317/1000 [29:51<49:07,  4.32s/it] 32%|███▏      | 318/1000 [29:55<50:15,  4.42s/it] 32%|███▏      | 319/1000 [30:00<51:00,  4.49s/it] 32%|███▏      | 320/1000 [30:13<1:18:35,  6.93s/it] 32%|███▏      | 321/1000 [30:18<1:14:14,  6.56s/it] 32%|███▏      | 322/1000 [30:28<1:24:34,  7.48s/it] 32%|███▏      | 323/1000 [30:36<1:24:29,  7.49s/it] 32%|███▏      | 324/1000 [30:40<1:15:05,  6.66s/it] 32%|███▎      | 325/1000 [30:44<1:04:46,  5.76s/it] 33%|███▎      | 326/1000 [30:50<1:05:20,  5.82s/it] 33%|███▎      | 327/1000 [30:53<56:05,  5.00s/it]   33%|███▎      | 328/1000 [30:59<59:32,  5.32s/it] 33%|███▎      | 329/1000 [31:04<57:21,  5.13s/it] 33%|███▎      | 330/1000 [31:22<1:40:15,  8.98s/it] 33%|███▎      | 331/1000 [31:28<1:29:56,  8.07s/it] 33%|███▎      | 332/1000 [31:31<1:15:30,  6.78s/it] 33%|███▎      | 333/1000 [31:39<1:19:15,  7.13s/it] 33%|███▎      | 334/1000 [31:47<1:19:46,  7.19s/it] 34%|███▎      | 335/1000 [31:50<1:06:43,  6.02s/it] 34%|███▎      | 336/1000 [31:58<1:12:22,  6.54s/it] 34%|███▎      | 337/1000 [32:01<1:01:20,  5.55s/it] 34%|███▍      | 338/1000 [32:07<1:02:32,  5.67s/it] 34%|███▍      | 339/1000 [32:12<1:01:16,  5.56s/it] 34%|███▍      | 340/1000 [32:20<1:09:21,  6.30s/it] 34%|███▍      | 341/1000 [32:26<1:06:23,  6.04s/it] 34%|███▍      | 342/1000 [32:33<1:09:10,  6.31s/it] 34%|███▍      | 343/1000 [32:37<1:02:27,  5.70s/it] 34%|███▍      | 344/1000 [32:42<1:01:32,  5.63s/it] 34%|███▍      | 345/1000 [32:46<54:00,  4.95s/it]   35%|███▍      | 346/1000 [32:49<47:07,  4.32s/it] 35%|███▍      | 347/1000 [32:53<46:09,  4.24s/it] 35%|███▍      | 348/1000 [32:56<41:31,  3.82s/it] 35%|███▍      | 349/1000 [33:00<42:17,  3.90s/it] 35%|███▌      | 350/1000 [33:14<1:16:39,  7.08s/it] 35%|███▌      | 351/1000 [33:22<1:18:54,  7.30s/it] 35%|███▌      | 352/1000 [33:26<1:09:01,  6.39s/it] 35%|███▌      | 353/1000 [33:34<1:11:58,  6.67s/it] 35%|███▌      | 354/1000 [33:38<1:05:26,  6.08s/it] 36%|███▌      | 355/1000 [33:43<1:00:28,  5.63s/it] 36%|███▌      | 356/1000 [33:49<1:03:30,  5.92s/it] 36%|███▌      | 357/1000 [33:54<58:12,  5.43s/it]   36%|███▌      | 358/1000 [33:59<58:34,  5.47s/it] 36%|███▌      | 359/1000 [34:06<1:01:46,  5.78s/it] 36%|███▌      | 360/1000 [34:10<57:25,  5.38s/it]   36%|███▌      | 361/1000 [34:17<1:02:52,  5.90s/it] 36%|███▌      | 362/1000 [34:22<1:00:01,  5.65s/it] 36%|███▋      | 363/1000 [34:27<57:41,  5.43s/it]   36%|███▋      | 364/1000 [34:34<1:02:34,  5.90s/it] 36%|███▋      | 365/1000 [34:40<1:01:46,  5.84s/it] 37%|███▋      | 366/1000 [34:44<57:04,  5.40s/it]   37%|███▋      | 367/1000 [34:53<1:07:46,  6.42s/it] 37%|███▋      | 368/1000 [34:58<1:01:25,  5.83s/it] 37%|███▋      | 369/1000 [35:03<1:00:34,  5.76s/it] 37%|███▋      | 370/1000 [35:11<1:08:08,  6.49s/it] 37%|███▋      | 371/1000 [35:15<59:02,  5.63s/it]   37%|███▋      | 372/1000 [35:21<1:01:04,  5.84s/it] 37%|███▋      | 373/1000 [35:26<57:24,  5.49s/it]   37%|███▋      | 374/1000 [35:32<59:52,  5.74s/it] 38%|███▊      | 375/1000 [35:37<57:14,  5.50s/it] 38%|███▊      | 376/1000 [35:43<58:48,  5.66s/it] 38%|███▊      | 377/1000 [35:46<49:38,  4.78s/it] 38%|███▊      | 378/1000 [35:51<50:44,  4.89s/it] 38%|███▊      | 379/1000 [35:53<40:32,  3.92s/it] 38%|███▊      | 380/1000 [35:59<46:26,  4.49s/it] 38%|███▊      | 381/1000 [36:02<43:17,  4.20s/it] 38%|███▊      | 382/1000 [36:09<51:52,  5.04s/it] 38%|███▊      | 383/1000 [36:12<44:11,  4.30s/it] 38%|███▊      | 384/1000 [36:17<46:24,  4.52s/it] 38%|███▊      | 385/1000 [36:23<50:47,  4.95s/it] 39%|███▊      | 386/1000 [36:27<48:48,  4.77s/it] 39%|███▊      | 387/1000 [36:32<49:56,  4.89s/it] 39%|███▉      | 388/1000 [36:44<1:10:49,  6.94s/it] 39%|███▉      | 389/1000 [36:47<57:20,  5.63s/it]   39%|███▉      | 390/1000 [36:55<1:05:58,  6.49s/it] 39%|███▉      | 391/1000 [37:02<1:08:25,  6.74s/it] 39%|███▉      | 392/1000 [37:07<1:00:28,  5.97s/it] 39%|███▉      | 393/1000 [37:12<1:00:15,  5.96s/it] 39%|███▉      | 394/1000 [37:19<1:01:38,  6.10s/it] 40%|███▉      | 395/1000 [37:25<1:01:19,  6.08s/it] 40%|███▉      | 396/1000 [37:33<1:06:35,  6.62s/it] 40%|███▉      | 397/1000 [37:36<57:01,  5.67s/it]   40%|███▉      | 398/1000 [37:39<48:00,  4.78s/it] 40%|███▉      | 399/1000 [37:43<44:20,  4.43s/it] 40%|████      | 400/1000 [37:46<41:17,  4.13s/it] 40%|████      | 401/1000 [37:50<42:05,  4.22s/it] 40%|████      | 402/1000 [37:57<48:18,  4.85s/it] 40%|████      | 403/1000 [38:17<1:35:18,  9.58s/it] 40%|████      | 404/1000 [38:22<1:20:38,  8.12s/it] 40%|████      | 405/1000 [38:30<1:19:42,  8.04s/it] 41%|████      | 406/1000 [38:34<1:07:39,  6.83s/it] 41%|████      | 407/1000 [38:39<1:01:16,  6.20s/it] 41%|████      | 408/1000 [38:45<1:00:16,  6.11s/it] 41%|████      | 409/1000 [38:53<1:08:19,  6.94s/it] 41%|████      | 410/1000 [39:00<1:05:35,  6.67s/it] 41%|████      | 411/1000 [39:10<1:16:28,  7.79s/it] 41%|████      | 412/1000 [39:17<1:12:54,  7.44s/it] 41%|████▏     | 413/1000 [39:19<57:36,  5.89s/it]   41%|████▏     | 414/1000 [39:21<46:55,  4.80s/it] 42%|████▏     | 415/1000 [39:27<51:06,  5.24s/it] 42%|████▏     | 416/1000 [39:30<44:43,  4.59s/it] 42%|████▏     | 417/1000 [39:34<40:21,  4.15s/it] 42%|████▏     | 418/1000 [39:38<39:46,  4.10s/it] 42%|████▏     | 419/1000 [39:43<43:07,  4.45s/it] 42%|████▏     | 420/1000 [39:47<42:04,  4.35s/it] 42%|████▏     | 421/1000 [39:51<40:30,  4.20s/it] 42%|████▏     | 422/1000 [39:59<53:12,  5.52s/it] 42%|████▏     | 423/1000 [40:04<51:46,  5.38s/it] 42%|████▏     | 424/1000 [40:08<47:11,  4.92s/it] 42%|████▎     | 425/1000 [40:14<48:11,  5.03s/it] 43%|████▎     | 426/1000 [40:20<50:57,  5.33s/it] 43%|████▎     | 427/1000 [40:23<46:27,  4.86s/it] 43%|████▎     | 428/1000 [40:32<55:51,  5.86s/it] 43%|████▎     | 429/1000 [40:40<1:04:31,  6.78s/it] 43%|████▎     | 430/1000 [40:45<58:45,  6.19s/it]   43%|████▎     | 431/1000 [40:52<1:00:36,  6.39s/it] 43%|████▎     | 432/1000 [40:56<53:47,  5.68s/it]   43%|████▎     | 433/1000 [41:06<1:05:36,  6.94s/it] 43%|████▎     | 434/1000 [41:10<55:43,  5.91s/it]   44%|████▎     | 435/1000 [41:12<44:37,  4.74s/it] 44%|████▎     | 436/1000 [41:18<50:43,  5.40s/it] 44%|████▎     | 437/1000 [41:25<53:22,  5.69s/it] 44%|████▍     | 438/1000 [41:31<53:14,  5.68s/it] 44%|████▍     | 439/1000 [41:33<44:53,  4.80s/it] 44%|████▍     | 440/1000 [41:37<41:18,  4.43s/it] 44%|████▍     | 441/1000 [41:42<43:35,  4.68s/it] 44%|████▍     | 442/1000 [41:49<49:30,  5.32s/it] 44%|████▍     | 443/1000 [41:52<44:04,  4.75s/it] 44%|████▍     | 444/1000 [41:56<41:37,  4.49s/it] 44%|████▍     | 445/1000 [41:59<37:43,  4.08s/it] 45%|████▍     | 446/1000 [42:03<35:57,  3.89s/it] 45%|████▍     | 447/1000 [42:08<38:39,  4.19s/it] 45%|████▍     | 448/1000 [42:11<37:04,  4.03s/it] 45%|████▍     | 449/1000 [42:17<40:34,  4.42s/it] 45%|████▌     | 450/1000 [42:20<36:45,  4.01s/it] 45%|████▌     | 451/1000 [42:30<54:01,  5.90s/it] 45%|████▌     | 452/1000 [42:38<59:01,  6.46s/it] 45%|████▌     | 453/1000 [42:46<1:03:58,  7.02s/it] 45%|████▌     | 454/1000 [42:51<59:01,  6.49s/it]   46%|████▌     | 455/1000 [43:00<1:05:26,  7.20s/it] 46%|████▌     | 456/1000 [43:03<54:25,  6.00s/it]   46%|████▌     | 457/1000 [43:06<46:03,  5.09s/it] 46%|████▌     | 458/1000 [43:15<55:49,  6.18s/it] 46%|████▌     | 459/1000 [43:20<51:42,  5.74s/it] 46%|████▌     | 460/1000 [43:26<51:59,  5.78s/it] 46%|████▌     | 461/1000 [43:32<53:45,  5.98s/it] 46%|████▌     | 462/1000 [43:37<49:20,  5.50s/it] 46%|████▋     | 463/1000 [43:39<42:17,  4.72s/it] 46%|████▋     | 464/1000 [43:42<36:59,  4.14s/it] 46%|████▋     | 465/1000 [43:46<36:38,  4.11s/it] 47%|████▋     | 466/1000 [43:50<34:43,  3.90s/it] 47%|████▋     | 467/1000 [43:52<31:24,  3.54s/it] 47%|████▋     | 468/1000 [43:58<36:09,  4.08s/it] 47%|████▋     | 469/1000 [44:01<35:00,  3.95s/it] 47%|████▋     | 470/1000 [44:04<32:24,  3.67s/it] 47%|████▋     | 471/1000 [44:08<32:22,  3.67s/it] 47%|████▋     | 472/1000 [44:12<33:59,  3.86s/it] 47%|████▋     | 473/1000 [44:24<55:07,  6.28s/it] 47%|████▋     | 474/1000 [44:32<58:46,  6.70s/it] 48%|████▊     | 475/1000 [44:45<1:16:17,  8.72s/it] 48%|████▊     | 476/1000 [45:00<1:30:25, 10.35s/it] 48%|████▊     | 477/1000 [45:07<1:21:51,  9.39s/it] 48%|████▊     | 478/1000 [45:14<1:16:20,  8.77s/it] 48%|████▊     | 479/1000 [45:29<1:33:09, 10.73s/it] 48%|████▊     | 480/1000 [45:38<1:27:22, 10.08s/it] 48%|████▊     | 481/1000 [45:43<1:14:34,  8.62s/it] 48%|████▊     | 482/1000 [45:48<1:04:21,  7.45s/it] 48%|████▊     | 483/1000 [45:59<1:14:17,  8.62s/it] 48%|████▊     | 484/1000 [46:05<1:08:01,  7.91s/it] 48%|████▊     | 485/1000 [46:10<58:38,  6.83s/it]   49%|████▊     | 486/1000 [46:14<51:54,  6.06s/it] 49%|████▊     | 487/1000 [46:20<51:27,  6.02s/it] 49%|████▉     | 488/1000 [46:23<44:27,  5.21s/it] 49%|████▉     | 489/1000 [46:29<44:30,  5.23s/it] 49%|████▉     | 490/1000 [46:37<51:34,  6.07s/it] 49%|████▉     | 491/1000 [46:45<57:27,  6.77s/it] 49%|████▉     | 492/1000 [46:49<51:26,  6.08s/it] 49%|████▉     | 493/1000 [46:52<43:02,  5.09s/it] 49%|████▉     | 494/1000 [46:55<37:50,  4.49s/it] 50%|████▉     | 495/1000 [46:58<33:24,  3.97s/it] 50%|████▉     | 496/1000 [47:00<29:04,  3.46s/it] 50%|████▉     | 497/1000 [47:04<30:22,  3.62s/it] 50%|████▉     | 498/1000 [47:08<30:57,  3.70s/it] 50%|████▉     | 499/1000 [47:11<28:58,  3.47s/it] 50%|█████     | 500/1000 [47:15<29:33,  3.55s/it] 50%|█████     | 501/1000 [47:19<30:42,  3.69s/it] 50%|█████     | 502/1000 [47:24<34:45,  4.19s/it] 50%|█████     | 503/1000 [47:41<1:05:34,  7.92s/it] 50%|█████     | 504/1000 [47:49<1:06:44,  8.07s/it] 50%|█████     | 505/1000 [47:54<59:17,  7.19s/it]   51%|█████     | 506/1000 [48:02<59:16,  7.20s/it] 51%|█████     | 507/1000 [48:06<51:38,  6.29s/it] 51%|█████     | 508/1000 [48:13<54:07,  6.60s/it] 51%|█████     | 509/1000 [48:17<47:30,  5.80s/it] 51%|█████     | 510/1000 [48:21<42:16,  5.18s/it] 51%|█████     | 511/1000 [48:26<43:09,  5.30s/it] 51%|█████     | 512/1000 [48:31<42:19,  5.20s/it] 51%|█████▏    | 513/1000 [48:35<39:35,  4.88s/it] 51%|█████▏    | 514/1000 [48:50<1:03:39,  7.86s/it] 52%|█████▏    | 515/1000 [48:57<1:00:29,  7.48s/it] 52%|█████▏    | 516/1000 [49:04<59:42,  7.40s/it]   52%|█████▏    | 517/1000 [49:10<55:32,  6.90s/it] 52%|█████▏    | 518/1000 [49:14<47:48,  5.95s/it] 52%|█████▏    | 519/1000 [49:19<46:05,  5.75s/it] 52%|█████▏    | 520/1000 [49:23<43:10,  5.40s/it] 52%|█████▏    | 521/1000 [49:28<41:53,  5.25s/it] 52%|█████▏    | 522/1000 [49:33<39:30,  4.96s/it] 52%|█████▏    | 523/1000 [49:38<39:26,  4.96s/it] 52%|█████▏    | 524/1000 [49:46<47:54,  6.04s/it] 52%|█████▎    | 525/1000 [49:51<44:57,  5.68s/it] 53%|█████▎    | 526/1000 [50:01<55:40,  7.05s/it] 53%|█████▎    | 527/1000 [50:12<1:04:10,  8.14s/it] 53%|█████▎    | 528/1000 [50:17<55:43,  7.08s/it]   53%|█████▎    | 529/1000 [50:21<49:40,  6.33s/it] 53%|█████▎    | 530/1000 [50:28<50:58,  6.51s/it] 53%|█████▎    | 531/1000 [50:33<46:30,  5.95s/it] 53%|█████▎    | 532/1000 [50:39<47:33,  6.10s/it] 53%|█████▎    | 533/1000 [50:47<52:08,  6.70s/it] 53%|█████▎    | 534/1000 [50:52<47:15,  6.08s/it] 54%|█████▎    | 535/1000 [51:01<53:14,  6.87s/it] 54%|█████▎    | 536/1000 [51:04<45:40,  5.91s/it] 54%|█████▎    | 537/1000 [51:08<41:33,  5.39s/it] 54%|█████▍    | 538/1000 [51:13<40:31,  5.26s/it] 54%|█████▍    | 539/1000 [51:17<37:21,  4.86s/it] 54%|█████▍    | 540/1000 [51:21<35:36,  4.65s/it] 54%|█████▍    | 541/1000 [51:27<37:59,  4.97s/it] 54%|█████▍    | 542/1000 [51:31<36:02,  4.72s/it] 54%|█████▍    | 543/1000 [51:36<36:51,  4.84s/it] 54%|█████▍    | 544/1000 [51:40<34:31,  4.54s/it] 55%|█████▍    | 545/1000 [51:45<34:56,  4.61s/it] 55%|█████▍    | 546/1000 [51:50<35:37,  4.71s/it] 55%|█████▍    | 547/1000 [51:58<42:07,  5.58s/it] 55%|█████▍    | 548/1000 [52:08<52:52,  7.02s/it] 55%|█████▍    | 549/1000 [52:14<51:27,  6.85s/it] 55%|█████▌    | 550/1000 [52:22<52:05,  6.95s/it] 55%|█████▌    | 551/1000 [52:29<53:17,  7.12s/it] 55%|█████▌    | 552/1000 [52:39<58:57,  7.90s/it] 55%|█████▌    | 553/1000 [52:45<54:24,  7.30s/it] 55%|█████▌    | 554/1000 [52:50<49:19,  6.63s/it] 56%|█████▌    | 555/1000 [52:54<43:46,  5.90s/it] 56%|█████▌    | 556/1000 [52:57<37:17,  5.04s/it] 56%|█████▌    | 557/1000 [53:01<34:26,  4.67s/it] 56%|█████▌    | 558/1000 [53:05<32:42,  4.44s/it] 56%|█████▌    | 559/1000 [53:08<29:22,  4.00s/it] 56%|█████▌    | 560/1000 [53:12<31:00,  4.23s/it] 56%|█████▌    | 561/1000 [53:16<29:17,  4.00s/it] 56%|█████▌    | 562/1000 [53:22<34:37,  4.74s/it] 56%|█████▋    | 563/1000 [53:27<34:39,  4.76s/it] 56%|█████▋    | 564/1000 [53:32<34:08,  4.70s/it] 56%|█████▋    | 565/1000 [53:36<32:15,  4.45s/it] 57%|█████▋    | 566/1000 [53:39<30:49,  4.26s/it] 57%|█████▋    | 567/1000 [53:44<30:25,  4.22s/it] 57%|█████▋    | 568/1000 [53:48<30:48,  4.28s/it] 57%|█████▋    | 569/1000 [53:58<43:43,  6.09s/it] 57%|█████▋    | 570/1000 [54:08<50:58,  7.11s/it] 57%|█████▋    | 571/1000 [54:13<46:50,  6.55s/it] 57%|█████▋    | 572/1000 [54:19<45:18,  6.35s/it] 57%|█████▋    | 573/1000 [54:25<44:31,  6.26s/it] 57%|█████▋    | 574/1000 [54:30<41:15,  5.81s/it] 57%|█████▊    | 575/1000 [54:34<36:55,  5.21s/it] 58%|█████▊    | 576/1000 [54:38<34:19,  4.86s/it] 58%|█████▊    | 577/1000 [54:41<30:37,  4.34s/it] 58%|█████▊    | 578/1000 [54:43<27:08,  3.86s/it] 58%|█████▊    | 579/1000 [54:50<33:00,  4.70s/it] 58%|█████▊    | 580/1000 [54:55<32:50,  4.69s/it] 58%|█████▊    | 581/1000 [55:03<39:30,  5.66s/it] 58%|█████▊    | 582/1000 [55:08<39:26,  5.66s/it] 58%|█████▊    | 583/1000 [55:17<44:49,  6.45s/it] 58%|█████▊    | 584/1000 [55:20<39:16,  5.66s/it] 58%|█████▊    | 585/1000 [55:28<43:19,  6.26s/it] 59%|█████▊    | 586/1000 [55:31<35:24,  5.13s/it] 59%|█████▊    | 587/1000 [55:34<31:22,  4.56s/it] 59%|█████▉    | 588/1000 [55:37<28:14,  4.11s/it] 59%|█████▉    | 589/1000 [55:41<27:18,  3.99s/it] 59%|█████▉    | 590/1000 [55:44<25:11,  3.69s/it] 59%|█████▉    | 591/1000 [55:48<27:08,  3.98s/it] 59%|█████▉    | 592/1000 [55:54<30:29,  4.48s/it] 59%|█████▉    | 593/1000 [55:57<28:17,  4.17s/it] 59%|█████▉    | 594/1000 [56:02<29:38,  4.38s/it] 60%|█████▉    | 595/1000 [56:05<25:50,  3.83s/it] 60%|█████▉    | 596/1000 [56:10<27:53,  4.14s/it] 60%|█████▉    | 597/1000 [56:15<29:31,  4.40s/it] 60%|█████▉    | 598/1000 [56:26<43:26,  6.48s/it] 60%|█████▉    | 599/1000 [56:29<36:12,  5.42s/it] 60%|██████    | 600/1000 [56:33<34:23,  5.16s/it] 60%|██████    | 601/1000 [56:39<35:07,  5.28s/it] 60%|██████    | 602/1000 [56:45<36:52,  5.56s/it] 60%|██████    | 603/1000 [56:50<34:50,  5.27s/it] 60%|██████    | 604/1000 [56:53<31:31,  4.78s/it] 60%|██████    | 605/1000 [56:59<31:58,  4.86s/it] 61%|██████    | 606/1000 [57:05<34:10,  5.20s/it] 61%|██████    | 607/1000 [57:18<51:15,  7.83s/it] 61%|██████    | 608/1000 [57:24<47:24,  7.26s/it] 61%|██████    | 609/1000 [57:27<38:31,  5.91s/it] 61%|██████    | 610/1000 [57:31<34:16,  5.27s/it] 61%|██████    | 611/1000 [57:34<29:42,  4.58s/it] 61%|██████    | 612/1000 [57:38<28:50,  4.46s/it] 61%|██████▏   | 613/1000 [57:42<27:25,  4.25s/it] 61%|██████▏   | 614/1000 [57:48<31:20,  4.87s/it] 62%|██████▏   | 615/1000 [57:52<28:22,  4.42s/it] 62%|██████▏   | 616/1000 [57:56<28:59,  4.53s/it] 62%|██████▏   | 617/1000 [58:00<26:36,  4.17s/it] 62%|██████▏   | 618/1000 [58:03<25:46,  4.05s/it] 62%|██████▏   | 619/1000 [58:09<28:03,  4.42s/it] 62%|██████▏   | 620/1000 [58:12<26:41,  4.21s/it] 62%|██████▏   | 621/1000 [58:17<26:33,  4.21s/it] 62%|██████▏   | 622/1000 [58:20<24:47,  3.93s/it] 62%|██████▏   | 623/1000 [58:36<47:22,  7.54s/it] 62%|██████▏   | 624/1000 [58:40<40:25,  6.45s/it] 62%|██████▎   | 625/1000 [58:49<45:52,  7.34s/it] 63%|██████▎   | 626/1000 [58:55<42:08,  6.76s/it] 63%|██████▎   | 627/1000 [58:59<37:34,  6.05s/it] 63%|██████▎   | 628/1000 [59:03<34:14,  5.52s/it] 63%|██████▎   | 629/1000 [59:10<37:02,  5.99s/it] 63%|██████▎   | 630/1000 [59:16<36:16,  5.88s/it] 63%|██████▎   | 631/1000 [59:22<36:22,  5.92s/it] 63%|██████▎   | 632/1000 [59:29<37:55,  6.18s/it] 63%|██████▎   | 633/1000 [59:32<33:00,  5.40s/it] 63%|██████▎   | 634/1000 [59:42<39:51,  6.53s/it] 64%|██████▎   | 635/1000 [59:45<34:38,  5.69s/it] 64%|██████▎   | 636/1000 [59:48<29:55,  4.93s/it] 64%|██████▎   | 637/1000 [59:52<28:08,  4.65s/it] 64%|██████▍   | 638/1000 [59:57<26:59,  4.47s/it] 64%|██████▍   | 639/1000 [1:00:02<28:33,  4.75s/it] 64%|██████▍   | 640/1000 [1:00:10<34:55,  5.82s/it] 64%|██████▍   | 641/1000 [1:00:18<38:45,  6.48s/it] 64%|██████▍   | 642/1000 [1:00:24<36:59,  6.20s/it] 64%|██████▍   | 643/1000 [1:00:27<32:08,  5.40s/it] 64%|██████▍   | 644/1000 [1:00:35<36:20,  6.12s/it] 64%|██████▍   | 645/1000 [1:00:39<31:45,  5.37s/it] 65%|██████▍   | 646/1000 [1:00:46<34:46,  5.90s/it] 65%|██████▍   | 647/1000 [1:00:49<30:21,  5.16s/it] 65%|██████▍   | 648/1000 [1:01:03<44:51,  7.65s/it] 65%|██████▍   | 649/1000 [1:01:07<38:23,  6.56s/it] 65%|██████▌   | 650/1000 [1:01:20<49:09,  8.43s/it] 65%|██████▌   | 651/1000 [1:01:23<39:58,  6.87s/it] 65%|██████▌   | 652/1000 [1:01:28<36:28,  6.29s/it] 65%|██████▌   | 653/1000 [1:01:38<42:58,  7.43s/it] 65%|██████▌   | 654/1000 [1:01:42<36:32,  6.34s/it] 66%|██████▌   | 655/1000 [1:01:48<36:05,  6.28s/it] 66%|██████▌   | 656/1000 [1:01:51<29:58,  5.23s/it] 66%|██████▌   | 657/1000 [1:01:57<31:22,  5.49s/it] 66%|██████▌   | 658/1000 [1:02:01<28:51,  5.06s/it] 66%|██████▌   | 659/1000 [1:02:03<24:53,  4.38s/it] 66%|██████▌   | 660/1000 [1:02:07<23:45,  4.19s/it] 66%|██████▌   | 661/1000 [1:02:13<26:28,  4.69s/it] 66%|██████▌   | 662/1000 [1:02:17<24:47,  4.40s/it] 66%|██████▋   | 663/1000 [1:02:21<24:13,  4.31s/it] 66%|██████▋   | 664/1000 [1:02:30<31:42,  5.66s/it] 66%|██████▋   | 665/1000 [1:02:31<25:00,  4.48s/it] 67%|██████▋   | 666/1000 [1:02:38<27:42,  4.98s/it] 67%|██████▋   | 667/1000 [1:02:41<25:41,  4.63s/it] 67%|██████▋   | 668/1000 [1:02:46<25:01,  4.52s/it] 67%|██████▋   | 669/1000 [1:02:48<21:37,  3.92s/it] 67%|██████▋   | 670/1000 [1:02:53<23:20,  4.24s/it] 67%|██████▋   | 671/1000 [1:02:55<19:46,  3.61s/it] 67%|██████▋   | 672/1000 [1:02:59<20:07,  3.68s/it] 67%|██████▋   | 673/1000 [1:03:02<19:08,  3.51s/it] 67%|██████▋   | 674/1000 [1:03:07<21:24,  3.94s/it] 68%|██████▊   | 675/1000 [1:03:11<21:21,  3.94s/it] 68%|██████▊   | 676/1000 [1:03:17<24:36,  4.56s/it] 68%|██████▊   | 677/1000 [1:03:19<19:44,  3.67s/it] 68%|██████▊   | 678/1000 [1:03:22<19:23,  3.61s/it] 68%|██████▊   | 679/1000 [1:03:30<25:26,  4.76s/it] 68%|██████▊   | 680/1000 [1:03:34<23:58,  4.50s/it] 68%|██████▊   | 681/1000 [1:03:41<28:27,  5.35s/it] 68%|██████▊   | 682/1000 [1:03:47<29:11,  5.51s/it] 68%|██████▊   | 683/1000 [1:03:54<31:38,  5.99s/it] 68%|██████▊   | 684/1000 [1:04:00<31:08,  5.91s/it] 68%|██████▊   | 685/1000 [1:04:09<36:46,  7.00s/it] 69%|██████▊   | 686/1000 [1:04:12<30:13,  5.78s/it] 69%|██████▊   | 687/1000 [1:04:21<34:31,  6.62s/it] 69%|██████▉   | 688/1000 [1:04:26<33:04,  6.36s/it] 69%|██████▉   | 689/1000 [1:04:38<41:00,  7.91s/it] 69%|██████▉   | 690/1000 [1:04:43<36:38,  7.09s/it] 69%|██████▉   | 691/1000 [1:04:45<29:10,  5.66s/it] 69%|██████▉   | 692/1000 [1:04:49<26:04,  5.08s/it] 69%|██████▉   | 693/1000 [1:04:52<22:50,  4.46s/it] 69%|██████▉   | 694/1000 [1:04:55<20:26,  4.01s/it] 70%|██████▉   | 695/1000 [1:04:58<19:11,  3.78s/it] 70%|██████▉   | 696/1000 [1:05:01<17:11,  3.39s/it] 70%|██████▉   | 697/1000 [1:05:07<20:47,  4.12s/it] 70%|██████▉   | 698/1000 [1:05:10<19:02,  3.78s/it] 70%|██████▉   | 699/1000 [1:05:14<20:18,  4.05s/it] 70%|███████   | 700/1000 [1:05:18<19:48,  3.96s/it] 70%|███████   | 701/1000 [1:05:21<18:39,  3.74s/it] 70%|███████   | 702/1000 [1:05:23<14:50,  2.99s/it] 70%|███████   | 703/1000 [1:05:28<18:28,  3.73s/it] 70%|███████   | 704/1000 [1:05:35<23:03,  4.67s/it] 70%|███████   | 705/1000 [1:05:39<22:37,  4.60s/it] 71%|███████   | 706/1000 [1:05:43<21:10,  4.32s/it] 71%|███████   | 707/1000 [1:05:49<22:56,  4.70s/it] 71%|███████   | 708/1000 [1:05:53<22:35,  4.64s/it] 71%|███████   | 709/1000 [1:05:56<19:27,  4.01s/it] 71%|███████   | 710/1000 [1:06:02<22:14,  4.60s/it] 71%|███████   | 711/1000 [1:06:09<26:21,  5.47s/it] 71%|███████   | 712/1000 [1:06:13<24:36,  5.13s/it] 71%|███████▏  | 713/1000 [1:06:18<23:29,  4.91s/it] 71%|███████▏  | 714/1000 [1:06:22<22:29,  4.72s/it] 72%|███████▏  | 715/1000 [1:06:27<21:58,  4.63s/it] 72%|███████▏  | 716/1000 [1:06:31<20:57,  4.43s/it] 72%|███████▏  | 717/1000 [1:06:33<18:23,  3.90s/it] 72%|███████▏  | 718/1000 [1:06:38<20:02,  4.27s/it] 72%|███████▏  | 719/1000 [1:06:45<23:12,  4.95s/it] 72%|███████▏  | 720/1000 [1:06:53<27:49,  5.96s/it] 72%|███████▏  | 721/1000 [1:06:59<26:54,  5.78s/it] 72%|███████▏  | 722/1000 [1:07:05<28:04,  6.06s/it] 72%|███████▏  | 723/1000 [1:07:08<23:08,  5.01s/it] 72%|███████▏  | 724/1000 [1:07:17<28:40,  6.23s/it] 72%|███████▎  | 725/1000 [1:07:20<24:50,  5.42s/it] 73%|███████▎  | 726/1000 [1:07:25<23:57,  5.25s/it] 73%|███████▎  | 727/1000 [1:07:30<23:13,  5.10s/it] 73%|███████▎  | 728/1000 [1:07:34<20:58,  4.63s/it] 73%|███████▎  | 729/1000 [1:07:37<19:54,  4.41s/it] 73%|███████▎  | 730/1000 [1:07:40<17:52,  3.97s/it] 73%|███████▎  | 731/1000 [1:07:44<16:45,  3.74s/it] 73%|███████▎  | 732/1000 [1:07:48<16:59,  3.80s/it] 73%|███████▎  | 733/1000 [1:07:51<16:02,  3.60s/it] 73%|███████▎  | 734/1000 [1:07:58<21:24,  4.83s/it] 74%|███████▎  | 735/1000 [1:08:02<19:44,  4.47s/it] 74%|███████▎  | 736/1000 [1:08:05<17:03,  3.88s/it] 74%|███████▎  | 737/1000 [1:08:09<17:42,  4.04s/it] 74%|███████▍  | 738/1000 [1:08:12<16:57,  3.88s/it] 74%|███████▍  | 739/1000 [1:08:24<27:13,  6.26s/it] 74%|███████▍  | 740/1000 [1:08:37<34:56,  8.06s/it] 74%|███████▍  | 741/1000 [1:08:47<37:33,  8.70s/it] 74%|███████▍  | 742/1000 [1:08:55<36:30,  8.49s/it] 74%|███████▍  | 743/1000 [1:09:02<34:37,  8.08s/it] 74%|███████▍  | 744/1000 [1:09:09<33:29,  7.85s/it] 74%|███████▍  | 745/1000 [1:09:20<36:40,  8.63s/it] 75%|███████▍  | 746/1000 [1:09:29<37:42,  8.91s/it] 75%|███████▍  | 747/1000 [1:09:35<34:11,  8.11s/it] 75%|███████▍  | 748/1000 [1:09:46<37:44,  8.99s/it] 75%|███████▍  | 749/1000 [1:09:57<39:51,  9.53s/it] 75%|███████▌  | 750/1000 [1:10:10<43:15, 10.38s/it] 75%|███████▌  | 751/1000 [1:10:19<41:46, 10.07s/it] 75%|███████▌  | 752/1000 [1:10:24<35:38,  8.62s/it] 75%|███████▌  | 753/1000 [1:10:33<36:17,  8.81s/it] 75%|███████▌  | 754/1000 [1:10:46<40:30,  9.88s/it] 76%|███████▌  | 755/1000 [1:10:57<42:20, 10.37s/it] 76%|███████▌  | 756/1000 [1:11:05<38:42,  9.52s/it] 76%|███████▌  | 757/1000 [1:11:15<38:43,  9.56s/it] 76%|███████▌  | 758/1000 [1:11:20<34:02,  8.44s/it] 76%|███████▌  | 759/1000 [1:11:31<36:08,  9.00s/it] 76%|███████▌  | 760/1000 [1:11:43<40:33, 10.14s/it] 76%|███████▌  | 761/1000 [1:11:52<38:23,  9.64s/it] 76%|███████▌  | 762/1000 [1:12:03<39:35,  9.98s/it] 76%|███████▋  | 763/1000 [1:12:18<45:40, 11.56s/it] 76%|███████▋  | 764/1000 [1:12:24<39:20, 10.00s/it] 76%|███████▋  | 765/1000 [1:12:29<33:28,  8.55s/it] 77%|███████▋  | 766/1000 [1:12:37<32:21,  8.30s/it] 77%|███████▋  | 767/1000 [1:12:42<28:35,  7.36s/it] 77%|███████▋  | 768/1000 [1:12:49<27:04,  7.00s/it] 77%|███████▋  | 769/1000 [1:13:01<33:37,  8.74s/it] 77%|███████▋  | 770/1000 [1:13:06<29:14,  7.63s/it] 77%|███████▋  | 771/1000 [1:13:13<28:30,  7.47s/it] 77%|███████▋  | 772/1000 [1:13:21<28:22,  7.47s/it] 77%|███████▋  | 773/1000 [1:13:30<29:50,  7.89s/it] 77%|███████▋  | 774/1000 [1:13:47<40:25, 10.73s/it] 78%|███████▊  | 775/1000 [1:13:52<33:34,  8.95s/it] 78%|███████▊  | 776/1000 [1:14:02<34:56,  9.36s/it] 78%|███████▊  | 777/1000 [1:14:10<33:03,  8.89s/it] 78%|███████▊  | 778/1000 [1:14:19<33:11,  8.97s/it] 78%|███████▊  | 779/1000 [1:14:32<37:24, 10.16s/it] 78%|███████▊  | 780/1000 [1:14:39<33:14,  9.06s/it] 78%|███████▊  | 781/1000 [1:14:50<35:43,  9.79s/it] 78%|███████▊  | 782/1000 [1:15:01<36:27, 10.03s/it] 78%|███████▊  | 783/1000 [1:15:07<32:24,  8.96s/it] 78%|███████▊  | 784/1000 [1:15:13<28:40,  7.96s/it] 78%|███████▊  | 785/1000 [1:15:25<33:21,  9.31s/it] 79%|███████▊  | 786/1000 [1:15:30<28:16,  7.93s/it] 79%|███████▊  | 787/1000 [1:15:36<26:21,  7.43s/it] 79%|███████▉  | 788/1000 [1:15:46<29:00,  8.21s/it] 79%|███████▉  | 789/1000 [1:15:58<32:30,  9.24s/it] 79%|███████▉  | 790/1000 [1:16:05<29:46,  8.51s/it] 79%|███████▉  | 791/1000 [1:16:13<29:05,  8.35s/it] 79%|███████▉  | 792/1000 [1:16:19<26:46,  7.72s/it] 79%|███████▉  | 793/1000 [1:16:32<31:56,  9.26s/it] 79%|███████▉  | 794/1000 [1:16:41<31:54,  9.29s/it] 80%|███████▉  | 795/1000 [1:16:48<29:19,  8.58s/it] 80%|███████▉  | 796/1000 [1:17:03<35:42, 10.50s/it] 80%|███████▉  | 797/1000 [1:17:09<30:34,  9.04s/it] 80%|███████▉  | 798/1000 [1:17:14<27:05,  8.04s/it] 80%|███████▉  | 799/1000 [1:17:19<23:18,  6.96s/it] 80%|████████  | 800/1000 [1:17:23<20:45,  6.23s/it] 80%|████████  | 801/1000 [1:17:32<23:20,  7.04s/it] 80%|████████  | 802/1000 [1:17:40<24:08,  7.31s/it] 80%|████████  | 803/1000 [1:17:46<21:58,  6.69s/it] 80%|████████  | 804/1000 [1:17:51<20:51,  6.38s/it] 80%|████████  | 805/1000 [1:17:54<17:25,  5.36s/it] 81%|████████  | 806/1000 [1:17:57<14:27,  4.47s/it] 81%|████████  | 807/1000 [1:18:01<14:32,  4.52s/it] 81%|████████  | 808/1000 [1:18:05<13:27,  4.20s/it] 81%|████████  | 809/1000 [1:18:07<12:05,  3.80s/it] 81%|████████  | 810/1000 [1:18:11<11:45,  3.71s/it] 81%|████████  | 811/1000 [1:18:16<13:08,  4.17s/it] 81%|████████  | 812/1000 [1:18:24<16:11,  5.17s/it] 81%|████████▏ | 813/1000 [1:18:35<21:38,  6.94s/it] 81%|████████▏ | 814/1000 [1:18:43<23:03,  7.44s/it] 82%|████████▏ | 815/1000 [1:18:51<22:54,  7.43s/it] 82%|████████▏ | 816/1000 [1:18:57<21:55,  7.15s/it] 82%|████████▏ | 817/1000 [1:19:03<20:36,  6.75s/it] 82%|████████▏ | 818/1000 [1:19:08<19:04,  6.29s/it] 82%|████████▏ | 819/1000 [1:19:12<16:56,  5.61s/it] 82%|████████▏ | 820/1000 [1:19:15<14:34,  4.86s/it] 82%|████████▏ | 821/1000 [1:19:32<25:01,  8.39s/it] 82%|████████▏ | 822/1000 [1:19:37<22:10,  7.48s/it] 82%|████████▏ | 823/1000 [1:19:42<19:30,  6.61s/it] 82%|████████▏ | 824/1000 [1:19:47<17:58,  6.13s/it] 82%|████████▎ | 825/1000 [1:19:51<15:52,  5.44s/it] 83%|████████▎ | 826/1000 [1:19:54<14:01,  4.84s/it] 83%|████████▎ | 827/1000 [1:20:04<18:00,  6.24s/it] 83%|████████▎ | 828/1000 [1:20:12<19:23,  6.77s/it] 83%|████████▎ | 829/1000 [1:20:18<18:56,  6.65s/it] 83%|████████▎ | 830/1000 [1:20:26<19:54,  7.03s/it] 83%|████████▎ | 831/1000 [1:20:33<19:31,  6.93s/it] 83%|████████▎ | 832/1000 [1:20:44<22:42,  8.11s/it] 83%|████████▎ | 833/1000 [1:20:47<18:45,  6.74s/it] 83%|████████▎ | 834/1000 [1:20:51<16:19,  5.90s/it] 84%|████████▎ | 835/1000 [1:20:56<15:32,  5.65s/it] 84%|████████▎ | 836/1000 [1:21:00<13:32,  4.95s/it] 84%|████████▎ | 837/1000 [1:21:07<15:11,  5.59s/it] 84%|████████▍ | 838/1000 [1:21:09<12:30,  4.64s/it] 84%|████████▍ | 839/1000 [1:21:14<12:49,  4.78s/it] 84%|████████▍ | 840/1000 [1:21:22<15:16,  5.73s/it] 84%|████████▍ | 841/1000 [1:21:29<15:42,  5.92s/it] 84%|████████▍ | 842/1000 [1:21:32<13:31,  5.14s/it] 84%|████████▍ | 843/1000 [1:21:43<18:02,  6.89s/it] 84%|████████▍ | 844/1000 [1:21:48<16:34,  6.37s/it] 84%|████████▍ | 845/1000 [1:21:52<14:26,  5.59s/it] 85%|████████▍ | 846/1000 [1:21:56<13:42,  5.34s/it] 85%|████████▍ | 847/1000 [1:22:02<13:49,  5.42s/it] 85%|████████▍ | 848/1000 [1:22:07<13:11,  5.21s/it] 85%|████████▍ | 849/1000 [1:22:12<13:10,  5.24s/it] 85%|████████▌ | 850/1000 [1:22:17<12:37,  5.05s/it] 85%|████████▌ | 851/1000 [1:22:19<10:25,  4.20s/it] 85%|████████▌ | 852/1000 [1:22:21<08:55,  3.62s/it] 85%|████████▌ | 853/1000 [1:22:24<08:35,  3.51s/it] 85%|████████▌ | 854/1000 [1:22:29<08:57,  3.68s/it] 86%|████████▌ | 855/1000 [1:22:31<08:07,  3.36s/it] 86%|████████▌ | 856/1000 [1:22:35<08:35,  3.58s/it] 86%|████████▌ | 857/1000 [1:22:41<09:45,  4.09s/it] 86%|████████▌ | 858/1000 [1:22:49<12:38,  5.34s/it] 86%|████████▌ | 859/1000 [1:22:58<15:35,  6.63s/it] 86%|████████▌ | 860/1000 [1:23:06<16:23,  7.02s/it] 86%|████████▌ | 861/1000 [1:23:22<21:55,  9.46s/it] 86%|████████▌ | 862/1000 [1:23:26<18:35,  8.08s/it] 86%|████████▋ | 863/1000 [1:23:34<18:17,  8.01s/it] 86%|████████▋ | 864/1000 [1:23:43<18:44,  8.27s/it] 86%|████████▋ | 865/1000 [1:23:54<20:10,  8.96s/it] 87%|████████▋ | 866/1000 [1:24:03<20:00,  8.96s/it] 87%|████████▋ | 867/1000 [1:24:15<21:48,  9.84s/it] 87%|████████▋ | 868/1000 [1:24:21<19:21,  8.80s/it] 87%|████████▋ | 869/1000 [1:24:26<16:39,  7.63s/it] 87%|████████▋ | 870/1000 [1:24:30<14:29,  6.69s/it] 87%|████████▋ | 871/1000 [1:24:37<14:19,  6.66s/it] 87%|████████▋ | 872/1000 [1:24:50<18:24,  8.63s/it] 87%|████████▋ | 873/1000 [1:24:53<14:22,  6.79s/it] 87%|████████▋ | 874/1000 [1:24:56<12:23,  5.90s/it] 88%|████████▊ | 875/1000 [1:25:02<12:00,  5.77s/it] 88%|████████▊ | 876/1000 [1:25:05<10:09,  4.91s/it] 88%|████████▊ | 877/1000 [1:25:10<10:21,  5.05s/it] 88%|████████▊ | 878/1000 [1:25:12<08:20,  4.10s/it] 88%|████████▊ | 879/1000 [1:25:16<07:58,  3.96s/it] 88%|████████▊ | 880/1000 [1:25:24<10:21,  5.18s/it] 88%|████████▊ | 881/1000 [1:25:33<12:47,  6.45s/it] 88%|████████▊ | 882/1000 [1:25:38<11:57,  6.08s/it] 88%|████████▊ | 883/1000 [1:25:42<10:16,  5.27s/it] 88%|████████▊ | 884/1000 [1:25:46<09:31,  4.93s/it] 88%|████████▊ | 885/1000 [1:25:59<14:22,  7.50s/it] 89%|████████▊ | 886/1000 [1:26:07<14:27,  7.61s/it] 89%|████████▊ | 887/1000 [1:26:12<12:28,  6.62s/it] 89%|████████▉ | 888/1000 [1:26:17<11:37,  6.23s/it] 89%|████████▉ | 889/1000 [1:26:22<11:05,  5.99s/it] 89%|████████▉ | 890/1000 [1:26:32<13:03,  7.13s/it] 89%|████████▉ | 891/1000 [1:26:36<11:25,  6.29s/it] 89%|████████▉ | 892/1000 [1:26:43<11:19,  6.30s/it] 89%|████████▉ | 893/1000 [1:26:47<09:59,  5.60s/it] 89%|████████▉ | 894/1000 [1:26:52<09:32,  5.40s/it] 90%|████████▉ | 895/1000 [1:26:56<08:52,  5.07s/it] 90%|████████▉ | 896/1000 [1:27:00<08:21,  4.82s/it] 90%|████████▉ | 897/1000 [1:27:04<07:48,  4.55s/it] 90%|████████▉ | 898/1000 [1:27:12<09:21,  5.51s/it] 90%|████████▉ | 899/1000 [1:27:16<08:38,  5.14s/it] 90%|█████████ | 900/1000 [1:27:18<07:07,  4.27s/it] 90%|█████████ | 901/1000 [1:27:24<07:31,  4.56s/it] 90%|█████████ | 902/1000 [1:27:26<06:36,  4.05s/it] 90%|█████████ | 903/1000 [1:27:30<06:20,  3.92s/it] 90%|█████████ | 904/1000 [1:27:34<06:11,  3.87s/it] 90%|█████████ | 905/1000 [1:27:37<05:45,  3.64s/it] 91%|█████████ | 906/1000 [1:27:42<06:21,  4.06s/it] 91%|█████████ | 907/1000 [1:27:49<07:54,  5.10s/it] 91%|█████████ | 908/1000 [1:27:58<09:26,  6.16s/it] 91%|█████████ | 909/1000 [1:28:03<08:33,  5.64s/it] 91%|█████████ | 910/1000 [1:28:13<10:32,  7.02s/it] 91%|█████████ | 911/1000 [1:28:19<10:04,  6.80s/it] 91%|█████████ | 912/1000 [1:28:26<10:05,  6.88s/it] 91%|█████████▏| 913/1000 [1:28:32<09:41,  6.69s/it] 91%|█████████▏| 914/1000 [1:28:38<09:17,  6.48s/it] 92%|█████████▏| 915/1000 [1:28:43<08:14,  5.82s/it] 92%|█████████▏| 916/1000 [1:28:48<07:53,  5.63s/it] 92%|█████████▏| 917/1000 [1:28:57<09:24,  6.80s/it] 92%|█████████▏| 918/1000 [1:29:04<09:17,  6.80s/it] 92%|█████████▏| 919/1000 [1:29:09<08:23,  6.22s/it] 92%|█████████▏| 920/1000 [1:29:14<07:41,  5.77s/it] 92%|█████████▏| 921/1000 [1:29:21<08:01,  6.10s/it] 92%|█████████▏| 922/1000 [1:29:25<07:16,  5.60s/it] 92%|█████████▏| 923/1000 [1:29:29<06:22,  4.96s/it] 92%|█████████▏| 924/1000 [1:29:33<06:08,  4.85s/it] 92%|█████████▎| 925/1000 [1:29:37<05:38,  4.51s/it] 93%|█████████▎| 926/1000 [1:29:42<05:49,  4.72s/it] 93%|█████████▎| 927/1000 [1:29:50<07:00,  5.76s/it] 93%|█████████▎| 928/1000 [1:29:56<06:55,  5.76s/it] 93%|█████████▎| 929/1000 [1:30:03<07:23,  6.25s/it] 93%|█████████▎| 930/1000 [1:30:07<06:13,  5.33s/it] 93%|█████████▎| 931/1000 [1:30:15<07:11,  6.26s/it] 93%|█████████▎| 932/1000 [1:30:19<06:10,  5.45s/it] 93%|█████████▎| 933/1000 [1:30:24<06:03,  5.42s/it] 93%|█████████▎| 934/1000 [1:30:30<06:20,  5.77s/it] 94%|█████████▎| 935/1000 [1:30:35<05:56,  5.49s/it] 94%|█████████▎| 936/1000 [1:30:40<05:30,  5.16s/it] 94%|█████████▎| 937/1000 [1:30:47<06:05,  5.81s/it] 94%|█████████▍| 938/1000 [1:31:01<08:38,  8.36s/it] 94%|█████████▍| 939/1000 [1:31:05<07:07,  7.01s/it] 94%|█████████▍| 940/1000 [1:31:12<07:03,  7.06s/it] 94%|█████████▍| 941/1000 [1:31:20<07:06,  7.23s/it] 94%|█████████▍| 942/1000 [1:31:23<05:52,  6.08s/it] 94%|█████████▍| 943/1000 [1:31:28<05:13,  5.50s/it] 94%|█████████▍| 944/1000 [1:31:30<04:08,  4.43s/it] 94%|█████████▍| 945/1000 [1:31:34<04:08,  4.51s/it] 95%|█████████▍| 946/1000 [1:31:40<04:21,  4.84s/it] 95%|█████████▍| 947/1000 [1:31:44<04:11,  4.75s/it] 95%|█████████▍| 948/1000 [1:31:51<04:40,  5.40s/it] 95%|█████████▍| 949/1000 [1:32:01<05:38,  6.63s/it] 95%|█████████▌| 950/1000 [1:32:07<05:19,  6.40s/it] 95%|█████████▌| 951/1000 [1:32:14<05:30,  6.74s/it] 95%|█████████▌| 952/1000 [1:32:19<04:54,  6.13s/it] 95%|█████████▌| 953/1000 [1:32:25<04:47,  6.11s/it] 95%|█████████▌| 954/1000 [1:32:29<04:10,  5.44s/it] 96%|█████████▌| 955/1000 [1:32:37<04:43,  6.31s/it] 96%|█████████▌| 956/1000 [1:32:43<04:34,  6.25s/it] 96%|█████████▌| 957/1000 [1:32:47<04:01,  5.62s/it] 96%|█████████▌| 958/1000 [1:32:52<03:49,  5.46s/it] 96%|█████████▌| 959/1000 [1:32:56<03:18,  4.85s/it] 96%|█████████▌| 960/1000 [1:33:02<03:24,  5.11s/it] 96%|█████████▌| 961/1000 [1:33:06<03:13,  4.96s/it] 96%|█████████▌| 962/1000 [1:33:11<03:00,  4.76s/it] 96%|█████████▋| 963/1000 [1:33:18<03:22,  5.47s/it] 96%|█████████▋| 964/1000 [1:33:25<03:31,  5.88s/it] 96%|█████████▋| 965/1000 [1:33:32<03:40,  6.30s/it] 97%|█████████▋| 966/1000 [1:33:35<02:59,  5.28s/it] 97%|█████████▋| 967/1000 [1:33:37<02:21,  4.30s/it] 97%|█████████▋| 968/1000 [1:33:40<02:09,  4.06s/it] 97%|█████████▋| 969/1000 [1:33:43<01:57,  3.78s/it] 97%|█████████▋| 970/1000 [1:33:48<02:03,  4.11s/it] 97%|█████████▋| 971/1000 [1:33:51<01:50,  3.80s/it] 97%|█████████▋| 972/1000 [1:33:55<01:48,  3.88s/it] 97%|█████████▋| 973/1000 [1:33:59<01:45,  3.92s/it] 97%|█████████▋| 974/1000 [1:34:03<01:43,  3.98s/it] 98%|█████████▊| 975/1000 [1:34:11<02:02,  4.91s/it] 98%|█████████▊| 976/1000 [1:34:15<01:51,  4.66s/it] 98%|█████████▊| 977/1000 [1:34:27<02:40,  6.98s/it] 98%|█████████▊| 978/1000 [1:34:36<02:47,  7.64s/it] 98%|█████████▊| 979/1000 [1:34:46<02:50,  8.13s/it] 98%|█████████▊| 980/1000 [1:34:49<02:16,  6.84s/it] 98%|█████████▊| 981/1000 [1:34:54<01:55,  6.06s/it] 98%|█████████▊| 982/1000 [1:35:00<01:49,  6.06s/it] 98%|█████████▊| 983/1000 [1:35:10<02:03,  7.29s/it] 98%|█████████▊| 984/1000 [1:35:13<01:35,  5.97s/it] 98%|█████████▊| 985/1000 [1:35:20<01:34,  6.33s/it] 99%|█████████▊| 986/1000 [1:35:25<01:22,  5.92s/it] 99%|█████████▊| 987/1000 [1:35:31<01:19,  6.13s/it] 99%|█████████▉| 988/1000 [1:35:37<01:11,  5.97s/it] 99%|█████████▉| 989/1000 [1:35:43<01:04,  5.87s/it] 99%|█████████▉| 990/1000 [1:35:46<00:50,  5.07s/it] 99%|█████████▉| 991/1000 [1:35:53<00:51,  5.69s/it] 99%|█████████▉| 992/1000 [1:36:00<00:47,  5.94s/it] 99%|█████████▉| 993/1000 [1:36:04<00:39,  5.60s/it] 99%|█████████▉| 994/1000 [1:36:09<00:32,  5.39s/it]100%|█████████▉| 995/1000 [1:36:19<00:33,  6.61s/it]100%|█████████▉| 996/1000 [1:36:23<00:23,  5.85s/it]100%|█████████▉| 997/1000 [1:36:35<00:22,  7.65s/it]100%|█████████▉| 998/1000 [1:36:39<00:13,  6.73s/it]100%|█████████▉| 999/1000 [1:36:41<00:05,  5.40s/it]100%|██████████| 1000/1000 [1:36:44<00:00,  4.42s/it]100%|██████████| 1000/1000 [1:36:44<00:00,  5.80s/it]
