/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
ClipTestArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
attn_implementation=flash_attention_2,
augmentation=False,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
consecutive_n_frames_threshold=1,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
dataset_config=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
embed_mark=2fps_384_1+3x3,
end_idx=1000,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=no,
eval_use_gather_object=False,
evaluation_strategy=None,
finetune_modules=['connector', 'mm_projector', 'response_head', 'related_head'],
first_n_frames_no_generate=0,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
frame_fps=2.0,
frame_num_tokens=49,
frame_resolution=384,
frame_token_cls=False,
frame_token_pooled=[7, 7],
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
grounding_mode=False,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
input_dir=/share2/wangyq/projects/MMDuet/datasets/shot2story/videos,
is_online_model=True,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
live_version=test,
llm_pretrained=lmms-lab/llava-onevision-qwen2-7b-ov,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=outputs/debug/runs/Apr19_15-00-43_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lora_alpha=32,
lora_modules=model\.layers.*(q_proj|k_proj|v_proj|o_proj|gate_proj|up_proj|down_proj)$,
lora_pretrained=None,
lora_r=16,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_num_frames=400,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
no_output_before_user_input=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=outputs/debug,
output_fname=outputs/llava-ov/magqa/4sec.jsonl.0,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_assistant_turns=False,
remove_unused_columns=True,
repetition_penalty=None,
report_to=['tensorboard', 'wandb'],
response_min_interval_frames=None,
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=outputs/debug,
running_list_length=20,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
score_heads=informative_score,
seed=42,
skip_memory_metrics=True,
split_batches=None,
start_idx=0,
stream_end_prob_threshold=None,
stream_end_score_sum_threshold=None,
stream_loss_weight=1.0,
system_prompt=A multimodal AI assistant is helping users with some activities. Below is their conversation, interleaved with the list of video frames received by the assistant.,
test_fname=/share2/wangyq/projects/MMDuet/datasets/shot2story/annotations/magqa_test.json,
tf32=None,
threshold_z=None,
time_instruction_format=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
v_placeholder=<image>,
video_chunk_sec=4,
video_pooling_stride=4,
vision_pretrained=google/siglip-large-patch16-384,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Loaded LLaVA model: lmms-lab/llava-onevision-qwen2-7b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 420.67it/s]
Loading vision tower: google/siglip-so400m-patch14-384
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.patch_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.patch_embedding.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.position_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.probe: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.attention.in_proj_weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.attention.in_proj_bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.attention.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.attention.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:19<00:58, 19.51s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:40<00:41, 20.57s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:03<00:21, 21.52s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:09<00:00, 15.42s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:09<00:00, 17.38s/it]
Model Class: LlavaQwenForCausalLM
  0%|          | 0/1000 [00:00<?, ?it/s]/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:612: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  0%|          | 1/1000 [00:16<4:39:43, 16.80s/it]  0%|          | 2/1000 [00:23<3:04:55, 11.12s/it]  0%|          | 3/1000 [00:32<2:43:31,  9.84s/it]  0%|          | 4/1000 [00:38<2:18:37,  8.35s/it]  0%|          | 5/1000 [00:44<2:07:03,  7.66s/it]  1%|          | 6/1000 [00:54<2:21:08,  8.52s/it]  1%|          | 7/1000 [01:08<2:46:16, 10.05s/it]  1%|          | 8/1000 [01:11<2:11:07,  7.93s/it]  1%|          | 9/1000 [01:16<1:57:54,  7.14s/it]  1%|          | 10/1000 [01:22<1:51:24,  6.75s/it]  1%|          | 11/1000 [01:29<1:51:26,  6.76s/it]  1%|          | 12/1000 [01:33<1:38:12,  5.96s/it]  1%|▏         | 13/1000 [01:48<2:23:16,  8.71s/it]  1%|▏         | 14/1000 [01:56<2:19:08,  8.47s/it]  2%|▏         | 15/1000 [02:09<2:39:30,  9.72s/it]  2%|▏         | 16/1000 [02:16<2:28:39,  9.06s/it]  2%|▏         | 17/1000 [02:25<2:28:24,  9.06s/it]  2%|▏         | 18/1000 [02:36<2:34:09,  9.42s/it]  2%|▏         | 19/1000 [02:42<2:20:34,  8.60s/it]  2%|▏         | 20/1000 [02:52<2:25:49,  8.93s/it]  2%|▏         | 21/1000 [02:57<2:08:42,  7.89s/it]  2%|▏         | 22/1000 [03:06<2:10:21,  8.00s/it]  2%|▏         | 23/1000 [03:13<2:08:09,  7.87s/it]  2%|▏         | 24/1000 [03:19<1:57:53,  7.25s/it]  2%|▎         | 25/1000 [03:25<1:52:14,  6.91s/it]  3%|▎         | 26/1000 [03:33<1:55:36,  7.12s/it]  3%|▎         | 27/1000 [03:48<2:32:37,  9.41s/it]  3%|▎         | 28/1000 [03:56<2:29:12,  9.21s/it]  3%|▎         | 29/1000 [03:59<1:58:54,  7.35s/it]  3%|▎         | 30/1000 [04:08<2:05:57,  7.79s/it]  3%|▎         | 31/1000 [04:17<2:08:33,  7.96s/it]  3%|▎         | 32/1000 [04:19<1:44:14,  6.46s/it]  3%|▎         | 33/1000 [04:26<1:45:21,  6.54s/it]  3%|▎         | 34/1000 [04:30<1:34:11,  5.85s/it]  4%|▎         | 35/1000 [04:35<1:26:43,  5.39s/it]  4%|▎         | 36/1000 [04:39<1:20:46,  5.03s/it]  4%|▎         | 37/1000 [04:42<1:09:55,  4.36s/it]  4%|▍         | 38/1000 [04:48<1:18:04,  4.87s/it]  4%|▍         | 39/1000 [04:54<1:22:06,  5.13s/it]  4%|▍         | 40/1000 [05:05<1:54:11,  7.14s/it]  4%|▍         | 41/1000 [05:17<2:15:59,  8.51s/it]  4%|▍         | 42/1000 [05:22<2:00:57,  7.58s/it]  4%|▍         | 43/1000 [05:28<1:49:15,  6.85s/it]  4%|▍         | 44/1000 [05:31<1:33:10,  5.85s/it]  4%|▍         | 45/1000 [05:34<1:16:43,  4.82s/it]  5%|▍         | 46/1000 [05:36<1:03:50,  4.02s/it]  5%|▍         | 47/1000 [05:40<1:05:16,  4.11s/it]  5%|▍         | 48/1000 [05:44<1:02:39,  3.95s/it]  5%|▍         | 49/1000 [05:48<1:05:05,  4.11s/it]  5%|▌         | 50/1000 [05:52<1:01:45,  3.90s/it]  5%|▌         | 51/1000 [06:02<1:33:50,  5.93s/it]  5%|▌         | 52/1000 [06:07<1:28:56,  5.63s/it]  5%|▌         | 53/1000 [06:11<1:19:03,  5.01s/it]  5%|▌         | 54/1000 [06:15<1:14:45,  4.74s/it]  6%|▌         | 55/1000 [06:23<1:30:10,  5.73s/it]  6%|▌         | 56/1000 [06:27<1:22:59,  5.27s/it]  6%|▌         | 57/1000 [06:33<1:27:03,  5.54s/it]  6%|▌         | 58/1000 [06:37<1:18:51,  5.02s/it]  6%|▌         | 59/1000 [06:46<1:38:28,  6.28s/it]  6%|▌         | 60/1000 [06:52<1:36:41,  6.17s/it]  6%|▌         | 61/1000 [06:57<1:28:32,  5.66s/it]  6%|▌         | 62/1000 [07:03<1:34:20,  6.03s/it]  6%|▋         | 63/1000 [07:07<1:23:30,  5.35s/it]  6%|▋         | 64/1000 [07:11<1:14:00,  4.74s/it]  6%|▋         | 65/1000 [07:15<1:13:08,  4.69s/it]  7%|▋         | 66/1000 [07:18<1:04:07,  4.12s/it]  7%|▋         | 67/1000 [07:24<1:12:01,  4.63s/it]  7%|▋         | 68/1000 [07:27<1:05:26,  4.21s/it]  7%|▋         | 69/1000 [07:32<1:09:29,  4.48s/it]  7%|▋         | 70/1000 [07:37<1:12:09,  4.66s/it]  7%|▋         | 71/1000 [07:43<1:17:45,  5.02s/it]  7%|▋         | 72/1000 [07:46<1:10:15,  4.54s/it]  7%|▋         | 73/1000 [07:52<1:13:00,  4.73s/it]  7%|▋         | 74/1000 [07:57<1:18:16,  5.07s/it]  8%|▊         | 75/1000 [08:01<1:09:25,  4.50s/it]  8%|▊         | 76/1000 [08:06<1:11:40,  4.65s/it]  8%|▊         | 77/1000 [08:16<1:36:14,  6.26s/it]  8%|▊         | 78/1000 [08:20<1:27:10,  5.67s/it]  8%|▊         | 79/1000 [08:29<1:42:46,  6.70s/it]  8%|▊         | 80/1000 [08:33<1:31:20,  5.96s/it]  8%|▊         | 81/1000 [08:37<1:20:07,  5.23s/it]  8%|▊         | 82/1000 [08:40<1:12:17,  4.72s/it]  8%|▊         | 83/1000 [08:43<1:02:35,  4.10s/it]  8%|▊         | 84/1000 [08:47<1:03:02,  4.13s/it]  8%|▊         | 85/1000 [08:50<57:32,  3.77s/it]    9%|▊         | 86/1000 [08:53<53:20,  3.50s/it]  9%|▊         | 87/1000 [08:59<1:04:31,  4.24s/it]  9%|▉         | 88/1000 [09:05<1:10:50,  4.66s/it]  9%|▉         | 89/1000 [09:11<1:16:38,  5.05s/it]  9%|▉         | 90/1000 [09:15<1:13:04,  4.82s/it]  9%|▉         | 91/1000 [09:19<1:10:11,  4.63s/it]  9%|▉         | 92/1000 [09:23<1:08:58,  4.56s/it]  9%|▉         | 93/1000 [09:32<1:28:47,  5.87s/it]  9%|▉         | 94/1000 [09:37<1:22:36,  5.47s/it] 10%|▉         | 95/1000 [09:45<1:32:09,  6.11s/it] 10%|▉         | 96/1000 [09:49<1:23:30,  5.54s/it] 10%|▉         | 97/1000 [09:55<1:26:22,  5.74s/it] 10%|▉         | 98/1000 [10:01<1:29:50,  5.98s/it] 10%|▉         | 99/1000 [10:06<1:21:57,  5.46s/it] 10%|█         | 100/1000 [10:11<1:23:18,  5.55s/it] 10%|█         | 101/1000 [10:15<1:16:12,  5.09s/it] 10%|█         | 102/1000 [10:21<1:20:04,  5.35s/it] 10%|█         | 103/1000 [10:23<1:04:13,  4.30s/it] 10%|█         | 104/1000 [10:27<1:00:26,  4.05s/it] 10%|█         | 105/1000 [10:32<1:06:34,  4.46s/it] 11%|█         | 106/1000 [10:40<1:20:46,  5.42s/it] 11%|█         | 107/1000 [10:48<1:31:23,  6.14s/it] 11%|█         | 108/1000 [10:51<1:20:52,  5.44s/it] 11%|█         | 109/1000 [10:56<1:18:29,  5.29s/it] 11%|█         | 110/1000 [11:00<1:09:30,  4.69s/it] 11%|█         | 111/1000 [11:05<1:11:54,  4.85s/it] 11%|█         | 112/1000 [11:09<1:07:51,  4.59s/it] 11%|█▏        | 113/1000 [11:16<1:19:21,  5.37s/it] 11%|█▏        | 114/1000 [11:20<1:14:04,  5.02s/it] 12%|█▏        | 115/1000 [11:28<1:27:33,  5.94s/it] 12%|█▏        | 116/1000 [11:36<1:35:50,  6.50s/it] 12%|█▏        | 117/1000 [11:40<1:24:18,  5.73s/it] 12%|█▏        | 118/1000 [11:47<1:31:23,  6.22s/it] 12%|█▏        | 119/1000 [11:56<1:41:04,  6.88s/it] 12%|█▏        | 120/1000 [12:00<1:27:01,  5.93s/it] 12%|█▏        | 121/1000 [12:06<1:30:57,  6.21s/it] 12%|█▏        | 122/1000 [12:12<1:28:14,  6.03s/it] 12%|█▏        | 123/1000 [12:16<1:19:41,  5.45s/it] 12%|█▏        | 124/1000 [12:19<1:10:11,  4.81s/it] 12%|█▎        | 125/1000 [12:24<1:06:51,  4.58s/it] 13%|█▎        | 126/1000 [12:31<1:18:16,  5.37s/it] 13%|█▎        | 127/1000 [12:40<1:36:39,  6.64s/it] 13%|█▎        | 128/1000 [12:52<1:57:26,  8.08s/it] 13%|█▎        | 129/1000 [13:00<1:56:22,  8.02s/it] 13%|█▎        | 130/1000 [13:04<1:41:23,  6.99s/it] 13%|█▎        | 131/1000 [13:11<1:40:32,  6.94s/it] 13%|█▎        | 132/1000 [13:21<1:54:46,  7.93s/it] 13%|█▎        | 133/1000 [13:29<1:52:12,  7.77s/it] 13%|█▎        | 134/1000 [13:37<1:55:55,  8.03s/it] 14%|█▎        | 135/1000 [13:47<2:03:07,  8.54s/it] 14%|█▎        | 136/1000 [13:57<2:10:46,  9.08s/it] 14%|█▎        | 137/1000 [14:09<2:20:45,  9.79s/it] 14%|█▍        | 138/1000 [14:13<1:57:48,  8.20s/it] 14%|█▍        | 139/1000 [14:21<1:57:05,  8.16s/it] 14%|█▍        | 140/1000 [14:27<1:46:13,  7.41s/it] 14%|█▍        | 141/1000 [14:31<1:33:03,  6.50s/it] 14%|█▍        | 142/1000 [14:34<1:14:58,  5.24s/it] 14%|█▍        | 143/1000 [14:39<1:12:55,  5.11s/it] 14%|█▍        | 144/1000 [14:47<1:25:46,  6.01s/it] 14%|█▍        | 145/1000 [14:55<1:34:07,  6.60s/it] 15%|█▍        | 146/1000 [15:03<1:42:18,  7.19s/it] 15%|█▍        | 147/1000 [15:08<1:32:43,  6.52s/it] 15%|█▍        | 148/1000 [15:16<1:36:49,  6.82s/it] 15%|█▍        | 149/1000 [15:27<1:55:27,  8.14s/it] 15%|█▌        | 150/1000 [15:36<1:58:01,  8.33s/it] 15%|█▌        | 151/1000 [15:40<1:42:00,  7.21s/it] 15%|█▌        | 152/1000 [15:47<1:40:12,  7.09s/it] 15%|█▌        | 153/1000 [15:54<1:41:00,  7.16s/it] 15%|█▌        | 154/1000 [15:59<1:31:42,  6.50s/it] 16%|█▌        | 155/1000 [16:05<1:27:09,  6.19s/it] 16%|█▌        | 156/1000 [16:11<1:27:27,  6.22s/it] 16%|█▌        | 157/1000 [16:21<1:42:02,  7.26s/it] 16%|█▌        | 158/1000 [16:25<1:29:39,  6.39s/it] 16%|█▌        | 159/1000 [16:33<1:35:21,  6.80s/it] 16%|█▌        | 160/1000 [16:42<1:45:51,  7.56s/it] 16%|█▌        | 161/1000 [16:51<1:49:43,  7.85s/it] 16%|█▌        | 162/1000 [16:55<1:36:15,  6.89s/it] 16%|█▋        | 163/1000 [17:07<1:57:28,  8.42s/it] 16%|█▋        | 164/1000 [17:13<1:45:07,  7.55s/it] 16%|█▋        | 165/1000 [17:19<1:37:23,  7.00s/it] 17%|█▋        | 166/1000 [17:22<1:23:40,  6.02s/it] 17%|█▋        | 167/1000 [17:26<1:14:37,  5.38s/it] 17%|█▋        | 168/1000 [17:31<1:11:24,  5.15s/it] 17%|█▋        | 169/1000 [17:38<1:21:14,  5.87s/it] 17%|█▋        | 170/1000 [17:48<1:34:35,  6.84s/it] 17%|█▋        | 171/1000 [17:54<1:32:00,  6.66s/it] 17%|█▋        | 172/1000 [17:59<1:24:15,  6.11s/it] 17%|█▋        | 173/1000 [18:06<1:28:05,  6.39s/it] 17%|█▋        | 174/1000 [18:12<1:26:26,  6.28s/it] 18%|█▊        | 175/1000 [18:16<1:19:30,  5.78s/it] 18%|█▊        | 176/1000 [18:20<1:11:46,  5.23s/it] 18%|█▊        | 177/1000 [18:24<1:03:39,  4.64s/it] 18%|█▊        | 178/1000 [18:28<1:03:14,  4.62s/it] 18%|█▊        | 179/1000 [18:31<57:31,  4.20s/it]   18%|█▊        | 180/1000 [18:36<1:00:17,  4.41s/it] 18%|█▊        | 181/1000 [18:39<52:48,  3.87s/it]   18%|█▊        | 182/1000 [18:43<55:17,  4.06s/it] 18%|█▊        | 183/1000 [18:47<55:09,  4.05s/it] 18%|█▊        | 184/1000 [18:50<47:57,  3.53s/it] 18%|█▊        | 185/1000 [18:56<59:14,  4.36s/it] 19%|█▊        | 186/1000 [18:59<55:32,  4.09s/it] 19%|█▊        | 187/1000 [19:04<56:08,  4.14s/it] 19%|█▉        | 188/1000 [19:09<1:02:30,  4.62s/it] 19%|█▉        | 189/1000 [19:13<57:42,  4.27s/it]   19%|█▉        | 190/1000 [19:18<1:00:57,  4.52s/it] 19%|█▉        | 191/1000 [19:22<59:46,  4.43s/it]   19%|█▉        | 192/1000 [19:28<1:04:42,  4.80s/it] 19%|█▉        | 193/1000 [19:33<1:06:03,  4.91s/it] 19%|█▉        | 194/1000 [19:38<1:05:08,  4.85s/it] 20%|█▉        | 195/1000 [19:45<1:14:58,  5.59s/it] 20%|█▉        | 196/1000 [19:51<1:16:16,  5.69s/it] 20%|█▉        | 197/1000 [19:54<1:04:53,  4.85s/it] 20%|█▉        | 198/1000 [20:00<1:10:07,  5.25s/it] 20%|█▉        | 199/1000 [20:08<1:20:01,  5.99s/it] 20%|██        | 200/1000 [20:11<1:07:58,  5.10s/it] 20%|██        | 201/1000 [20:16<1:09:47,  5.24s/it] 20%|██        | 202/1000 [20:21<1:06:49,  5.02s/it] 20%|██        | 203/1000 [20:24<1:00:50,  4.58s/it] 20%|██        | 204/1000 [20:30<1:05:09,  4.91s/it] 20%|██        | 205/1000 [20:33<58:37,  4.42s/it]   21%|██        | 206/1000 [20:39<1:04:27,  4.87s/it] 21%|██        | 207/1000 [20:46<1:09:36,  5.27s/it] 21%|██        | 208/1000 [20:49<1:01:16,  4.64s/it] 21%|██        | 209/1000 [20:52<57:12,  4.34s/it]   21%|██        | 210/1000 [21:01<1:14:31,  5.66s/it] 21%|██        | 211/1000 [21:07<1:15:10,  5.72s/it] 21%|██        | 212/1000 [21:10<1:06:09,  5.04s/it] 21%|██▏       | 213/1000 [21:13<55:22,  4.22s/it]   21%|██▏       | 214/1000 [21:18<1:00:06,  4.59s/it] 22%|██▏       | 215/1000 [21:28<1:19:15,  6.06s/it] 22%|██▏       | 216/1000 [21:31<1:09:02,  5.28s/it] 22%|██▏       | 217/1000 [21:34<57:52,  4.43s/it]   22%|██▏       | 218/1000 [21:39<1:01:02,  4.68s/it] 22%|██▏       | 219/1000 [21:44<1:02:59,  4.84s/it] 22%|██▏       | 220/1000 [21:48<59:10,  4.55s/it]   22%|██▏       | 221/1000 [21:54<1:05:22,  5.04s/it] 22%|██▏       | 222/1000 [22:06<1:32:15,  7.12s/it] 22%|██▏       | 223/1000 [22:12<1:27:37,  6.77s/it] 22%|██▏       | 224/1000 [22:16<1:15:09,  5.81s/it] 22%|██▎       | 225/1000 [22:20<1:10:55,  5.49s/it] 23%|██▎       | 226/1000 [22:24<1:05:11,  5.05s/it] 23%|██▎       | 227/1000 [22:29<1:01:58,  4.81s/it] 23%|██▎       | 228/1000 [22:31<52:23,  4.07s/it]   23%|██▎       | 229/1000 [22:34<48:08,  3.75s/it] 23%|██▎       | 230/1000 [22:38<49:13,  3.84s/it] 23%|██▎       | 231/1000 [22:43<53:21,  4.16s/it] 23%|██▎       | 232/1000 [22:47<52:35,  4.11s/it] 23%|██▎       | 233/1000 [22:53<1:00:01,  4.70s/it] 23%|██▎       | 234/1000 [22:55<51:41,  4.05s/it]   24%|██▎       | 235/1000 [22:59<49:38,  3.89s/it] 24%|██▎       | 236/1000 [23:04<54:41,  4.29s/it] 24%|██▎       | 237/1000 [23:08<52:06,  4.10s/it] 24%|██▍       | 238/1000 [23:16<1:07:09,  5.29s/it] 24%|██▍       | 239/1000 [23:20<1:04:14,  5.06s/it] 24%|██▍       | 240/1000 [23:25<1:02:39,  4.95s/it] 24%|██▍       | 241/1000 [23:32<1:10:53,  5.60s/it] 24%|██▍       | 242/1000 [23:37<1:06:51,  5.29s/it] 24%|██▍       | 243/1000 [23:40<1:00:25,  4.79s/it] 24%|██▍       | 244/1000 [23:44<54:47,  4.35s/it]   24%|██▍       | 245/1000 [23:49<56:22,  4.48s/it] 25%|██▍       | 246/1000 [23:53<54:30,  4.34s/it] 25%|██▍       | 247/1000 [23:56<52:33,  4.19s/it] 25%|██▍       | 248/1000 [24:01<53:00,  4.23s/it] 25%|██▍       | 249/1000 [24:08<1:02:57,  5.03s/it] 25%|██▌       | 250/1000 [24:11<56:12,  4.50s/it]   25%|██▌       | 251/1000 [24:16<59:28,  4.76s/it] 25%|██▌       | 252/1000 [24:22<1:01:35,  4.94s/it] 25%|██▌       | 253/1000 [24:26<57:59,  4.66s/it]   25%|██▌       | 254/1000 [24:31<1:00:34,  4.87s/it] 26%|██▌       | 255/1000 [24:35<55:25,  4.46s/it]   26%|██▌       | 256/1000 [24:38<52:10,  4.21s/it] 26%|██▌       | 257/1000 [24:41<46:13,  3.73s/it] 26%|██▌       | 258/1000 [24:43<40:57,  3.31s/it] 26%|██▌       | 259/1000 [24:47<41:28,  3.36s/it] 26%|██▌       | 260/1000 [24:49<39:43,  3.22s/it] 26%|██▌       | 261/1000 [24:52<36:49,  2.99s/it] 26%|██▌       | 262/1000 [24:54<32:28,  2.64s/it] 26%|██▋       | 263/1000 [24:58<37:29,  3.05s/it] 26%|██▋       | 264/1000 [25:04<51:04,  4.16s/it] 26%|██▋       | 265/1000 [25:10<54:33,  4.45s/it] 27%|██▋       | 266/1000 [25:12<46:39,  3.81s/it] 27%|██▋       | 267/1000 [25:16<48:25,  3.96s/it] 27%|██▋       | 268/1000 [25:23<1:00:03,  4.92s/it] 27%|██▋       | 269/1000 [25:27<54:56,  4.51s/it]   27%|██▋       | 270/1000 [25:31<53:39,  4.41s/it] 27%|██▋       | 271/1000 [25:35<50:08,  4.13s/it] 27%|██▋       | 272/1000 [25:39<51:31,  4.25s/it] 27%|██▋       | 273/1000 [25:43<51:15,  4.23s/it] 27%|██▋       | 274/1000 [25:50<1:00:42,  5.02s/it] 28%|██▊       | 275/1000 [25:58<1:10:34,  5.84s/it] 28%|██▊       | 276/1000 [26:05<1:13:13,  6.07s/it] 28%|██▊       | 277/1000 [26:09<1:07:20,  5.59s/it] 28%|██▊       | 278/1000 [26:14<1:06:08,  5.50s/it] 28%|██▊       | 279/1000 [26:18<1:00:50,  5.06s/it] 28%|██▊       | 280/1000 [26:25<1:05:48,  5.48s/it] 28%|██▊       | 281/1000 [26:29<1:01:01,  5.09s/it] 28%|██▊       | 282/1000 [26:31<51:12,  4.28s/it]   28%|██▊       | 283/1000 [26:37<57:28,  4.81s/it] 28%|██▊       | 284/1000 [26:45<1:06:55,  5.61s/it] 28%|██▊       | 285/1000 [26:47<55:48,  4.68s/it]   29%|██▊       | 286/1000 [26:52<56:23,  4.74s/it] 29%|██▊       | 287/1000 [26:57<55:28,  4.67s/it] 29%|██▉       | 288/1000 [27:00<49:35,  4.18s/it] 29%|██▉       | 289/1000 [27:03<47:12,  3.98s/it] 29%|██▉       | 290/1000 [27:09<53:39,  4.53s/it] 29%|██▉       | 291/1000 [27:15<57:23,  4.86s/it] 29%|██▉       | 292/1000 [27:18<50:21,  4.27s/it] 29%|██▉       | 293/1000 [27:23<55:09,  4.68s/it] 29%|██▉       | 294/1000 [27:28<54:01,  4.59s/it] 30%|██▉       | 295/1000 [27:32<51:37,  4.39s/it] 30%|██▉       | 296/1000 [27:36<52:04,  4.44s/it] 30%|██▉       | 297/1000 [27:42<56:30,  4.82s/it] 30%|██▉       | 298/1000 [27:47<56:35,  4.84s/it] 30%|██▉       | 299/1000 [27:57<1:15:07,  6.43s/it] 30%|███       | 300/1000 [28:05<1:21:48,  7.01s/it] 30%|███       | 301/1000 [28:10<1:15:04,  6.44s/it] 30%|███       | 302/1000 [28:14<1:06:06,  5.68s/it] 30%|███       | 303/1000 [28:17<55:36,  4.79s/it]   30%|███       | 304/1000 [28:20<50:27,  4.35s/it] 30%|███       | 305/1000 [28:24<46:30,  4.02s/it] 31%|███       | 306/1000 [28:28<48:59,  4.24s/it] 31%|███       | 307/1000 [28:30<40:33,  3.51s/it] 31%|███       | 308/1000 [28:35<45:56,  3.98s/it] 31%|███       | 309/1000 [28:38<41:05,  3.57s/it] 31%|███       | 310/1000 [28:47<1:00:11,  5.23s/it] 31%|███       | 311/1000 [28:54<1:05:41,  5.72s/it] 31%|███       | 312/1000 [28:59<1:02:07,  5.42s/it] 31%|███▏      | 313/1000 [29:03<59:03,  5.16s/it]   31%|███▏      | 314/1000 [29:06<52:00,  4.55s/it] 32%|███▏      | 315/1000 [29:10<48:34,  4.25s/it] 32%|███▏      | 316/1000 [29:16<54:29,  4.78s/it] 32%|███▏      | 317/1000 [29:19<49:26,  4.34s/it] 32%|███▏      | 318/1000 [29:25<54:06,  4.76s/it] 32%|███▏      | 319/1000 [29:30<54:34,  4.81s/it] 32%|███▏      | 320/1000 [29:42<1:18:13,  6.90s/it] 32%|███▏      | 321/1000 [29:45<1:06:06,  5.84s/it] 32%|███▏      | 322/1000 [29:50<1:04:45,  5.73s/it] 32%|███▏      | 323/1000 [29:56<1:04:07,  5.68s/it] 32%|███▏      | 324/1000 [30:00<59:21,  5.27s/it]   32%|███▎      | 325/1000 [30:04<52:41,  4.68s/it] 33%|███▎      | 326/1000 [30:09<53:31,  4.76s/it] 33%|███▎      | 327/1000 [30:14<54:47,  4.89s/it] 33%|███▎      | 328/1000 [30:25<1:15:40,  6.76s/it] 33%|███▎      | 329/1000 [30:32<1:15:51,  6.78s/it] 33%|███▎      | 330/1000 [30:43<1:31:37,  8.21s/it] 33%|███▎      | 331/1000 [30:49<1:22:22,  7.39s/it] 33%|███▎      | 332/1000 [30:52<1:10:10,  6.30s/it] 33%|███▎      | 333/1000 [31:00<1:13:33,  6.62s/it] 33%|███▎      | 334/1000 [31:06<1:11:38,  6.45s/it] 34%|███▎      | 335/1000 [31:10<1:03:22,  5.72s/it] 34%|███▎      | 336/1000 [31:19<1:14:22,  6.72s/it] 34%|███▎      | 337/1000 [31:22<1:02:38,  5.67s/it] 34%|███▍      | 338/1000 [31:28<1:03:59,  5.80s/it] 34%|███▍      | 339/1000 [31:32<57:02,  5.18s/it]   34%|███▍      | 340/1000 [31:37<55:02,  5.00s/it] 34%|███▍      | 341/1000 [31:40<51:22,  4.68s/it] 34%|███▍      | 342/1000 [31:46<55:16,  5.04s/it] 34%|███▍      | 343/1000 [31:51<52:19,  4.78s/it] 34%|███▍      | 344/1000 [31:55<52:35,  4.81s/it] 34%|███▍      | 345/1000 [31:59<47:03,  4.31s/it] 35%|███▍      | 346/1000 [32:01<41:52,  3.84s/it] 35%|███▍      | 347/1000 [32:05<40:27,  3.72s/it] 35%|███▍      | 348/1000 [32:10<44:48,  4.12s/it] 35%|███▍      | 349/1000 [32:16<51:02,  4.70s/it] 35%|███▌      | 350/1000 [32:28<1:16:19,  7.04s/it] 35%|███▌      | 351/1000 [32:36<1:16:52,  7.11s/it] 35%|███▌      | 352/1000 [32:40<1:06:27,  6.15s/it] 35%|███▌      | 353/1000 [32:49<1:17:04,  7.15s/it] 35%|███▌      | 354/1000 [32:53<1:08:01,  6.32s/it] 36%|███▌      | 355/1000 [32:58<1:01:53,  5.76s/it] 36%|███▌      | 356/1000 [33:04<1:03:43,  5.94s/it] 36%|███▌      | 357/1000 [33:07<52:25,  4.89s/it]   36%|███▌      | 358/1000 [33:09<44:59,  4.20s/it] 36%|███▌      | 359/1000 [33:13<43:54,  4.11s/it] 36%|███▌      | 360/1000 [33:16<39:14,  3.68s/it] 36%|███▌      | 361/1000 [33:21<45:30,  4.27s/it] 36%|███▌      | 362/1000 [33:26<45:09,  4.25s/it] 36%|███▋      | 363/1000 [33:30<44:13,  4.17s/it] 36%|███▋      | 364/1000 [33:37<53:23,  5.04s/it] 36%|███▋      | 365/1000 [33:47<1:10:29,  6.66s/it] 37%|███▋      | 366/1000 [33:53<1:08:11,  6.45s/it] 37%|███▋      | 367/1000 [34:05<1:23:50,  7.95s/it] 37%|███▋      | 368/1000 [34:08<1:10:46,  6.72s/it] 37%|███▋      | 369/1000 [34:13<1:03:46,  6.06s/it] 37%|███▋      | 370/1000 [34:20<1:07:12,  6.40s/it] 37%|███▋      | 371/1000 [34:23<56:53,  5.43s/it]   37%|███▋      | 372/1000 [34:29<56:24,  5.39s/it] 37%|███▋      | 373/1000 [34:32<50:27,  4.83s/it] 37%|███▋      | 374/1000 [34:37<50:27,  4.84s/it] 38%|███▊      | 375/1000 [34:42<49:39,  4.77s/it] 38%|███▊      | 376/1000 [34:48<53:39,  5.16s/it] 38%|███▊      | 377/1000 [34:52<50:36,  4.87s/it] 38%|███▊      | 378/1000 [34:58<55:38,  5.37s/it] 38%|███▊      | 379/1000 [35:01<46:02,  4.45s/it] 38%|███▊      | 380/1000 [35:07<51:57,  5.03s/it] 38%|███▊      | 381/1000 [35:11<47:33,  4.61s/it] 38%|███▊      | 382/1000 [35:18<55:23,  5.38s/it] 38%|███▊      | 383/1000 [35:20<45:27,  4.42s/it] 38%|███▊      | 384/1000 [35:24<45:00,  4.38s/it] 38%|███▊      | 385/1000 [35:29<46:01,  4.49s/it] 39%|███▊      | 386/1000 [35:32<39:50,  3.89s/it] 39%|███▊      | 387/1000 [35:37<45:53,  4.49s/it] 39%|███▉      | 388/1000 [35:53<1:20:28,  7.89s/it] 39%|███▉      | 389/1000 [35:56<1:03:32,  6.24s/it] 39%|███▉      | 390/1000 [36:05<1:13:22,  7.22s/it] 39%|███▉      | 391/1000 [36:12<1:11:59,  7.09s/it] 39%|███▉      | 392/1000 [36:15<1:00:47,  6.00s/it] 39%|███▉      | 393/1000 [36:21<58:50,  5.82s/it]   39%|███▉      | 394/1000 [36:27<59:27,  5.89s/it] 40%|███▉      | 395/1000 [36:34<1:04:19,  6.38s/it] 40%|███▉      | 396/1000 [36:44<1:14:26,  7.39s/it] 40%|███▉      | 397/1000 [36:49<1:07:19,  6.70s/it] 40%|███▉      | 398/1000 [36:54<1:00:14,  6.00s/it] 40%|███▉      | 399/1000 [36:58<56:30,  5.64s/it]   40%|████      | 400/1000 [37:03<52:20,  5.23s/it] 40%|████      | 401/1000 [37:07<49:39,  4.97s/it] 40%|████      | 402/1000 [37:11<45:44,  4.59s/it] 40%|████      | 403/1000 [37:20<59:54,  6.02s/it] 40%|████      | 404/1000 [37:23<51:52,  5.22s/it] 40%|████      | 405/1000 [37:29<52:02,  5.25s/it] 41%|████      | 406/1000 [37:31<43:41,  4.41s/it] 41%|████      | 407/1000 [37:36<44:12,  4.47s/it] 41%|████      | 408/1000 [37:42<50:12,  5.09s/it] 41%|████      | 409/1000 [37:50<58:05,  5.90s/it] 41%|████      | 410/1000 [37:55<53:49,  5.47s/it] 41%|████      | 411/1000 [38:04<1:04:43,  6.59s/it] 41%|████      | 412/1000 [38:11<1:07:19,  6.87s/it] 41%|████▏     | 413/1000 [38:16<1:01:26,  6.28s/it] 41%|████▏     | 414/1000 [38:19<50:33,  5.18s/it]   42%|████▏     | 415/1000 [38:28<1:00:39,  6.22s/it] 42%|████▏     | 416/1000 [38:30<49:28,  5.08s/it]   42%|████▏     | 417/1000 [38:33<43:08,  4.44s/it] 42%|████▏     | 418/1000 [38:38<44:53,  4.63s/it] 42%|████▏     | 419/1000 [38:43<44:42,  4.62s/it] 42%|████▏     | 420/1000 [38:46<41:44,  4.32s/it] 42%|████▏     | 421/1000 [38:50<40:27,  4.19s/it] 42%|████▏     | 422/1000 [38:58<50:36,  5.25s/it] 42%|████▏     | 423/1000 [39:00<42:55,  4.46s/it] 42%|████▏     | 424/1000 [39:03<36:41,  3.82s/it] 42%|████▎     | 425/1000 [39:06<36:01,  3.76s/it] 43%|████▎     | 426/1000 [39:10<36:12,  3.78s/it] 43%|████▎     | 427/1000 [39:13<33:36,  3.52s/it] 43%|████▎     | 428/1000 [39:21<46:05,  4.83s/it] 43%|████▎     | 429/1000 [39:30<58:11,  6.11s/it] 43%|████▎     | 430/1000 [39:34<50:31,  5.32s/it] 43%|████▎     | 431/1000 [39:39<49:46,  5.25s/it] 43%|████▎     | 432/1000 [39:42<44:46,  4.73s/it] 43%|████▎     | 433/1000 [39:52<58:07,  6.15s/it] 43%|████▎     | 434/1000 [39:55<50:17,  5.33s/it] 44%|████▎     | 435/1000 [39:58<42:41,  4.53s/it] 44%|████▎     | 436/1000 [40:06<53:16,  5.67s/it] 44%|████▎     | 437/1000 [40:16<1:04:18,  6.85s/it] 44%|████▍     | 438/1000 [40:22<1:03:16,  6.76s/it] 44%|████▍     | 439/1000 [40:27<56:22,  6.03s/it]   44%|████▍     | 440/1000 [40:30<49:19,  5.28s/it] 44%|████▍     | 441/1000 [40:35<48:44,  5.23s/it] 44%|████▍     | 442/1000 [40:41<50:23,  5.42s/it] 44%|████▍     | 443/1000 [40:44<44:09,  4.76s/it] 44%|████▍     | 444/1000 [40:48<41:23,  4.47s/it] 44%|████▍     | 445/1000 [40:51<36:19,  3.93s/it] 45%|████▍     | 446/1000 [40:54<34:00,  3.68s/it] 45%|████▍     | 447/1000 [40:58<36:11,  3.93s/it] 45%|████▍     | 448/1000 [41:02<34:08,  3.71s/it] 45%|████▍     | 449/1000 [41:09<44:23,  4.83s/it] 45%|████▌     | 450/1000 [41:14<45:58,  5.02s/it] 45%|████▌     | 451/1000 [41:26<1:03:37,  6.95s/it] 45%|████▌     | 452/1000 [41:33<1:04:30,  7.06s/it] 45%|████▌     | 453/1000 [41:40<1:02:40,  6.87s/it] 45%|████▌     | 454/1000 [41:45<58:21,  6.41s/it]   46%|████▌     | 455/1000 [41:54<1:06:28,  7.32s/it] 46%|████▌     | 456/1000 [41:57<54:14,  5.98s/it]   46%|████▌     | 457/1000 [41:59<42:27,  4.69s/it] 46%|████▌     | 458/1000 [42:05<45:56,  5.09s/it] 46%|████▌     | 459/1000 [42:08<40:56,  4.54s/it] 46%|████▌     | 460/1000 [42:12<38:53,  4.32s/it] 46%|████▌     | 461/1000 [42:16<38:07,  4.24s/it] 46%|████▌     | 462/1000 [42:21<39:06,  4.36s/it] 46%|████▋     | 463/1000 [42:23<33:37,  3.76s/it] 46%|████▋     | 464/1000 [42:25<29:24,  3.29s/it] 46%|████▋     | 465/1000 [42:29<29:40,  3.33s/it] 47%|████▋     | 466/1000 [42:33<31:40,  3.56s/it] 47%|████▋     | 467/1000 [42:36<30:02,  3.38s/it] 47%|████▋     | 468/1000 [42:41<35:08,  3.96s/it] 47%|████▋     | 469/1000 [42:46<37:04,  4.19s/it] 47%|████▋     | 470/1000 [42:50<36:53,  4.18s/it] 47%|████▋     | 471/1000 [42:55<38:05,  4.32s/it] 47%|████▋     | 472/1000 [42:58<34:24,  3.91s/it] 47%|████▋     | 473/1000 [43:05<43:07,  4.91s/it] 47%|████▋     | 474/1000 [43:10<43:30,  4.96s/it] 48%|████▊     | 475/1000 [43:18<51:57,  5.94s/it] 48%|████▊     | 476/1000 [43:28<1:03:19,  7.25s/it] 48%|████▊     | 477/1000 [43:33<57:12,  6.56s/it]   48%|████▊     | 478/1000 [43:38<52:48,  6.07s/it] 48%|████▊     | 479/1000 [43:47<59:48,  6.89s/it] 48%|████▊     | 480/1000 [43:51<52:04,  6.01s/it] 48%|████▊     | 481/1000 [43:54<43:00,  4.97s/it] 48%|████▊     | 482/1000 [43:56<35:57,  4.17s/it] 48%|████▊     | 483/1000 [44:03<44:43,  5.19s/it] 48%|████▊     | 484/1000 [44:07<41:22,  4.81s/it] 48%|████▊     | 485/1000 [44:10<36:08,  4.21s/it] 49%|████▊     | 486/1000 [44:13<32:15,  3.77s/it] 49%|████▊     | 487/1000 [44:17<31:45,  3.71s/it] 49%|████▉     | 488/1000 [44:21<33:10,  3.89s/it] 49%|████▉     | 489/1000 [44:27<39:40,  4.66s/it] 49%|████▉     | 490/1000 [44:36<50:00,  5.88s/it] 49%|████▉     | 491/1000 [44:48<1:05:59,  7.78s/it] 49%|████▉     | 492/1000 [44:53<59:05,  6.98s/it]   49%|████▉     | 493/1000 [44:56<48:40,  5.76s/it] 49%|████▉     | 494/1000 [44:59<41:15,  4.89s/it] 50%|████▉     | 495/1000 [45:03<37:38,  4.47s/it] 50%|████▉     | 496/1000 [45:05<32:35,  3.88s/it] 50%|████▉     | 497/1000 [45:10<35:52,  4.28s/it] 50%|████▉     | 498/1000 [45:15<35:57,  4.30s/it] 50%|████▉     | 499/1000 [45:19<35:58,  4.31s/it] 50%|█████     | 500/1000 [45:23<36:12,  4.34s/it] 50%|█████     | 501/1000 [45:28<37:03,  4.46s/it] 50%|█████     | 502/1000 [45:35<43:42,  5.27s/it] 50%|█████     | 503/1000 [45:47<59:27,  7.18s/it] 50%|█████     | 504/1000 [45:52<54:50,  6.63s/it] 50%|█████     | 505/1000 [45:56<47:30,  5.76s/it] 51%|█████     | 506/1000 [46:00<44:10,  5.37s/it] 51%|█████     | 507/1000 [46:03<37:44,  4.59s/it] 51%|█████     | 508/1000 [46:08<39:09,  4.78s/it] 51%|█████     | 509/1000 [46:11<34:01,  4.16s/it] 51%|█████     | 510/1000 [46:14<31:05,  3.81s/it] 51%|█████     | 511/1000 [46:18<29:55,  3.67s/it] 51%|█████     | 512/1000 [46:21<28:25,  3.49s/it] 51%|█████▏    | 513/1000 [46:27<35:18,  4.35s/it] 51%|█████▏    | 514/1000 [46:39<53:37,  6.62s/it] 52%|█████▏    | 515/1000 [46:45<51:28,  6.37s/it] 52%|█████▏    | 516/1000 [46:51<50:59,  6.32s/it] 52%|█████▏    | 517/1000 [46:57<49:48,  6.19s/it] 52%|█████▏    | 518/1000 [47:03<48:46,  6.07s/it] 52%|█████▏    | 519/1000 [47:10<52:43,  6.58s/it] 52%|█████▏    | 520/1000 [47:18<54:27,  6.81s/it] 52%|█████▏    | 521/1000 [47:24<53:09,  6.66s/it] 52%|█████▏    | 522/1000 [47:30<50:58,  6.40s/it] 52%|█████▏    | 523/1000 [47:35<48:25,  6.09s/it] 52%|█████▏    | 524/1000 [47:39<43:01,  5.42s/it] 52%|█████▎    | 525/1000 [47:41<35:56,  4.54s/it] 53%|█████▎    | 526/1000 [47:46<36:14,  4.59s/it] 53%|█████▎    | 527/1000 [47:52<39:49,  5.05s/it] 53%|█████▎    | 528/1000 [47:56<35:51,  4.56s/it] 53%|█████▎    | 529/1000 [47:59<31:59,  4.08s/it] 53%|█████▎    | 530/1000 [48:02<29:04,  3.71s/it] 53%|█████▎    | 531/1000 [48:08<35:04,  4.49s/it] 53%|█████▎    | 532/1000 [48:14<38:04,  4.88s/it] 53%|█████▎    | 533/1000 [48:20<40:27,  5.20s/it] 53%|█████▎    | 534/1000 [48:23<37:21,  4.81s/it] 54%|█████▎    | 535/1000 [48:30<41:00,  5.29s/it] 54%|█████▎    | 536/1000 [48:34<38:55,  5.03s/it] 54%|█████▎    | 537/1000 [48:40<39:25,  5.11s/it] 54%|█████▍    | 538/1000 [48:46<42:27,  5.51s/it] 54%|█████▍    | 539/1000 [48:50<39:17,  5.11s/it] 54%|█████▍    | 540/1000 [48:58<45:36,  5.95s/it] 54%|█████▍    | 541/1000 [49:05<47:39,  6.23s/it] 54%|█████▍    | 542/1000 [49:11<46:44,  6.12s/it] 54%|█████▍    | 543/1000 [49:15<42:58,  5.64s/it] 54%|█████▍    | 544/1000 [49:19<37:38,  4.95s/it] 55%|█████▍    | 545/1000 [49:23<34:53,  4.60s/it] 55%|█████▍    | 546/1000 [49:25<30:20,  4.01s/it] 55%|█████▍    | 547/1000 [49:31<34:49,  4.61s/it] 55%|█████▍    | 548/1000 [49:39<40:54,  5.43s/it] 55%|█████▍    | 549/1000 [49:43<39:10,  5.21s/it] 55%|█████▌    | 550/1000 [49:49<40:54,  5.45s/it] 55%|█████▌    | 551/1000 [49:58<47:09,  6.30s/it] 55%|█████▌    | 552/1000 [50:05<48:40,  6.52s/it] 55%|█████▌    | 553/1000 [50:10<46:21,  6.22s/it] 55%|█████▌    | 554/1000 [50:15<42:27,  5.71s/it] 56%|█████▌    | 555/1000 [50:20<41:27,  5.59s/it] 56%|█████▌    | 556/1000 [50:24<37:07,  5.02s/it] 56%|█████▌    | 557/1000 [50:28<36:27,  4.94s/it] 56%|█████▌    | 558/1000 [50:32<34:34,  4.69s/it] 56%|█████▌    | 559/1000 [50:38<35:41,  4.86s/it] 56%|█████▌    | 560/1000 [50:44<38:31,  5.25s/it] 56%|█████▌    | 561/1000 [50:49<37:02,  5.06s/it] 56%|█████▌    | 562/1000 [50:55<39:27,  5.41s/it] 56%|█████▋    | 563/1000 [50:59<37:27,  5.14s/it] 56%|█████▋    | 564/1000 [51:03<35:04,  4.83s/it] 56%|█████▋    | 565/1000 [51:07<33:14,  4.59s/it] 57%|█████▋    | 566/1000 [51:11<30:43,  4.25s/it] 57%|█████▋    | 567/1000 [51:15<29:41,  4.11s/it] 57%|█████▋    | 568/1000 [51:19<30:13,  4.20s/it] 57%|█████▋    | 569/1000 [51:24<32:35,  4.54s/it] 57%|█████▋    | 570/1000 [51:31<36:10,  5.05s/it] 57%|█████▋    | 571/1000 [51:37<39:15,  5.49s/it] 57%|█████▋    | 572/1000 [51:43<39:08,  5.49s/it] 57%|█████▋    | 573/1000 [51:48<39:04,  5.49s/it] 57%|█████▋    | 574/1000 [51:52<36:36,  5.16s/it] 57%|█████▊    | 575/1000 [51:56<32:38,  4.61s/it] 58%|█████▊    | 576/1000 [52:00<31:21,  4.44s/it] 58%|█████▊    | 577/1000 [52:03<29:29,  4.18s/it] 58%|█████▊    | 578/1000 [52:06<27:04,  3.85s/it] 58%|█████▊    | 579/1000 [52:12<30:40,  4.37s/it] 58%|█████▊    | 580/1000 [52:17<31:21,  4.48s/it] 58%|█████▊    | 581/1000 [52:23<33:51,  4.85s/it] 58%|█████▊    | 582/1000 [52:29<36:14,  5.20s/it] 58%|█████▊    | 583/1000 [52:35<38:18,  5.51s/it] 58%|█████▊    | 584/1000 [52:38<33:26,  4.82s/it] 58%|█████▊    | 585/1000 [52:44<36:00,  5.21s/it] 59%|█████▊    | 586/1000 [52:46<29:24,  4.26s/it] 59%|█████▊    | 587/1000 [52:49<26:32,  3.86s/it] 59%|█████▉    | 588/1000 [52:52<24:26,  3.56s/it] 59%|█████▉    | 589/1000 [52:55<24:04,  3.51s/it] 59%|█████▉    | 590/1000 [52:58<21:31,  3.15s/it] 59%|█████▉    | 591/1000 [53:01<22:53,  3.36s/it] 59%|█████▉    | 592/1000 [53:07<27:15,  4.01s/it] 59%|█████▉    | 593/1000 [53:09<23:48,  3.51s/it] 59%|█████▉    | 594/1000 [53:14<25:37,  3.79s/it] 60%|█████▉    | 595/1000 [53:18<26:09,  3.87s/it] 60%|█████▉    | 596/1000 [53:24<30:10,  4.48s/it] 60%|█████▉    | 597/1000 [53:26<25:57,  3.86s/it] 60%|█████▉    | 598/1000 [53:33<30:52,  4.61s/it] 60%|█████▉    | 599/1000 [53:36<28:15,  4.23s/it] 60%|██████    | 600/1000 [53:40<28:15,  4.24s/it] 60%|██████    | 601/1000 [53:45<29:56,  4.50s/it] 60%|██████    | 602/1000 [53:51<32:51,  4.95s/it] 60%|██████    | 603/1000 [53:56<31:59,  4.83s/it] 60%|██████    | 604/1000 [54:01<31:58,  4.84s/it] 60%|██████    | 605/1000 [54:06<32:33,  4.95s/it] 61%|██████    | 606/1000 [54:12<35:17,  5.37s/it] 61%|██████    | 607/1000 [54:22<43:26,  6.63s/it] 61%|██████    | 608/1000 [54:26<38:09,  5.84s/it] 61%|██████    | 609/1000 [54:28<30:27,  4.67s/it] 61%|██████    | 610/1000 [54:30<26:08,  4.02s/it] 61%|██████    | 611/1000 [54:34<24:41,  3.81s/it] 61%|██████    | 612/1000 [54:37<24:24,  3.78s/it] 61%|██████▏   | 613/1000 [54:41<24:43,  3.83s/it] 61%|██████▏   | 614/1000 [54:47<28:59,  4.51s/it] 62%|██████▏   | 615/1000 [54:53<31:51,  4.97s/it] 62%|██████▏   | 616/1000 [54:59<33:56,  5.30s/it] 62%|██████▏   | 617/1000 [55:03<30:50,  4.83s/it] 62%|██████▏   | 618/1000 [55:07<29:36,  4.65s/it] 62%|██████▏   | 619/1000 [55:14<33:22,  5.26s/it] 62%|██████▏   | 620/1000 [55:19<31:59,  5.05s/it] 62%|██████▏   | 621/1000 [55:24<31:58,  5.06s/it] 62%|██████▏   | 622/1000 [55:28<29:53,  4.75s/it] 62%|██████▏   | 623/1000 [55:45<52:45,  8.40s/it] 62%|██████▏   | 624/1000 [55:49<44:16,  7.07s/it] 62%|██████▎   | 625/1000 [55:58<48:12,  7.71s/it] 63%|██████▎   | 626/1000 [56:02<41:35,  6.67s/it] 63%|██████▎   | 627/1000 [56:05<33:59,  5.47s/it] 63%|██████▎   | 628/1000 [56:07<28:37,  4.62s/it] 63%|██████▎   | 629/1000 [56:12<29:13,  4.73s/it] 63%|██████▎   | 630/1000 [56:16<26:46,  4.34s/it] 63%|██████▎   | 631/1000 [56:19<24:10,  3.93s/it] 63%|██████▎   | 632/1000 [56:27<31:31,  5.14s/it] 63%|██████▎   | 633/1000 [56:33<33:20,  5.45s/it] 63%|██████▎   | 634/1000 [56:50<54:46,  8.98s/it] 64%|██████▎   | 635/1000 [56:56<49:37,  8.16s/it] 64%|██████▎   | 636/1000 [57:03<46:05,  7.60s/it] 64%|██████▎   | 637/1000 [57:08<41:26,  6.85s/it] 64%|██████▍   | 638/1000 [57:13<37:43,  6.25s/it] 64%|██████▍   | 639/1000 [57:18<36:16,  6.03s/it] 64%|██████▍   | 640/1000 [57:27<40:32,  6.76s/it] 64%|██████▍   | 641/1000 [57:37<46:25,  7.76s/it] 64%|██████▍   | 642/1000 [57:44<45:00,  7.54s/it] 64%|██████▍   | 643/1000 [57:50<42:27,  7.14s/it] 64%|██████▍   | 644/1000 [57:57<42:45,  7.21s/it] 64%|██████▍   | 645/1000 [58:01<35:40,  6.03s/it] 65%|██████▍   | 646/1000 [58:07<35:30,  6.02s/it] 65%|██████▍   | 647/1000 [58:09<29:36,  5.03s/it] 65%|██████▍   | 648/1000 [58:17<33:57,  5.79s/it] 65%|██████▍   | 649/1000 [58:20<28:31,  4.88s/it] 65%|██████▌   | 650/1000 [58:28<33:48,  5.80s/it] 65%|██████▌   | 651/1000 [58:30<28:12,  4.85s/it] 65%|██████▌   | 652/1000 [58:34<26:11,  4.52s/it] 65%|██████▌   | 653/1000 [58:39<27:26,  4.75s/it] 65%|██████▌   | 654/1000 [58:42<24:23,  4.23s/it] 66%|██████▌   | 655/1000 [58:46<24:26,  4.25s/it] 66%|██████▌   | 656/1000 [58:49<20:35,  3.59s/it] 66%|██████▌   | 657/1000 [58:52<20:53,  3.66s/it] 66%|██████▌   | 658/1000 [58:56<20:07,  3.53s/it] 66%|██████▌   | 659/1000 [58:58<18:13,  3.21s/it] 66%|██████▌   | 660/1000 [59:02<20:16,  3.58s/it] 66%|██████▌   | 661/1000 [59:10<26:40,  4.72s/it] 66%|██████▌   | 662/1000 [59:15<26:56,  4.78s/it] 66%|██████▋   | 663/1000 [59:19<26:19,  4.69s/it] 66%|██████▋   | 664/1000 [59:30<36:19,  6.49s/it] 66%|██████▋   | 665/1000 [59:32<29:25,  5.27s/it] 67%|██████▋   | 666/1000 [59:39<31:59,  5.75s/it] 67%|██████▋   | 667/1000 [59:46<33:09,  5.97s/it] 67%|██████▋   | 668/1000 [59:51<32:27,  5.87s/it] 67%|██████▋   | 669/1000 [59:54<26:47,  4.86s/it] 67%|██████▋   | 670/1000 [59:59<26:42,  4.86s/it] 67%|██████▋   | 671/1000 [1:00:01<22:12,  4.05s/it] 67%|██████▋   | 672/1000 [1:00:06<23:36,  4.32s/it] 67%|██████▋   | 673/1000 [1:00:09<20:56,  3.84s/it] 67%|██████▋   | 674/1000 [1:00:13<21:46,  4.01s/it] 68%|██████▊   | 675/1000 [1:00:17<22:30,  4.16s/it] 68%|██████▊   | 676/1000 [1:00:23<24:06,  4.46s/it] 68%|██████▊   | 677/1000 [1:00:24<19:11,  3.57s/it] 68%|██████▊   | 678/1000 [1:00:27<18:10,  3.39s/it] 68%|██████▊   | 679/1000 [1:00:33<21:30,  4.02s/it] 68%|██████▊   | 680/1000 [1:00:38<24:09,  4.53s/it] 68%|██████▊   | 681/1000 [1:00:49<34:13,  6.44s/it] 68%|██████▊   | 682/1000 [1:00:57<36:24,  6.87s/it] 68%|██████▊   | 683/1000 [1:01:06<40:18,  7.63s/it] 68%|██████▊   | 684/1000 [1:01:15<42:20,  8.04s/it] 68%|██████▊   | 685/1000 [1:01:28<49:42,  9.47s/it] 69%|██████▊   | 686/1000 [1:01:32<40:36,  7.76s/it] 69%|██████▊   | 687/1000 [1:01:42<43:24,  8.32s/it] 69%|██████▉   | 688/1000 [1:01:50<42:58,  8.26s/it] 69%|██████▉   | 689/1000 [1:02:08<57:50, 11.16s/it] 69%|██████▉   | 690/1000 [1:02:15<51:14,  9.92s/it] 69%|██████▉   | 691/1000 [1:02:18<40:24,  7.85s/it] 69%|██████▉   | 692/1000 [1:02:22<35:18,  6.88s/it] 69%|██████▉   | 693/1000 [1:02:26<29:55,  5.85s/it] 69%|██████▉   | 694/1000 [1:02:30<26:32,  5.20s/it] 70%|██████▉   | 695/1000 [1:02:34<24:37,  4.84s/it] 70%|██████▉   | 696/1000 [1:02:37<22:28,  4.44s/it] 70%|██████▉   | 697/1000 [1:02:45<27:51,  5.52s/it] 70%|██████▉   | 698/1000 [1:02:48<24:21,  4.84s/it] 70%|██████▉   | 699/1000 [1:02:54<25:58,  5.18s/it] 70%|███████   | 700/1000 [1:02:59<24:57,  4.99s/it] 70%|███████   | 701/1000 [1:03:02<21:55,  4.40s/it] 70%|███████   | 702/1000 [1:03:03<17:00,  3.43s/it] 70%|███████   | 703/1000 [1:03:08<19:30,  3.94s/it] 70%|███████   | 704/1000 [1:03:15<23:03,  4.67s/it] 70%|███████   | 705/1000 [1:03:18<21:04,  4.29s/it] 71%|███████   | 706/1000 [1:03:22<20:14,  4.13s/it] 71%|███████   | 707/1000 [1:03:25<19:18,  3.95s/it] 71%|███████   | 708/1000 [1:03:28<18:03,  3.71s/it] 71%|███████   | 709/1000 [1:03:30<14:51,  3.06s/it] 71%|███████   | 710/1000 [1:03:34<16:23,  3.39s/it] 71%|███████   | 711/1000 [1:03:39<19:15,  4.00s/it] 71%|███████   | 712/1000 [1:03:42<17:26,  3.63s/it] 71%|███████▏  | 713/1000 [1:03:46<16:58,  3.55s/it] 71%|███████▏  | 714/1000 [1:03:49<16:30,  3.46s/it] 72%|███████▏  | 715/1000 [1:03:54<18:09,  3.82s/it] 72%|███████▏  | 716/1000 [1:03:57<17:21,  3.67s/it] 72%|███████▏  | 717/1000 [1:03:59<15:11,  3.22s/it] 72%|███████▏  | 718/1000 [1:04:04<17:53,  3.81s/it] 72%|███████▏  | 719/1000 [1:04:10<20:27,  4.37s/it] 72%|███████▏  | 720/1000 [1:04:16<22:12,  4.76s/it] 72%|███████▏  | 721/1000 [1:04:20<21:50,  4.70s/it] 72%|███████▏  | 722/1000 [1:04:27<24:55,  5.38s/it] 72%|███████▏  | 723/1000 [1:04:30<21:48,  4.72s/it] 72%|███████▏  | 724/1000 [1:04:43<33:12,  7.22s/it] 72%|███████▎  | 725/1000 [1:04:48<29:17,  6.39s/it] 73%|███████▎  | 726/1000 [1:04:53<27:17,  5.97s/it] 73%|███████▎  | 727/1000 [1:04:57<24:11,  5.32s/it] 73%|███████▎  | 728/1000 [1:04:59<20:43,  4.57s/it] 73%|███████▎  | 729/1000 [1:05:03<19:38,  4.35s/it] 73%|███████▎  | 730/1000 [1:05:06<17:12,  3.82s/it] 73%|███████▎  | 731/1000 [1:05:09<15:57,  3.56s/it] 73%|███████▎  | 732/1000 [1:05:12<15:11,  3.40s/it] 73%|███████▎  | 733/1000 [1:05:15<15:01,  3.38s/it] 73%|███████▎  | 734/1000 [1:05:22<18:59,  4.28s/it] 74%|███████▎  | 735/1000 [1:05:25<17:26,  3.95s/it] 74%|███████▎  | 736/1000 [1:05:27<14:47,  3.36s/it] 74%|███████▎  | 737/1000 [1:05:32<17:36,  4.02s/it] 74%|███████▍  | 738/1000 [1:05:39<20:49,  4.77s/it] 74%|███████▍  | 739/1000 [1:05:50<29:15,  6.73s/it] 74%|███████▍  | 740/1000 [1:05:59<32:05,  7.40s/it] 74%|███████▍  | 741/1000 [1:06:05<30:11,  7.00s/it] 74%|███████▍  | 742/1000 [1:06:10<27:05,  6.30s/it] 74%|███████▍  | 743/1000 [1:06:15<25:40,  5.99s/it] 74%|███████▍  | 744/1000 [1:06:19<23:26,  5.50s/it] 74%|███████▍  | 745/1000 [1:06:26<24:47,  5.83s/it] 75%|███████▍  | 746/1000 [1:06:34<26:56,  6.36s/it] 75%|███████▍  | 747/1000 [1:06:38<24:09,  5.73s/it] 75%|███████▍  | 748/1000 [1:06:44<25:01,  5.96s/it] 75%|███████▍  | 749/1000 [1:06:51<25:45,  6.16s/it] 75%|███████▌  | 750/1000 [1:06:57<25:59,  6.24s/it] 75%|███████▌  | 751/1000 [1:07:02<23:19,  5.62s/it] 75%|███████▌  | 752/1000 [1:07:04<18:42,  4.53s/it] 75%|███████▌  | 753/1000 [1:07:07<17:57,  4.36s/it] 75%|███████▌  | 754/1000 [1:07:13<19:15,  4.70s/it] 76%|███████▌  | 755/1000 [1:07:18<20:11,  4.95s/it] 76%|███████▌  | 756/1000 [1:07:23<19:30,  4.80s/it] 76%|███████▌  | 757/1000 [1:07:28<20:07,  4.97s/it] 76%|███████▌  | 758/1000 [1:07:32<18:11,  4.51s/it] 76%|███████▌  | 759/1000 [1:07:38<20:41,  5.15s/it] 76%|███████▌  | 760/1000 [1:07:46<23:45,  5.94s/it] 76%|███████▌  | 761/1000 [1:07:52<23:30,  5.90s/it] 76%|███████▌  | 762/1000 [1:07:56<21:30,  5.42s/it] 76%|███████▋  | 763/1000 [1:08:07<28:07,  7.12s/it] 76%|███████▋  | 764/1000 [1:08:11<23:50,  6.06s/it] 76%|███████▋  | 765/1000 [1:08:15<21:09,  5.40s/it] 77%|███████▋  | 766/1000 [1:08:20<20:42,  5.31s/it] 77%|███████▋  | 767/1000 [1:08:23<17:56,  4.62s/it] 77%|███████▋  | 768/1000 [1:08:26<16:07,  4.17s/it] 77%|███████▋  | 769/1000 [1:08:32<17:53,  4.65s/it] 77%|███████▋  | 770/1000 [1:08:34<15:31,  4.05s/it] 77%|███████▋  | 771/1000 [1:08:37<14:14,  3.73s/it] 77%|███████▋  | 772/1000 [1:08:41<13:47,  3.63s/it] 77%|███████▋  | 773/1000 [1:08:45<14:22,  3.80s/it] 77%|███████▋  | 774/1000 [1:08:54<19:44,  5.24s/it] 78%|███████▊  | 775/1000 [1:08:57<18:02,  4.81s/it] 78%|███████▊  | 776/1000 [1:09:05<20:38,  5.53s/it] 78%|███████▊  | 777/1000 [1:09:09<19:20,  5.21s/it] 78%|███████▊  | 778/1000 [1:09:15<19:58,  5.40s/it] 78%|███████▊  | 779/1000 [1:09:23<22:28,  6.10s/it] 78%|███████▊  | 780/1000 [1:09:27<19:55,  5.43s/it] 78%|███████▊  | 781/1000 [1:09:34<22:06,  6.06s/it] 78%|███████▊  | 782/1000 [1:09:42<23:55,  6.59s/it] 78%|███████▊  | 783/1000 [1:09:46<21:23,  5.91s/it] 78%|███████▊  | 784/1000 [1:09:51<19:44,  5.48s/it] 78%|███████▊  | 785/1000 [1:09:58<21:16,  5.94s/it] 79%|███████▊  | 786/1000 [1:10:00<17:09,  4.81s/it] 79%|███████▊  | 787/1000 [1:10:03<15:27,  4.36s/it] 79%|███████▉  | 788/1000 [1:10:08<16:00,  4.53s/it] 79%|███████▉  | 789/1000 [1:10:14<17:23,  4.95s/it] 79%|███████▉  | 790/1000 [1:10:16<14:38,  4.18s/it] 79%|███████▉  | 791/1000 [1:10:20<13:46,  3.95s/it] 79%|███████▉  | 792/1000 [1:10:23<13:16,  3.83s/it] 79%|███████▉  | 793/1000 [1:10:33<19:11,  5.56s/it] 79%|███████▉  | 794/1000 [1:10:41<21:27,  6.25s/it] 80%|███████▉  | 795/1000 [1:10:45<19:13,  5.63s/it] 80%|███████▉  | 796/1000 [1:10:56<24:54,  7.33s/it] 80%|███████▉  | 797/1000 [1:11:02<22:42,  6.71s/it] 80%|███████▉  | 798/1000 [1:11:07<21:01,  6.25s/it] 80%|███████▉  | 799/1000 [1:11:11<18:24,  5.49s/it] 80%|████████  | 800/1000 [1:11:15<16:57,  5.09s/it] 80%|████████  | 801/1000 [1:11:24<20:46,  6.26s/it] 80%|████████  | 802/1000 [1:11:30<21:08,  6.41s/it] 80%|████████  | 803/1000 [1:11:35<18:45,  5.71s/it] 80%|████████  | 804/1000 [1:11:40<18:50,  5.77s/it] 80%|████████  | 805/1000 [1:11:43<15:33,  4.79s/it] 81%|████████  | 806/1000 [1:11:46<13:25,  4.15s/it] 81%|████████  | 807/1000 [1:11:50<13:21,  4.16s/it] 81%|████████  | 808/1000 [1:11:53<12:50,  4.02s/it] 81%|████████  | 809/1000 [1:11:56<11:35,  3.64s/it] 81%|████████  | 810/1000 [1:11:59<11:04,  3.50s/it] 81%|████████  | 811/1000 [1:12:04<11:40,  3.71s/it] 81%|████████  | 812/1000 [1:12:09<13:39,  4.36s/it] 81%|████████▏ | 813/1000 [1:12:14<13:55,  4.47s/it] 81%|████████▏ | 814/1000 [1:12:18<13:01,  4.20s/it] 82%|████████▏ | 815/1000 [1:12:23<13:34,  4.40s/it] 82%|████████▏ | 816/1000 [1:12:30<15:55,  5.19s/it] 82%|████████▏ | 817/1000 [1:12:35<15:56,  5.23s/it] 82%|████████▏ | 818/1000 [1:12:40<15:53,  5.24s/it] 82%|████████▏ | 819/1000 [1:12:45<15:27,  5.13s/it] 82%|████████▏ | 820/1000 [1:12:48<13:29,  4.50s/it] 82%|████████▏ | 821/1000 [1:13:03<23:04,  7.74s/it] 82%|████████▏ | 822/1000 [1:13:08<20:13,  6.82s/it] 82%|████████▏ | 823/1000 [1:13:12<17:57,  6.08s/it] 82%|████████▏ | 824/1000 [1:13:17<16:42,  5.69s/it] 82%|████████▎ | 825/1000 [1:13:21<14:55,  5.12s/it] 83%|████████▎ | 826/1000 [1:13:24<13:09,  4.53s/it] 83%|████████▎ | 827/1000 [1:13:33<16:44,  5.81s/it] 83%|████████▎ | 828/1000 [1:13:40<17:44,  6.19s/it] 83%|████████▎ | 829/1000 [1:13:49<20:14,  7.10s/it] 83%|████████▎ | 830/1000 [1:13:54<18:12,  6.43s/it] 83%|████████▎ | 831/1000 [1:13:59<17:05,  6.07s/it] 83%|████████▎ | 832/1000 [1:14:10<20:39,  7.38s/it] 83%|████████▎ | 833/1000 [1:14:13<16:54,  6.07s/it] 83%|████████▎ | 834/1000 [1:14:16<14:32,  5.25s/it] 84%|████████▎ | 835/1000 [1:14:21<14:06,  5.13s/it] 84%|████████▎ | 836/1000 [1:14:25<13:11,  4.83s/it] 84%|████████▎ | 837/1000 [1:14:33<15:23,  5.66s/it] 84%|████████▍ | 838/1000 [1:14:35<12:36,  4.67s/it] 84%|████████▍ | 839/1000 [1:14:40<12:43,  4.74s/it] 84%|████████▍ | 840/1000 [1:14:45<12:38,  4.74s/it] 84%|████████▍ | 841/1000 [1:14:49<12:22,  4.67s/it] 84%|████████▍ | 842/1000 [1:14:52<10:45,  4.08s/it] 84%|████████▍ | 843/1000 [1:15:01<14:24,  5.51s/it] 84%|████████▍ | 844/1000 [1:15:05<13:13,  5.09s/it] 84%|████████▍ | 845/1000 [1:15:08<11:50,  4.58s/it] 85%|████████▍ | 846/1000 [1:15:12<11:26,  4.46s/it] 85%|████████▍ | 847/1000 [1:15:18<11:54,  4.67s/it] 85%|████████▍ | 848/1000 [1:15:24<13:00,  5.13s/it] 85%|████████▍ | 849/1000 [1:15:31<14:15,  5.67s/it] 85%|████████▌ | 850/1000 [1:15:37<14:39,  5.86s/it] 85%|████████▌ | 851/1000 [1:15:40<12:31,  5.04s/it] 85%|████████▌ | 852/1000 [1:15:43<10:40,  4.32s/it] 85%|████████▌ | 853/1000 [1:15:47<10:06,  4.13s/it] 85%|████████▌ | 854/1000 [1:15:52<10:47,  4.44s/it] 86%|████████▌ | 855/1000 [1:15:55<09:51,  4.08s/it] 86%|████████▌ | 856/1000 [1:16:00<10:50,  4.52s/it] 86%|████████▌ | 857/1000 [1:16:07<11:51,  4.98s/it] 86%|████████▌ | 858/1000 [1:16:12<12:24,  5.24s/it] 86%|████████▌ | 859/1000 [1:16:19<13:12,  5.62s/it] 86%|████████▌ | 860/1000 [1:16:24<12:51,  5.51s/it] 86%|████████▌ | 861/1000 [1:16:31<13:40,  5.90s/it] 86%|████████▌ | 862/1000 [1:16:33<11:13,  4.88s/it] 86%|████████▋ | 863/1000 [1:16:37<10:25,  4.56s/it] 86%|████████▋ | 864/1000 [1:16:41<09:57,  4.39s/it] 86%|████████▋ | 865/1000 [1:16:48<11:43,  5.21s/it] 87%|████████▋ | 866/1000 [1:16:54<11:48,  5.29s/it] 87%|████████▋ | 867/1000 [1:17:01<12:48,  5.78s/it] 87%|████████▋ | 868/1000 [1:17:05<11:29,  5.22s/it] 87%|████████▋ | 869/1000 [1:17:07<09:48,  4.50s/it] 87%|████████▋ | 870/1000 [1:17:10<08:35,  3.96s/it] 87%|████████▋ | 871/1000 [1:17:15<08:47,  4.09s/it] 87%|████████▋ | 872/1000 [1:17:34<18:15,  8.56s/it] 87%|████████▋ | 873/1000 [1:17:38<15:37,  7.38s/it] 87%|████████▋ | 874/1000 [1:17:46<15:31,  7.40s/it] 88%|████████▊ | 875/1000 [1:17:56<17:09,  8.24s/it] 88%|████████▊ | 876/1000 [1:18:02<15:38,  7.57s/it] 88%|████████▊ | 877/1000 [1:18:10<15:45,  7.69s/it] 88%|████████▊ | 878/1000 [1:18:13<13:10,  6.48s/it] 88%|████████▊ | 879/1000 [1:18:20<12:50,  6.37s/it] 88%|████████▊ | 880/1000 [1:18:26<12:33,  6.28s/it] 88%|████████▊ | 881/1000 [1:18:32<12:15,  6.18s/it] 88%|████████▊ | 882/1000 [1:18:35<10:26,  5.31s/it] 88%|████████▊ | 883/1000 [1:18:38<09:09,  4.70s/it] 88%|████████▊ | 884/1000 [1:18:42<08:44,  4.52s/it] 88%|████████▊ | 885/1000 [1:18:55<13:25,  7.01s/it] 89%|████████▊ | 886/1000 [1:19:03<13:34,  7.14s/it] 89%|████████▊ | 887/1000 [1:19:06<11:25,  6.06s/it] 89%|████████▉ | 888/1000 [1:19:11<10:57,  5.87s/it] 89%|████████▉ | 889/1000 [1:19:16<10:19,  5.58s/it] 89%|████████▉ | 890/1000 [1:19:26<12:28,  6.81s/it] 89%|████████▉ | 891/1000 [1:19:30<10:51,  5.98s/it] 89%|████████▉ | 892/1000 [1:19:36<10:37,  5.90s/it] 89%|████████▉ | 893/1000 [1:19:39<09:14,  5.18s/it] 89%|████████▉ | 894/1000 [1:19:44<08:38,  4.89s/it] 90%|████████▉ | 895/1000 [1:19:48<08:19,  4.76s/it] 90%|████████▉ | 896/1000 [1:19:56<10:05,  5.82s/it] 90%|████████▉ | 897/1000 [1:20:03<10:13,  5.96s/it] 90%|████████▉ | 898/1000 [1:20:14<13:02,  7.67s/it] 90%|████████▉ | 899/1000 [1:20:19<11:27,  6.80s/it] 90%|█████████ | 900/1000 [1:20:22<09:31,  5.72s/it] 90%|█████████ | 901/1000 [1:20:30<10:37,  6.44s/it] 90%|█████████ | 902/1000 [1:20:34<09:01,  5.52s/it] 90%|█████████ | 903/1000 [1:20:38<08:25,  5.22s/it] 90%|█████████ | 904/1000 [1:20:43<07:55,  4.95s/it] 90%|█████████ | 905/1000 [1:20:48<07:56,  5.02s/it] 91%|█████████ | 906/1000 [1:20:51<07:04,  4.52s/it] 91%|█████████ | 907/1000 [1:20:56<07:09,  4.62s/it] 91%|█████████ | 908/1000 [1:21:02<07:35,  4.95s/it] 91%|█████████ | 909/1000 [1:21:04<06:16,  4.14s/it] 91%|█████████ | 910/1000 [1:21:08<06:24,  4.27s/it] 91%|█████████ | 911/1000 [1:21:11<05:43,  3.86s/it] 91%|█████████ | 912/1000 [1:21:15<05:22,  3.66s/it] 91%|█████████▏| 913/1000 [1:21:17<04:57,  3.42s/it] 91%|█████████▏| 914/1000 [1:21:20<04:33,  3.18s/it] 92%|█████████▏| 915/1000 [1:21:23<04:22,  3.09s/it] 92%|█████████▏| 916/1000 [1:21:26<04:16,  3.06s/it] 92%|█████████▏| 917/1000 [1:21:33<05:48,  4.19s/it] 92%|█████████▏| 918/1000 [1:21:38<05:58,  4.37s/it] 92%|█████████▏| 919/1000 [1:21:41<05:25,  4.02s/it] 92%|█████████▏| 920/1000 [1:21:44<04:51,  3.65s/it] 92%|█████████▏| 921/1000 [1:21:47<04:50,  3.67s/it] 92%|█████████▏| 922/1000 [1:21:50<04:19,  3.33s/it] 92%|█████████▏| 923/1000 [1:21:53<04:13,  3.29s/it] 92%|█████████▏| 924/1000 [1:21:56<03:57,  3.13s/it] 92%|█████████▎| 925/1000 [1:21:59<03:49,  3.05s/it] 93%|█████████▎| 926/1000 [1:22:03<04:19,  3.51s/it] 93%|█████████▎| 927/1000 [1:22:10<05:34,  4.58s/it] 93%|█████████▎| 928/1000 [1:22:15<05:28,  4.56s/it] 93%|█████████▎| 929/1000 [1:22:21<06:03,  5.11s/it] 93%|█████████▎| 930/1000 [1:22:24<05:05,  4.37s/it] 93%|█████████▎| 931/1000 [1:22:30<05:33,  4.83s/it] 93%|█████████▎| 932/1000 [1:22:32<04:40,  4.12s/it] 93%|█████████▎| 933/1000 [1:22:37<04:48,  4.31s/it] 93%|█████████▎| 934/1000 [1:22:45<05:54,  5.38s/it] 94%|█████████▎| 935/1000 [1:22:49<05:27,  5.04s/it] 94%|█████████▎| 936/1000 [1:22:53<05:11,  4.86s/it] 94%|█████████▎| 937/1000 [1:23:00<05:29,  5.23s/it] 94%|█████████▍| 938/1000 [1:23:10<07:02,  6.81s/it] 94%|█████████▍| 939/1000 [1:23:14<05:56,  5.84s/it] 94%|█████████▍| 940/1000 [1:23:22<06:41,  6.69s/it] 94%|█████████▍| 941/1000 [1:23:32<07:22,  7.51s/it] 94%|█████████▍| 942/1000 [1:23:36<06:26,  6.67s/it] 94%|█████████▍| 943/1000 [1:23:40<05:26,  5.73s/it] 94%|█████████▍| 944/1000 [1:23:42<04:16,  4.58s/it] 94%|█████████▍| 945/1000 [1:23:46<04:07,  4.50s/it] 95%|█████████▍| 946/1000 [1:23:51<04:14,  4.72s/it] 95%|█████████▍| 947/1000 [1:23:56<04:00,  4.55s/it] 95%|█████████▍| 948/1000 [1:24:01<04:09,  4.80s/it] 95%|█████████▍| 949/1000 [1:24:07<04:27,  5.25s/it] 95%|█████████▌| 950/1000 [1:24:12<04:10,  5.02s/it] 95%|█████████▌| 951/1000 [1:24:16<03:58,  4.87s/it] 95%|█████████▌| 952/1000 [1:24:19<03:27,  4.32s/it] 95%|█████████▌| 953/1000 [1:24:23<03:19,  4.26s/it] 95%|█████████▌| 954/1000 [1:24:28<03:19,  4.34s/it] 96%|█████████▌| 955/1000 [1:24:34<03:33,  4.75s/it] 96%|█████████▌| 956/1000 [1:24:39<03:34,  4.87s/it] 96%|█████████▌| 957/1000 [1:24:43<03:15,  4.55s/it] 96%|█████████▌| 958/1000 [1:24:48<03:19,  4.76s/it] 96%|█████████▌| 959/1000 [1:24:52<03:05,  4.53s/it] 96%|█████████▌| 960/1000 [1:24:58<03:25,  5.14s/it] 96%|█████████▌| 961/1000 [1:25:05<03:34,  5.50s/it] 96%|█████████▌| 962/1000 [1:25:12<03:46,  5.96s/it] 96%|█████████▋| 963/1000 [1:25:22<04:25,  7.19s/it] 96%|█████████▋| 964/1000 [1:25:29<04:17,  7.16s/it] 96%|█████████▋| 965/1000 [1:25:35<04:01,  6.91s/it] 97%|█████████▋| 966/1000 [1:25:38<03:12,  5.67s/it] 97%|█████████▋| 967/1000 [1:25:40<02:26,  4.43s/it] 97%|█████████▋| 968/1000 [1:25:43<02:09,  4.04s/it] 97%|█████████▋| 969/1000 [1:25:46<01:54,  3.69s/it] 97%|█████████▋| 970/1000 [1:25:51<02:01,  4.06s/it] 97%|█████████▋| 971/1000 [1:25:53<01:43,  3.58s/it] 97%|█████████▋| 972/1000 [1:25:55<01:25,  3.05s/it] 97%|█████████▋| 973/1000 [1:25:57<01:19,  2.94s/it] 97%|█████████▋| 974/1000 [1:26:02<01:32,  3.56s/it] 98%|█████████▊| 975/1000 [1:26:09<01:49,  4.36s/it] 98%|█████████▊| 976/1000 [1:26:12<01:39,  4.15s/it] 98%|█████████▊| 977/1000 [1:26:23<02:23,  6.23s/it] 98%|█████████▊| 978/1000 [1:26:32<02:29,  6.80s/it] 98%|█████████▊| 979/1000 [1:26:39<02:25,  6.93s/it] 98%|█████████▊| 980/1000 [1:26:42<01:54,  5.74s/it] 98%|█████████▊| 981/1000 [1:26:47<01:43,  5.46s/it] 98%|█████████▊| 982/1000 [1:26:53<01:43,  5.76s/it] 98%|█████████▊| 983/1000 [1:27:05<02:07,  7.48s/it] 98%|█████████▊| 984/1000 [1:27:07<01:37,  6.12s/it] 98%|█████████▊| 985/1000 [1:27:13<01:29,  5.99s/it] 99%|█████████▊| 986/1000 [1:27:18<01:17,  5.51s/it] 99%|█████████▊| 987/1000 [1:27:23<01:11,  5.48s/it] 99%|█████████▉| 988/1000 [1:27:25<00:55,  4.60s/it] 99%|█████████▉| 989/1000 [1:27:29<00:47,  4.36s/it] 99%|█████████▉| 990/1000 [1:27:32<00:38,  3.86s/it] 99%|█████████▉| 991/1000 [1:27:39<00:42,  4.67s/it] 99%|█████████▉| 992/1000 [1:27:45<00:42,  5.34s/it] 99%|█████████▉| 993/1000 [1:27:50<00:35,  5.13s/it] 99%|█████████▉| 994/1000 [1:27:55<00:30,  5.10s/it]100%|█████████▉| 995/1000 [1:28:02<00:28,  5.63s/it]100%|█████████▉| 996/1000 [1:28:05<00:19,  4.78s/it]100%|█████████▉| 997/1000 [1:28:19<00:22,  7.66s/it]100%|█████████▉| 998/1000 [1:28:25<00:14,  7.08s/it]100%|█████████▉| 999/1000 [1:28:28<00:05,  5.82s/it]100%|██████████| 1000/1000 [1:28:30<00:00,  4.69s/it]100%|██████████| 1000/1000 [1:28:30<00:00,  5.31s/it]
