/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
ClipTestArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
attn_implementation=flash_attention_2,
augmentation=False,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
consecutive_n_frames_threshold=1,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
dataset_config=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
embed_mark=2fps_384_1+3x3,
end_idx=2000,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=no,
eval_use_gather_object=False,
evaluation_strategy=None,
finetune_modules=['connector', 'mm_projector', 'response_head', 'related_head'],
first_n_frames_no_generate=0,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
frame_fps=2.0,
frame_num_tokens=49,
frame_resolution=384,
frame_token_cls=False,
frame_token_pooled=[7, 7],
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
grounding_mode=False,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
input_dir=/share2/wangyq/projects/MMDuet/datasets/shot2story/videos,
is_online_model=True,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
live_version=test,
llm_pretrained=lmms-lab/llava-onevision-qwen2-7b-ov,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=outputs/debug/runs/Apr19_15-00-43_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lora_alpha=32,
lora_modules=model\.layers.*(q_proj|k_proj|v_proj|o_proj|gate_proj|up_proj|down_proj)$,
lora_pretrained=None,
lora_r=16,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_num_frames=400,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
no_output_before_user_input=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=outputs/debug,
output_fname=outputs/llava-ov/magqa/4sec.jsonl.1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_assistant_turns=False,
remove_unused_columns=True,
repetition_penalty=None,
report_to=['tensorboard', 'wandb'],
response_min_interval_frames=None,
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=outputs/debug,
running_list_length=20,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
score_heads=informative_score,
seed=42,
skip_memory_metrics=True,
split_batches=None,
start_idx=1000,
stream_end_prob_threshold=None,
stream_end_score_sum_threshold=None,
stream_loss_weight=1.0,
system_prompt=A multimodal AI assistant is helping users with some activities. Below is their conversation, interleaved with the list of video frames received by the assistant.,
test_fname=/share2/wangyq/projects/MMDuet/datasets/shot2story/annotations/magqa_test.json,
tf32=None,
threshold_z=None,
time_instruction_format=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
v_placeholder=<image>,
video_chunk_sec=4,
video_pooling_stride=4,
vision_pretrained=google/siglip-large-patch16-384,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Loaded LLaVA model: lmms-lab/llava-onevision-qwen2-7b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 437.49it/s]
Loading vision tower: google/siglip-so400m-patch14-384
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.patch_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.patch_embedding.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.position_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.probe: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.attention.in_proj_weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.attention.in_proj_bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.attention.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.attention.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:20<01:01, 20.58s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:41<00:41, 20.99s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:04<00:21, 21.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:10<00:00, 15.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:10<00:00, 17.68s/it]
Model Class: LlavaQwenForCausalLM
  0%|          | 0/1000 [00:00<?, ?it/s]/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:612: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  0%|          | 1/1000 [00:16<4:38:17, 16.71s/it]  0%|          | 2/1000 [00:27<3:39:54, 13.22s/it]  0%|          | 3/1000 [00:32<2:37:35,  9.48s/it]  0%|          | 4/1000 [00:40<2:30:48,  9.08s/it]  0%|          | 5/1000 [00:44<1:54:39,  6.91s/it]  1%|          | 6/1000 [00:48<1:43:16,  6.23s/it]  1%|          | 7/1000 [00:52<1:30:11,  5.45s/it]  1%|          | 8/1000 [00:57<1:27:07,  5.27s/it]  1%|          | 9/1000 [01:06<1:46:08,  6.43s/it]  1%|          | 10/1000 [01:11<1:38:50,  5.99s/it]  1%|          | 11/1000 [01:19<1:47:50,  6.54s/it]  1%|          | 12/1000 [01:39<2:56:27, 10.72s/it]  1%|▏         | 13/1000 [01:47<2:41:12,  9.80s/it]  1%|▏         | 14/1000 [01:51<2:13:41,  8.14s/it]  2%|▏         | 15/1000 [02:00<2:15:12,  8.24s/it]  2%|▏         | 16/1000 [02:08<2:13:08,  8.12s/it]  2%|▏         | 17/1000 [02:15<2:11:34,  8.03s/it]  2%|▏         | 18/1000 [02:23<2:11:31,  8.04s/it]  2%|▏         | 19/1000 [02:32<2:13:52,  8.19s/it]  2%|▏         | 20/1000 [02:41<2:16:57,  8.38s/it]  2%|▏         | 21/1000 [02:48<2:11:21,  8.05s/it]  2%|▏         | 22/1000 [02:56<2:08:59,  7.91s/it]  2%|▏         | 23/1000 [03:05<2:14:51,  8.28s/it]  2%|▏         | 24/1000 [03:15<2:24:26,  8.88s/it]  2%|▎         | 25/1000 [03:22<2:16:57,  8.43s/it]  3%|▎         | 26/1000 [03:28<2:02:06,  7.52s/it]  3%|▎         | 27/1000 [03:36<2:05:39,  7.75s/it]  3%|▎         | 28/1000 [03:42<1:58:21,  7.31s/it]  3%|▎         | 29/1000 [03:47<1:44:14,  6.44s/it]  3%|▎         | 30/1000 [03:52<1:38:44,  6.11s/it]  3%|▎         | 31/1000 [04:00<1:47:19,  6.65s/it]  3%|▎         | 32/1000 [04:04<1:33:59,  5.83s/it]  3%|▎         | 33/1000 [04:07<1:20:28,  4.99s/it]  3%|▎         | 34/1000 [04:12<1:18:00,  4.85s/it]  4%|▎         | 35/1000 [04:18<1:25:38,  5.32s/it]  4%|▎         | 36/1000 [04:27<1:43:05,  6.42s/it]  4%|▎         | 37/1000 [04:34<1:45:16,  6.56s/it]  4%|▍         | 38/1000 [04:41<1:48:20,  6.76s/it]  4%|▍         | 39/1000 [04:48<1:47:00,  6.68s/it]  4%|▍         | 40/1000 [04:54<1:47:46,  6.74s/it]  4%|▍         | 41/1000 [05:04<2:02:02,  7.64s/it]  4%|▍         | 42/1000 [05:13<2:09:34,  8.12s/it]  4%|▍         | 43/1000 [05:17<1:50:17,  6.92s/it]  4%|▍         | 44/1000 [05:21<1:32:22,  5.80s/it]  4%|▍         | 45/1000 [05:28<1:37:49,  6.15s/it]  5%|▍         | 46/1000 [05:33<1:35:01,  5.98s/it]  5%|▍         | 47/1000 [05:41<1:45:25,  6.64s/it]  5%|▍         | 48/1000 [05:50<1:53:07,  7.13s/it]  5%|▍         | 49/1000 [05:56<1:48:22,  6.84s/it]  5%|▌         | 50/1000 [05:59<1:32:32,  5.84s/it]  5%|▌         | 51/1000 [06:04<1:25:00,  5.37s/it]  5%|▌         | 52/1000 [06:16<1:59:59,  7.59s/it]  5%|▌         | 53/1000 [06:19<1:36:55,  6.14s/it]  5%|▌         | 54/1000 [06:23<1:24:01,  5.33s/it]  6%|▌         | 55/1000 [06:28<1:23:20,  5.29s/it]  6%|▌         | 56/1000 [06:37<1:40:10,  6.37s/it]  6%|▌         | 57/1000 [06:44<1:43:13,  6.57s/it]  6%|▌         | 58/1000 [06:52<1:51:52,  7.13s/it]  6%|▌         | 59/1000 [06:56<1:35:56,  6.12s/it]  6%|▌         | 60/1000 [07:00<1:27:47,  5.60s/it]  6%|▌         | 61/1000 [07:11<1:52:26,  7.18s/it]  6%|▌         | 62/1000 [07:20<2:01:47,  7.79s/it]  6%|▋         | 63/1000 [07:25<1:47:56,  6.91s/it]  6%|▋         | 64/1000 [07:34<1:56:31,  7.47s/it]  6%|▋         | 65/1000 [07:42<1:58:22,  7.60s/it]  7%|▋         | 66/1000 [07:52<2:09:50,  8.34s/it]  7%|▋         | 67/1000 [07:57<1:53:21,  7.29s/it]  7%|▋         | 68/1000 [08:03<1:49:55,  7.08s/it]  7%|▋         | 69/1000 [08:10<1:46:35,  6.87s/it]  7%|▋         | 70/1000 [08:20<2:03:43,  7.98s/it]  7%|▋         | 71/1000 [08:27<1:57:23,  7.58s/it]  7%|▋         | 72/1000 [08:32<1:43:00,  6.66s/it]  7%|▋         | 73/1000 [08:35<1:28:07,  5.70s/it]  7%|▋         | 74/1000 [08:40<1:26:25,  5.60s/it]  8%|▊         | 75/1000 [08:45<1:21:37,  5.29s/it]  8%|▊         | 76/1000 [08:53<1:34:02,  6.11s/it]  8%|▊         | 77/1000 [08:59<1:31:58,  5.98s/it]  8%|▊         | 78/1000 [09:05<1:34:33,  6.15s/it]  8%|▊         | 79/1000 [09:14<1:47:27,  7.00s/it]  8%|▊         | 80/1000 [09:19<1:39:38,  6.50s/it]  8%|▊         | 81/1000 [09:30<1:59:24,  7.80s/it]  8%|▊         | 82/1000 [09:37<1:53:34,  7.42s/it]  8%|▊         | 83/1000 [09:45<1:57:05,  7.66s/it]  8%|▊         | 84/1000 [09:51<1:50:56,  7.27s/it]  8%|▊         | 85/1000 [09:57<1:43:28,  6.79s/it]  9%|▊         | 86/1000 [10:06<1:54:19,  7.51s/it]  9%|▊         | 87/1000 [10:12<1:43:46,  6.82s/it]  9%|▉         | 88/1000 [10:18<1:42:50,  6.77s/it]  9%|▉         | 89/1000 [10:23<1:35:45,  6.31s/it]  9%|▉         | 90/1000 [10:26<1:17:03,  5.08s/it]  9%|▉         | 91/1000 [10:33<1:26:16,  5.69s/it]  9%|▉         | 92/1000 [10:37<1:21:19,  5.37s/it]  9%|▉         | 93/1000 [10:45<1:33:35,  6.19s/it]  9%|▉         | 94/1000 [10:49<1:23:04,  5.50s/it] 10%|▉         | 95/1000 [10:54<1:21:03,  5.37s/it] 10%|▉         | 96/1000 [10:59<1:16:16,  5.06s/it] 10%|▉         | 97/1000 [11:04<1:15:29,  5.02s/it] 10%|▉         | 98/1000 [11:11<1:26:22,  5.75s/it] 10%|▉         | 99/1000 [11:16<1:24:12,  5.61s/it] 10%|█         | 100/1000 [11:21<1:17:30,  5.17s/it] 10%|█         | 101/1000 [11:27<1:24:50,  5.66s/it] 10%|█         | 102/1000 [11:36<1:36:03,  6.42s/it] 10%|█         | 103/1000 [11:46<1:53:47,  7.61s/it] 10%|█         | 104/1000 [11:51<1:42:48,  6.88s/it] 10%|█         | 105/1000 [11:56<1:34:28,  6.33s/it] 11%|█         | 106/1000 [12:06<1:51:05,  7.46s/it] 11%|█         | 107/1000 [12:19<2:13:00,  8.94s/it] 11%|█         | 108/1000 [12:30<2:25:30,  9.79s/it] 11%|█         | 109/1000 [12:38<2:13:34,  9.00s/it] 11%|█         | 110/1000 [12:45<2:07:14,  8.58s/it] 11%|█         | 111/1000 [12:50<1:48:48,  7.34s/it] 11%|█         | 112/1000 [12:56<1:43:19,  6.98s/it] 11%|█▏        | 113/1000 [13:07<2:01:54,  8.25s/it] 11%|█▏        | 114/1000 [13:12<1:46:15,  7.20s/it] 12%|█▏        | 115/1000 [13:17<1:38:25,  6.67s/it] 12%|█▏        | 116/1000 [13:22<1:31:31,  6.21s/it] 12%|█▏        | 117/1000 [13:27<1:26:00,  5.84s/it] 12%|█▏        | 118/1000 [13:33<1:24:24,  5.74s/it] 12%|█▏        | 119/1000 [13:41<1:34:33,  6.44s/it] 12%|█▏        | 120/1000 [13:44<1:20:55,  5.52s/it] 12%|█▏        | 121/1000 [13:51<1:25:49,  5.86s/it] 12%|█▏        | 122/1000 [13:55<1:18:01,  5.33s/it] 12%|█▏        | 123/1000 [14:16<2:26:35, 10.03s/it] 12%|█▏        | 124/1000 [14:22<2:08:37,  8.81s/it] 12%|█▎        | 125/1000 [14:27<1:53:40,  7.80s/it] 13%|█▎        | 126/1000 [14:34<1:47:32,  7.38s/it] 13%|█▎        | 127/1000 [14:49<2:19:43,  9.60s/it] 13%|█▎        | 128/1000 [14:56<2:09:41,  8.92s/it] 13%|█▎        | 129/1000 [15:04<2:05:45,  8.66s/it] 13%|█▎        | 130/1000 [15:06<1:36:02,  6.62s/it] 13%|█▎        | 131/1000 [15:09<1:20:50,  5.58s/it] 13%|█▎        | 132/1000 [15:20<1:45:10,  7.27s/it] 13%|█▎        | 133/1000 [15:29<1:53:45,  7.87s/it] 13%|█▎        | 134/1000 [15:34<1:38:34,  6.83s/it] 14%|█▎        | 135/1000 [15:40<1:36:49,  6.72s/it] 14%|█▎        | 136/1000 [15:49<1:45:12,  7.31s/it] 14%|█▎        | 137/1000 [15:55<1:41:03,  7.03s/it] 14%|█▍        | 138/1000 [16:02<1:38:34,  6.86s/it] 14%|█▍        | 139/1000 [16:10<1:44:08,  7.26s/it] 14%|█▍        | 140/1000 [16:15<1:34:41,  6.61s/it] 14%|█▍        | 141/1000 [16:23<1:39:02,  6.92s/it] 14%|█▍        | 142/1000 [16:28<1:31:39,  6.41s/it] 14%|█▍        | 143/1000 [16:36<1:37:41,  6.84s/it] 14%|█▍        | 144/1000 [16:46<1:53:27,  7.95s/it] 14%|█▍        | 145/1000 [16:57<2:05:33,  8.81s/it] 15%|█▍        | 146/1000 [17:02<1:49:47,  7.71s/it] 15%|█▍        | 147/1000 [17:07<1:34:47,  6.67s/it] 15%|█▍        | 148/1000 [17:14<1:38:12,  6.92s/it] 15%|█▍        | 149/1000 [17:21<1:38:14,  6.93s/it] 15%|█▌        | 150/1000 [17:27<1:35:58,  6.78s/it] 15%|█▌        | 151/1000 [17:34<1:34:36,  6.69s/it] 15%|█▌        | 152/1000 [17:49<2:08:06,  9.06s/it] 15%|█▌        | 153/1000 [17:57<2:05:37,  8.90s/it] 15%|█▌        | 154/1000 [18:02<1:48:37,  7.70s/it] 16%|█▌        | 155/1000 [18:05<1:29:58,  6.39s/it] 16%|█▌        | 156/1000 [18:16<1:46:36,  7.58s/it] 16%|█▌        | 157/1000 [18:23<1:46:32,  7.58s/it] 16%|█▌        | 158/1000 [18:29<1:39:11,  7.07s/it] 16%|█▌        | 159/1000 [18:46<2:19:24,  9.95s/it] 16%|█▌        | 160/1000 [18:52<2:05:44,  8.98s/it] 16%|█▌        | 161/1000 [18:57<1:47:26,  7.68s/it] 16%|█▌        | 162/1000 [19:01<1:32:52,  6.65s/it] 16%|█▋        | 163/1000 [19:09<1:37:36,  7.00s/it] 16%|█▋        | 164/1000 [19:15<1:33:47,  6.73s/it] 16%|█▋        | 165/1000 [19:20<1:26:21,  6.21s/it] 17%|█▋        | 166/1000 [19:27<1:28:49,  6.39s/it] 17%|█▋        | 167/1000 [19:37<1:43:51,  7.48s/it] 17%|█▋        | 168/1000 [19:42<1:34:07,  6.79s/it] 17%|█▋        | 169/1000 [19:48<1:31:00,  6.57s/it] 17%|█▋        | 170/1000 [19:54<1:28:59,  6.43s/it] 17%|█▋        | 171/1000 [20:03<1:37:41,  7.07s/it] 17%|█▋        | 172/1000 [20:07<1:26:34,  6.27s/it] 17%|█▋        | 173/1000 [20:15<1:32:50,  6.74s/it] 17%|█▋        | 174/1000 [20:19<1:20:13,  5.83s/it] 18%|█▊        | 175/1000 [20:24<1:18:48,  5.73s/it] 18%|█▊        | 176/1000 [20:31<1:23:05,  6.05s/it] 18%|█▊        | 177/1000 [20:39<1:28:30,  6.45s/it] 18%|█▊        | 178/1000 [20:45<1:29:58,  6.57s/it] 18%|█▊        | 179/1000 [20:52<1:31:09,  6.66s/it] 18%|█▊        | 180/1000 [20:58<1:24:53,  6.21s/it] 18%|█▊        | 181/1000 [21:02<1:16:04,  5.57s/it] 18%|█▊        | 182/1000 [21:06<1:11:15,  5.23s/it] 18%|█▊        | 183/1000 [21:11<1:10:38,  5.19s/it] 18%|█▊        | 184/1000 [21:16<1:08:02,  5.00s/it] 18%|█▊        | 185/1000 [21:20<1:05:02,  4.79s/it] 19%|█▊        | 186/1000 [21:25<1:06:47,  4.92s/it] 19%|█▊        | 187/1000 [21:33<1:17:50,  5.75s/it] 19%|█▉        | 188/1000 [21:43<1:33:40,  6.92s/it] 19%|█▉        | 189/1000 [21:49<1:30:54,  6.73s/it] 19%|█▉        | 190/1000 [21:54<1:23:12,  6.16s/it] 19%|█▉        | 191/1000 [22:05<1:44:14,  7.73s/it] 19%|█▉        | 192/1000 [22:11<1:37:47,  7.26s/it] 19%|█▉        | 193/1000 [22:20<1:41:47,  7.57s/it] 19%|█▉        | 194/1000 [22:24<1:30:43,  6.75s/it] 20%|█▉        | 195/1000 [22:29<1:20:03,  5.97s/it] 20%|█▉        | 196/1000 [22:35<1:20:51,  6.03s/it] 20%|█▉        | 197/1000 [22:42<1:26:18,  6.45s/it] 20%|█▉        | 198/1000 [22:45<1:10:40,  5.29s/it] 20%|█▉        | 199/1000 [22:50<1:11:13,  5.34s/it] 20%|██        | 200/1000 [22:55<1:10:25,  5.28s/it] 20%|██        | 201/1000 [23:02<1:17:47,  5.84s/it] 20%|██        | 202/1000 [23:07<1:13:38,  5.54s/it] 20%|██        | 203/1000 [23:14<1:20:00,  6.02s/it] 20%|██        | 204/1000 [23:22<1:24:30,  6.37s/it] 20%|██        | 205/1000 [23:27<1:20:06,  6.05s/it] 21%|██        | 206/1000 [23:33<1:19:50,  6.03s/it] 21%|██        | 207/1000 [23:39<1:18:03,  5.91s/it] 21%|██        | 208/1000 [23:46<1:23:40,  6.34s/it] 21%|██        | 209/1000 [23:57<1:41:03,  7.67s/it] 21%|██        | 210/1000 [24:08<1:54:50,  8.72s/it] 21%|██        | 211/1000 [24:10<1:27:29,  6.65s/it] 21%|██        | 212/1000 [24:22<1:49:29,  8.34s/it] 21%|██▏       | 213/1000 [24:28<1:40:15,  7.64s/it] 21%|██▏       | 214/1000 [24:36<1:41:03,  7.71s/it] 22%|██▏       | 215/1000 [24:40<1:25:42,  6.55s/it] 22%|██▏       | 216/1000 [24:46<1:26:46,  6.64s/it] 22%|██▏       | 217/1000 [24:52<1:23:15,  6.38s/it] 22%|██▏       | 218/1000 [24:58<1:20:07,  6.15s/it] 22%|██▏       | 219/1000 [25:06<1:28:14,  6.78s/it] 22%|██▏       | 220/1000 [25:13<1:27:10,  6.71s/it] 22%|██▏       | 221/1000 [25:19<1:24:13,  6.49s/it] 22%|██▏       | 222/1000 [25:26<1:27:30,  6.75s/it] 22%|██▏       | 223/1000 [25:33<1:26:47,  6.70s/it] 22%|██▏       | 224/1000 [25:38<1:19:56,  6.18s/it] 22%|██▎       | 225/1000 [25:42<1:14:33,  5.77s/it] 23%|██▎       | 226/1000 [25:47<1:09:20,  5.38s/it] 23%|██▎       | 227/1000 [25:53<1:12:45,  5.65s/it] 23%|██▎       | 228/1000 [25:58<1:09:37,  5.41s/it] 23%|██▎       | 229/1000 [26:01<1:01:49,  4.81s/it] 23%|██▎       | 230/1000 [26:08<1:08:24,  5.33s/it] 23%|██▎       | 231/1000 [26:15<1:14:42,  5.83s/it] 23%|██▎       | 232/1000 [26:22<1:17:58,  6.09s/it] 23%|██▎       | 233/1000 [26:30<1:26:53,  6.80s/it] 23%|██▎       | 234/1000 [26:37<1:25:39,  6.71s/it] 24%|██▎       | 235/1000 [26:44<1:27:02,  6.83s/it] 24%|██▎       | 236/1000 [26:50<1:25:23,  6.71s/it] 24%|██▎       | 237/1000 [26:55<1:16:56,  6.05s/it] 24%|██▍       | 238/1000 [27:02<1:20:14,  6.32s/it] 24%|██▍       | 239/1000 [27:09<1:26:03,  6.78s/it] 24%|██▍       | 240/1000 [27:18<1:31:30,  7.22s/it] 24%|██▍       | 241/1000 [27:26<1:37:01,  7.67s/it] 24%|██▍       | 242/1000 [27:35<1:41:24,  8.03s/it] 24%|██▍       | 243/1000 [27:42<1:34:36,  7.50s/it] 24%|██▍       | 244/1000 [27:49<1:35:04,  7.55s/it] 24%|██▍       | 245/1000 [27:57<1:37:44,  7.77s/it] 25%|██▍       | 246/1000 [28:03<1:28:14,  7.02s/it] 25%|██▍       | 247/1000 [28:12<1:36:26,  7.68s/it] 25%|██▍       | 248/1000 [28:17<1:26:30,  6.90s/it] 25%|██▍       | 249/1000 [28:32<1:57:55,  9.42s/it] 25%|██▌       | 250/1000 [28:36<1:37:39,  7.81s/it] 25%|██▌       | 251/1000 [28:45<1:39:14,  7.95s/it] 25%|██▌       | 252/1000 [28:55<1:48:35,  8.71s/it] 25%|██▌       | 253/1000 [28:57<1:24:02,  6.75s/it] 25%|██▌       | 254/1000 [29:04<1:24:41,  6.81s/it] 26%|██▌       | 255/1000 [29:09<1:17:41,  6.26s/it] 26%|██▌       | 256/1000 [29:20<1:35:38,  7.71s/it] 26%|██▌       | 257/1000 [29:25<1:22:20,  6.65s/it] 26%|██▌       | 258/1000 [29:31<1:23:26,  6.75s/it] 26%|██▌       | 259/1000 [29:36<1:14:17,  6.02s/it] 26%|██▌       | 260/1000 [29:41<1:11:41,  5.81s/it] 26%|██▌       | 261/1000 [29:46<1:08:38,  5.57s/it] 26%|██▌       | 262/1000 [29:49<57:46,  4.70s/it]   26%|██▋       | 263/1000 [30:00<1:19:53,  6.50s/it] 26%|██▋       | 264/1000 [30:09<1:31:30,  7.46s/it] 26%|██▋       | 265/1000 [30:16<1:28:50,  7.25s/it] 27%|██▋       | 266/1000 [30:20<1:18:34,  6.42s/it] 27%|██▋       | 267/1000 [30:25<1:12:37,  5.94s/it] 27%|██▋       | 268/1000 [30:30<1:07:35,  5.54s/it] 27%|██▋       | 269/1000 [30:36<1:09:40,  5.72s/it] 27%|██▋       | 270/1000 [30:44<1:16:26,  6.28s/it] 27%|██▋       | 271/1000 [30:49<1:11:44,  5.90s/it] 27%|██▋       | 272/1000 [30:53<1:07:42,  5.58s/it] 27%|██▋       | 273/1000 [30:59<1:06:31,  5.49s/it] 27%|██▋       | 274/1000 [31:03<1:02:05,  5.13s/it] 28%|██▊       | 275/1000 [31:05<49:45,  4.12s/it]   28%|██▊       | 276/1000 [31:11<58:35,  4.86s/it] 28%|██▊       | 277/1000 [31:24<1:25:54,  7.13s/it] 28%|██▊       | 278/1000 [31:36<1:43:35,  8.61s/it] 28%|██▊       | 279/1000 [31:41<1:30:23,  7.52s/it] 28%|██▊       | 280/1000 [31:44<1:15:43,  6.31s/it] 28%|██▊       | 281/1000 [31:49<1:08:26,  5.71s/it] 28%|██▊       | 282/1000 [31:55<1:12:06,  6.03s/it] 28%|██▊       | 283/1000 [32:01<1:09:26,  5.81s/it] 28%|██▊       | 284/1000 [32:14<1:37:14,  8.15s/it] 28%|██▊       | 285/1000 [32:18<1:22:32,  6.93s/it] 29%|██▊       | 286/1000 [32:25<1:21:34,  6.85s/it] 29%|██▊       | 287/1000 [32:30<1:14:05,  6.23s/it] 29%|██▉       | 288/1000 [32:35<1:10:19,  5.93s/it] 29%|██▉       | 289/1000 [32:45<1:24:12,  7.11s/it] 29%|██▉       | 290/1000 [32:50<1:16:05,  6.43s/it] 29%|██▉       | 291/1000 [32:59<1:26:18,  7.30s/it] 29%|██▉       | 292/1000 [33:07<1:29:47,  7.61s/it] 29%|██▉       | 293/1000 [33:16<1:31:28,  7.76s/it] 29%|██▉       | 294/1000 [33:20<1:18:33,  6.68s/it] 30%|██▉       | 295/1000 [33:30<1:30:55,  7.74s/it] 30%|██▉       | 296/1000 [33:34<1:19:12,  6.75s/it] 30%|██▉       | 297/1000 [33:39<1:12:04,  6.15s/it] 30%|██▉       | 298/1000 [33:42<59:00,  5.04s/it]   30%|██▉       | 299/1000 [33:52<1:18:28,  6.72s/it] 30%|███       | 300/1000 [34:00<1:21:58,  7.03s/it] 30%|███       | 301/1000 [34:08<1:25:14,  7.32s/it] 30%|███       | 302/1000 [34:14<1:22:06,  7.06s/it] 30%|███       | 303/1000 [34:25<1:33:48,  8.08s/it] 30%|███       | 304/1000 [34:28<1:15:15,  6.49s/it] 30%|███       | 305/1000 [34:33<1:12:14,  6.24s/it] 31%|███       | 306/1000 [34:40<1:15:19,  6.51s/it] 31%|███       | 307/1000 [34:45<1:06:48,  5.78s/it] 31%|███       | 308/1000 [34:49<1:02:25,  5.41s/it] 31%|███       | 309/1000 [34:56<1:08:46,  5.97s/it] 31%|███       | 310/1000 [35:05<1:16:56,  6.69s/it] 31%|███       | 311/1000 [35:07<1:00:11,  5.24s/it] 31%|███       | 312/1000 [35:11<57:10,  4.99s/it]   31%|███▏      | 313/1000 [35:16<56:28,  4.93s/it] 31%|███▏      | 314/1000 [35:20<54:37,  4.78s/it] 32%|███▏      | 315/1000 [35:25<55:31,  4.86s/it] 32%|███▏      | 316/1000 [35:30<56:25,  4.95s/it] 32%|███▏      | 317/1000 [35:37<1:01:12,  5.38s/it] 32%|███▏      | 318/1000 [35:41<55:53,  4.92s/it]   32%|███▏      | 319/1000 [35:47<1:01:19,  5.40s/it] 32%|███▏      | 320/1000 [35:57<1:15:35,  6.67s/it] 32%|███▏      | 321/1000 [36:07<1:27:00,  7.69s/it] 32%|███▏      | 322/1000 [36:12<1:18:23,  6.94s/it] 32%|███▏      | 323/1000 [36:18<1:14:02,  6.56s/it] 32%|███▏      | 324/1000 [36:29<1:28:27,  7.85s/it] 32%|███▎      | 325/1000 [36:32<1:14:18,  6.61s/it] 33%|███▎      | 326/1000 [36:40<1:17:00,  6.86s/it] 33%|███▎      | 327/1000 [36:45<1:13:08,  6.52s/it] 33%|███▎      | 328/1000 [36:52<1:13:04,  6.52s/it] 33%|███▎      | 329/1000 [36:59<1:12:58,  6.53s/it] 33%|███▎      | 330/1000 [37:05<1:11:54,  6.44s/it] 33%|███▎      | 331/1000 [37:09<1:04:50,  5.81s/it] 33%|███▎      | 332/1000 [37:18<1:13:51,  6.63s/it] 33%|███▎      | 333/1000 [37:28<1:24:54,  7.64s/it] 33%|███▎      | 334/1000 [37:37<1:30:00,  8.11s/it] 34%|███▎      | 335/1000 [37:46<1:32:03,  8.31s/it] 34%|███▎      | 336/1000 [37:49<1:15:33,  6.83s/it] 34%|███▎      | 337/1000 [37:55<1:13:24,  6.64s/it] 34%|███▍      | 338/1000 [37:59<1:05:16,  5.92s/it] 34%|███▍      | 339/1000 [38:05<1:03:58,  5.81s/it] 34%|███▍      | 340/1000 [38:10<59:57,  5.45s/it]   34%|███▍      | 341/1000 [38:18<1:09:59,  6.37s/it] 34%|███▍      | 342/1000 [38:24<1:08:32,  6.25s/it] 34%|███▍      | 343/1000 [38:31<1:09:45,  6.37s/it] 34%|███▍      | 344/1000 [38:39<1:16:01,  6.95s/it] 34%|███▍      | 345/1000 [38:43<1:04:31,  5.91s/it] 35%|███▍      | 346/1000 [38:56<1:30:30,  8.30s/it] 35%|███▍      | 347/1000 [38:59<1:11:06,  6.53s/it] 35%|███▍      | 348/1000 [39:04<1:07:54,  6.25s/it] 35%|███▍      | 349/1000 [39:11<1:07:27,  6.22s/it] 35%|███▌      | 350/1000 [39:19<1:13:12,  6.76s/it] 35%|███▌      | 351/1000 [39:23<1:05:28,  6.05s/it] 35%|███▌      | 352/1000 [39:28<1:00:38,  5.62s/it] 35%|███▌      | 353/1000 [39:31<52:06,  4.83s/it]   35%|███▌      | 354/1000 [39:41<1:08:40,  6.38s/it] 36%|███▌      | 355/1000 [39:44<1:00:26,  5.62s/it] 36%|███▌      | 356/1000 [39:53<1:09:04,  6.44s/it] 36%|███▌      | 357/1000 [39:58<1:05:59,  6.16s/it] 36%|███▌      | 358/1000 [40:03<1:01:10,  5.72s/it] 36%|███▌      | 359/1000 [40:12<1:10:42,  6.62s/it] 36%|███▌      | 360/1000 [40:16<1:03:06,  5.92s/it] 36%|███▌      | 361/1000 [40:21<59:55,  5.63s/it]   36%|███▌      | 362/1000 [40:26<59:10,  5.57s/it] 36%|███▋      | 363/1000 [40:32<59:36,  5.61s/it] 36%|███▋      | 364/1000 [40:40<1:05:59,  6.23s/it] 36%|███▋      | 365/1000 [40:47<1:09:39,  6.58s/it] 37%|███▋      | 366/1000 [40:51<1:00:35,  5.73s/it] 37%|███▋      | 367/1000 [40:57<1:02:54,  5.96s/it] 37%|███▋      | 368/1000 [41:02<58:26,  5.55s/it]   37%|███▋      | 369/1000 [41:08<58:45,  5.59s/it] 37%|███▋      | 370/1000 [41:13<59:18,  5.65s/it] 37%|███▋      | 371/1000 [41:18<54:53,  5.24s/it] 37%|███▋      | 372/1000 [41:25<1:02:09,  5.94s/it] 37%|███▋      | 373/1000 [41:34<1:11:54,  6.88s/it] 37%|███▋      | 374/1000 [41:42<1:14:33,  7.15s/it] 38%|███▊      | 375/1000 [41:49<1:13:27,  7.05s/it] 38%|███▊      | 376/1000 [41:57<1:16:09,  7.32s/it] 38%|███▊      | 377/1000 [42:05<1:18:33,  7.57s/it] 38%|███▊      | 378/1000 [42:11<1:14:26,  7.18s/it] 38%|███▊      | 379/1000 [42:17<1:08:04,  6.58s/it] 38%|███▊      | 380/1000 [42:22<1:03:50,  6.18s/it] 38%|███▊      | 381/1000 [42:26<57:54,  5.61s/it]   38%|███▊      | 382/1000 [42:33<1:02:45,  6.09s/it] 38%|███▊      | 383/1000 [42:43<1:13:07,  7.11s/it] 38%|███▊      | 384/1000 [42:47<1:04:37,  6.30s/it] 38%|███▊      | 385/1000 [42:56<1:12:40,  7.09s/it] 39%|███▊      | 386/1000 [43:03<1:12:00,  7.04s/it] 39%|███▊      | 387/1000 [43:11<1:15:35,  7.40s/it] 39%|███▉      | 388/1000 [43:22<1:24:12,  8.26s/it] 39%|███▉      | 389/1000 [43:29<1:22:43,  8.12s/it] 39%|███▉      | 390/1000 [43:37<1:22:32,  8.12s/it] 39%|███▉      | 391/1000 [43:40<1:06:58,  6.60s/it] 39%|███▉      | 392/1000 [43:45<1:00:20,  5.96s/it] 39%|███▉      | 393/1000 [43:59<1:23:27,  8.25s/it] 39%|███▉      | 394/1000 [44:02<1:08:19,  6.77s/it] 40%|███▉      | 395/1000 [44:07<1:04:28,  6.39s/it] 40%|███▉      | 396/1000 [44:12<1:00:09,  5.98s/it] 40%|███▉      | 397/1000 [44:16<53:49,  5.36s/it]   40%|███▉      | 398/1000 [44:26<1:08:04,  6.78s/it] 40%|███▉      | 399/1000 [44:30<57:55,  5.78s/it]   40%|████      | 400/1000 [44:37<1:01:49,  6.18s/it] 40%|████      | 401/1000 [44:47<1:13:38,  7.38s/it] 40%|████      | 402/1000 [44:53<1:08:53,  6.91s/it] 40%|████      | 403/1000 [44:59<1:04:47,  6.51s/it] 40%|████      | 404/1000 [45:03<1:00:04,  6.05s/it] 40%|████      | 405/1000 [45:10<1:00:51,  6.14s/it] 41%|████      | 406/1000 [45:13<52:04,  5.26s/it]   41%|████      | 407/1000 [45:18<52:09,  5.28s/it] 41%|████      | 408/1000 [45:24<52:55,  5.36s/it] 41%|████      | 409/1000 [45:29<51:17,  5.21s/it] 41%|████      | 410/1000 [45:35<53:55,  5.48s/it] 41%|████      | 411/1000 [45:40<53:37,  5.46s/it] 41%|████      | 412/1000 [45:45<49:59,  5.10s/it] 41%|████▏     | 413/1000 [45:51<54:43,  5.59s/it] 41%|████▏     | 414/1000 [45:59<1:00:48,  6.23s/it] 42%|████▏     | 415/1000 [46:04<56:43,  5.82s/it]   42%|████▏     | 416/1000 [46:11<59:13,  6.08s/it] 42%|████▏     | 417/1000 [46:17<1:00:57,  6.27s/it] 42%|████▏     | 418/1000 [46:26<1:07:44,  6.98s/it] 42%|████▏     | 419/1000 [46:30<1:00:28,  6.24s/it] 42%|████▏     | 420/1000 [46:35<56:34,  5.85s/it]   42%|████▏     | 421/1000 [46:41<57:04,  5.91s/it] 42%|████▏     | 422/1000 [46:46<51:59,  5.40s/it] 42%|████▏     | 423/1000 [46:53<56:12,  5.85s/it] 42%|████▏     | 424/1000 [46:58<54:55,  5.72s/it] 42%|████▎     | 425/1000 [47:09<1:08:48,  7.18s/it] 43%|████▎     | 426/1000 [47:14<1:04:44,  6.77s/it] 43%|████▎     | 427/1000 [47:19<58:58,  6.18s/it]   43%|████▎     | 428/1000 [47:25<58:06,  6.10s/it] 43%|████▎     | 429/1000 [47:27<46:06,  4.84s/it] 43%|████▎     | 430/1000 [47:30<39:24,  4.15s/it] 43%|████▎     | 431/1000 [47:35<44:15,  4.67s/it] 43%|████▎     | 432/1000 [47:45<57:14,  6.05s/it] 43%|████▎     | 433/1000 [47:49<53:27,  5.66s/it] 43%|████▎     | 434/1000 [47:55<52:43,  5.59s/it] 44%|████▎     | 435/1000 [48:04<1:01:40,  6.55s/it] 44%|████▎     | 436/1000 [48:09<58:16,  6.20s/it]   44%|████▎     | 437/1000 [48:14<53:25,  5.69s/it] 44%|████▍     | 438/1000 [48:19<51:38,  5.51s/it] 44%|████▍     | 439/1000 [48:22<46:41,  4.99s/it] 44%|████▍     | 440/1000 [48:27<44:11,  4.73s/it] 44%|████▍     | 441/1000 [48:33<49:46,  5.34s/it] 44%|████▍     | 442/1000 [48:47<1:12:00,  7.74s/it] 44%|████▍     | 443/1000 [48:52<1:05:28,  7.05s/it] 44%|████▍     | 444/1000 [48:58<1:01:40,  6.66s/it] 44%|████▍     | 445/1000 [49:03<58:24,  6.31s/it]   45%|████▍     | 446/1000 [49:09<57:08,  6.19s/it] 45%|████▍     | 447/1000 [49:13<49:20,  5.35s/it] 45%|████▍     | 448/1000 [49:18<48:49,  5.31s/it] 45%|████▍     | 449/1000 [49:23<47:24,  5.16s/it] 45%|████▌     | 450/1000 [49:33<1:01:54,  6.75s/it] 45%|████▌     | 451/1000 [49:40<1:01:34,  6.73s/it] 45%|████▌     | 452/1000 [49:43<52:24,  5.74s/it]   45%|████▌     | 453/1000 [49:46<45:01,  4.94s/it] 45%|████▌     | 454/1000 [49:51<44:00,  4.84s/it] 46%|████▌     | 455/1000 [50:02<1:01:12,  6.74s/it] 46%|████▌     | 456/1000 [50:10<1:04:06,  7.07s/it] 46%|████▌     | 457/1000 [50:15<59:49,  6.61s/it]   46%|████▌     | 458/1000 [50:19<51:28,  5.70s/it] 46%|████▌     | 459/1000 [50:32<1:11:53,  7.97s/it] 46%|████▌     | 460/1000 [50:35<57:51,  6.43s/it]   46%|████▌     | 461/1000 [50:39<51:53,  5.78s/it] 46%|████▌     | 462/1000 [50:44<48:50,  5.45s/it] 46%|████▋     | 463/1000 [50:50<49:17,  5.51s/it] 46%|████▋     | 464/1000 [50:56<50:44,  5.68s/it] 46%|████▋     | 465/1000 [51:02<51:00,  5.72s/it] 47%|████▋     | 466/1000 [51:08<51:40,  5.81s/it] 47%|████▋     | 467/1000 [51:15<56:48,  6.39s/it] 47%|████▋     | 468/1000 [51:24<1:02:52,  7.09s/it] 47%|████▋     | 469/1000 [51:28<55:21,  6.26s/it]   47%|████▋     | 470/1000 [51:35<55:51,  6.32s/it] 47%|████▋     | 471/1000 [51:43<1:00:22,  6.85s/it] 47%|████▋     | 472/1000 [51:48<54:51,  6.23s/it]   47%|████▋     | 473/1000 [51:58<1:05:14,  7.43s/it] 47%|████▋     | 474/1000 [52:03<59:55,  6.84s/it]   48%|████▊     | 475/1000 [52:08<53:21,  6.10s/it] 48%|████▊     | 476/1000 [52:14<54:29,  6.24s/it] 48%|████▊     | 477/1000 [52:21<55:55,  6.42s/it] 48%|████▊     | 478/1000 [52:30<1:02:01,  7.13s/it] 48%|████▊     | 479/1000 [52:38<1:05:05,  7.50s/it] 48%|████▊     | 480/1000 [52:43<56:27,  6.51s/it]   48%|████▊     | 481/1000 [52:47<50:37,  5.85s/it] 48%|████▊     | 482/1000 [52:50<44:07,  5.11s/it] 48%|████▊     | 483/1000 [52:57<47:36,  5.53s/it] 48%|████▊     | 484/1000 [53:09<1:04:42,  7.52s/it] 48%|████▊     | 485/1000 [53:17<1:05:26,  7.63s/it] 49%|████▊     | 486/1000 [53:22<1:00:07,  7.02s/it] 49%|████▊     | 487/1000 [53:27<53:15,  6.23s/it]   49%|████▉     | 488/1000 [53:30<45:18,  5.31s/it] 49%|████▉     | 489/1000 [53:40<57:15,  6.72s/it] 49%|████▉     | 490/1000 [53:51<1:08:55,  8.11s/it] 49%|████▉     | 491/1000 [53:59<1:07:21,  7.94s/it] 49%|████▉     | 492/1000 [54:03<57:36,  6.80s/it]   49%|████▉     | 493/1000 [54:07<51:02,  6.04s/it] 49%|████▉     | 494/1000 [54:15<54:46,  6.49s/it] 50%|████▉     | 495/1000 [54:20<51:47,  6.15s/it] 50%|████▉     | 496/1000 [54:27<53:43,  6.40s/it] 50%|████▉     | 497/1000 [54:34<53:56,  6.43s/it] 50%|████▉     | 498/1000 [54:38<49:19,  5.89s/it] 50%|████▉     | 499/1000 [54:43<46:26,  5.56s/it] 50%|█████     | 500/1000 [54:51<52:33,  6.31s/it] 50%|█████     | 501/1000 [54:54<44:15,  5.32s/it] 50%|█████     | 502/1000 [54:59<41:46,  5.03s/it] 50%|█████     | 503/1000 [55:06<47:21,  5.72s/it] 50%|█████     | 504/1000 [55:09<41:16,  4.99s/it] 50%|█████     | 505/1000 [55:11<33:27,  4.06s/it] 51%|█████     | 506/1000 [55:14<30:18,  3.68s/it] 51%|█████     | 507/1000 [55:18<31:30,  3.83s/it] 51%|█████     | 508/1000 [55:23<33:45,  4.12s/it] 51%|█████     | 509/1000 [55:27<34:58,  4.27s/it] 51%|█████     | 510/1000 [55:33<38:54,  4.76s/it] 51%|█████     | 511/1000 [55:37<37:19,  4.58s/it] 51%|█████     | 512/1000 [55:42<37:12,  4.58s/it] 51%|█████▏    | 513/1000 [55:47<37:58,  4.68s/it] 51%|█████▏    | 514/1000 [55:55<47:06,  5.82s/it] 52%|█████▏    | 515/1000 [55:58<39:19,  4.87s/it] 52%|█████▏    | 516/1000 [56:08<52:10,  6.47s/it] 52%|█████▏    | 517/1000 [56:15<53:22,  6.63s/it] 52%|█████▏    | 518/1000 [56:25<1:01:35,  7.67s/it] 52%|█████▏    | 519/1000 [56:34<1:04:52,  8.09s/it] 52%|█████▏    | 520/1000 [56:38<54:59,  6.87s/it]   52%|█████▏    | 521/1000 [56:47<57:55,  7.26s/it] 52%|█████▏    | 522/1000 [56:51<52:00,  6.53s/it] 52%|█████▏    | 523/1000 [56:59<54:34,  6.86s/it] 52%|█████▏    | 524/1000 [57:05<51:14,  6.46s/it] 52%|█████▎    | 525/1000 [57:11<51:02,  6.45s/it] 53%|█████▎    | 526/1000 [57:17<50:39,  6.41s/it] 53%|█████▎    | 527/1000 [57:24<50:42,  6.43s/it] 53%|█████▎    | 528/1000 [57:34<1:00:09,  7.65s/it] 53%|█████▎    | 529/1000 [57:38<51:08,  6.52s/it]   53%|█████▎    | 530/1000 [57:44<50:21,  6.43s/it] 53%|█████▎    | 531/1000 [57:47<41:40,  5.33s/it] 53%|█████▎    | 532/1000 [57:54<44:11,  5.67s/it] 53%|█████▎    | 533/1000 [57:57<38:01,  4.89s/it] 53%|█████▎    | 534/1000 [58:02<39:06,  5.04s/it] 54%|█████▎    | 535/1000 [58:06<35:37,  4.60s/it] 54%|█████▎    | 536/1000 [58:11<37:01,  4.79s/it] 54%|█████▎    | 537/1000 [58:15<36:14,  4.70s/it] 54%|█████▍    | 538/1000 [58:19<33:08,  4.30s/it] 54%|█████▍    | 539/1000 [58:23<33:32,  4.37s/it] 54%|█████▍    | 540/1000 [58:29<36:00,  4.70s/it] 54%|█████▍    | 541/1000 [58:35<39:49,  5.21s/it] 54%|█████▍    | 542/1000 [58:43<46:01,  6.03s/it] 54%|█████▍    | 543/1000 [58:53<54:31,  7.16s/it] 54%|█████▍    | 544/1000 [59:02<57:46,  7.60s/it] 55%|█████▍    | 545/1000 [59:07<53:54,  7.11s/it] 55%|█████▍    | 546/1000 [59:17<59:20,  7.84s/it] 55%|█████▍    | 547/1000 [59:24<57:17,  7.59s/it] 55%|█████▍    | 548/1000 [59:27<46:40,  6.19s/it] 55%|█████▍    | 549/1000 [59:34<48:29,  6.45s/it] 55%|█████▌    | 550/1000 [59:42<52:49,  7.04s/it] 55%|█████▌    | 551/1000 [59:53<1:00:36,  8.10s/it] 55%|█████▌    | 552/1000 [1:00:02<1:02:20,  8.35s/it] 55%|█████▌    | 553/1000 [1:00:07<53:57,  7.24s/it]   55%|█████▌    | 554/1000 [1:00:13<51:14,  6.89s/it] 56%|█████▌    | 555/1000 [1:00:25<1:03:56,  8.62s/it] 56%|█████▌    | 556/1000 [1:00:30<53:55,  7.29s/it]   56%|█████▌    | 557/1000 [1:00:37<55:14,  7.48s/it] 56%|█████▌    | 558/1000 [1:00:43<50:01,  6.79s/it] 56%|█████▌    | 559/1000 [1:00:51<54:17,  7.39s/it] 56%|█████▌    | 560/1000 [1:00:56<48:54,  6.67s/it] 56%|█████▌    | 561/1000 [1:01:03<49:20,  6.74s/it] 56%|█████▌    | 562/1000 [1:01:07<43:11,  5.92s/it] 56%|█████▋    | 563/1000 [1:01:14<45:10,  6.20s/it] 56%|█████▋    | 564/1000 [1:01:18<39:00,  5.37s/it] 56%|█████▋    | 565/1000 [1:01:22<37:02,  5.11s/it] 57%|█████▋    | 566/1000 [1:01:26<34:25,  4.76s/it] 57%|█████▋    | 567/1000 [1:01:32<37:55,  5.25s/it] 57%|█████▋    | 568/1000 [1:01:39<41:23,  5.75s/it] 57%|█████▋    | 569/1000 [1:01:46<42:23,  5.90s/it] 57%|█████▋    | 570/1000 [1:01:56<52:35,  7.34s/it] 57%|█████▋    | 571/1000 [1:02:04<53:20,  7.46s/it] 57%|█████▋    | 572/1000 [1:02:10<50:11,  7.04s/it] 57%|█████▋    | 573/1000 [1:02:15<45:29,  6.39s/it] 57%|█████▋    | 574/1000 [1:02:18<37:35,  5.29s/it] 57%|█████▊    | 575/1000 [1:02:22<35:49,  5.06s/it] 58%|█████▊    | 576/1000 [1:02:36<53:14,  7.53s/it] 58%|█████▊    | 577/1000 [1:02:40<47:34,  6.75s/it] 58%|█████▊    | 578/1000 [1:02:45<43:33,  6.19s/it] 58%|█████▊    | 579/1000 [1:02:50<40:20,  5.75s/it] 58%|█████▊    | 580/1000 [1:02:55<38:32,  5.50s/it] 58%|█████▊    | 581/1000 [1:03:00<37:10,  5.32s/it] 58%|█████▊    | 582/1000 [1:03:06<39:40,  5.70s/it] 58%|█████▊    | 583/1000 [1:03:11<38:06,  5.48s/it] 58%|█████▊    | 584/1000 [1:03:18<39:26,  5.69s/it] 58%|█████▊    | 585/1000 [1:03:27<46:50,  6.77s/it] 59%|█████▊    | 586/1000 [1:03:34<47:18,  6.86s/it] 59%|█████▊    | 587/1000 [1:03:38<40:58,  5.95s/it] 59%|█████▉    | 588/1000 [1:03:50<53:44,  7.83s/it] 59%|█████▉    | 589/1000 [1:03:54<46:12,  6.75s/it] 59%|█████▉    | 590/1000 [1:04:01<46:04,  6.74s/it] 59%|█████▉    | 591/1000 [1:04:08<47:30,  6.97s/it] 59%|█████▉    | 592/1000 [1:04:22<1:00:38,  8.92s/it] 59%|█████▉    | 593/1000 [1:04:26<50:28,  7.44s/it]   59%|█████▉    | 594/1000 [1:04:29<42:01,  6.21s/it] 60%|█████▉    | 595/1000 [1:04:36<42:05,  6.24s/it] 60%|█████▉    | 596/1000 [1:04:40<39:08,  5.81s/it] 60%|█████▉    | 597/1000 [1:04:45<36:30,  5.44s/it] 60%|█████▉    | 598/1000 [1:04:50<35:09,  5.25s/it] 60%|█████▉    | 599/1000 [1:04:56<36:20,  5.44s/it] 60%|██████    | 600/1000 [1:05:05<44:07,  6.62s/it] 60%|██████    | 601/1000 [1:05:10<39:59,  6.01s/it] 60%|██████    | 602/1000 [1:05:14<36:44,  5.54s/it] 60%|██████    | 603/1000 [1:05:21<39:30,  5.97s/it] 60%|██████    | 604/1000 [1:05:26<38:15,  5.80s/it] 60%|██████    | 605/1000 [1:05:29<31:55,  4.85s/it] 61%|██████    | 606/1000 [1:05:34<31:52,  4.85s/it] 61%|██████    | 607/1000 [1:05:43<39:07,  5.97s/it] 61%|██████    | 608/1000 [1:05:47<36:15,  5.55s/it] 61%|██████    | 609/1000 [1:05:54<39:22,  6.04s/it] 61%|██████    | 610/1000 [1:05:59<36:40,  5.64s/it] 61%|██████    | 611/1000 [1:06:04<35:55,  5.54s/it] 61%|██████    | 612/1000 [1:06:15<46:23,  7.17s/it] 61%|██████▏   | 613/1000 [1:06:21<44:14,  6.86s/it] 61%|██████▏   | 614/1000 [1:06:26<40:37,  6.32s/it] 62%|██████▏   | 615/1000 [1:06:41<56:17,  8.77s/it] 62%|██████▏   | 616/1000 [1:06:43<42:55,  6.71s/it] 62%|██████▏   | 617/1000 [1:06:48<40:27,  6.34s/it] 62%|██████▏   | 618/1000 [1:06:53<36:29,  5.73s/it] 62%|██████▏   | 619/1000 [1:06:59<36:51,  5.80s/it] 62%|██████▏   | 620/1000 [1:07:05<37:49,  5.97s/it] 62%|██████▏   | 621/1000 [1:07:10<35:53,  5.68s/it] 62%|██████▏   | 622/1000 [1:07:18<39:49,  6.32s/it] 62%|██████▏   | 623/1000 [1:07:24<39:55,  6.35s/it] 62%|██████▏   | 624/1000 [1:07:29<36:49,  5.88s/it] 62%|██████▎   | 625/1000 [1:07:33<32:58,  5.28s/it] 63%|██████▎   | 626/1000 [1:07:38<32:10,  5.16s/it] 63%|██████▎   | 627/1000 [1:07:45<35:27,  5.70s/it] 63%|██████▎   | 628/1000 [1:07:49<33:02,  5.33s/it] 63%|██████▎   | 629/1000 [1:07:54<31:25,  5.08s/it] 63%|██████▎   | 630/1000 [1:07:59<32:29,  5.27s/it] 63%|██████▎   | 631/1000 [1:08:12<45:59,  7.48s/it] 63%|██████▎   | 632/1000 [1:08:17<42:00,  6.85s/it] 63%|██████▎   | 633/1000 [1:08:22<38:40,  6.32s/it] 63%|██████▎   | 634/1000 [1:08:27<34:22,  5.63s/it] 64%|██████▎   | 635/1000 [1:08:30<31:14,  5.14s/it] 64%|██████▎   | 636/1000 [1:08:36<31:40,  5.22s/it] 64%|██████▎   | 637/1000 [1:08:50<46:57,  7.76s/it] 64%|██████▍   | 638/1000 [1:08:54<41:21,  6.85s/it] 64%|██████▍   | 639/1000 [1:08:57<33:34,  5.58s/it] 64%|██████▍   | 640/1000 [1:09:02<31:51,  5.31s/it] 64%|██████▍   | 641/1000 [1:09:07<32:08,  5.37s/it] 64%|██████▍   | 642/1000 [1:09:14<34:24,  5.77s/it] 64%|██████▍   | 643/1000 [1:09:18<30:40,  5.15s/it] 64%|██████▍   | 644/1000 [1:09:24<33:09,  5.59s/it] 64%|██████▍   | 645/1000 [1:09:29<31:52,  5.39s/it] 65%|██████▍   | 646/1000 [1:09:37<35:28,  6.01s/it] 65%|██████▍   | 647/1000 [1:09:47<42:47,  7.27s/it] 65%|██████▍   | 648/1000 [1:09:54<43:10,  7.36s/it] 65%|██████▍   | 649/1000 [1:10:04<47:29,  8.12s/it] 65%|██████▌   | 650/1000 [1:10:09<41:46,  7.16s/it] 65%|██████▌   | 651/1000 [1:10:13<36:32,  6.28s/it] 65%|██████▌   | 652/1000 [1:10:18<32:50,  5.66s/it] 65%|██████▌   | 653/1000 [1:10:23<31:48,  5.50s/it] 65%|██████▌   | 654/1000 [1:10:33<39:42,  6.89s/it] 66%|██████▌   | 655/1000 [1:10:40<40:22,  7.02s/it] 66%|██████▌   | 656/1000 [1:10:53<51:01,  8.90s/it] 66%|██████▌   | 657/1000 [1:10:57<42:12,  7.38s/it] 66%|██████▌   | 658/1000 [1:11:05<42:02,  7.37s/it] 66%|██████▌   | 659/1000 [1:11:09<37:26,  6.59s/it] 66%|██████▌   | 660/1000 [1:11:14<34:24,  6.07s/it] 66%|██████▌   | 661/1000 [1:11:20<33:48,  5.98s/it] 66%|██████▌   | 662/1000 [1:11:24<30:37,  5.44s/it] 66%|██████▋   | 663/1000 [1:11:32<33:41,  6.00s/it] 66%|██████▋   | 664/1000 [1:11:35<30:11,  5.39s/it] 66%|██████▋   | 665/1000 [1:11:43<33:52,  6.07s/it] 67%|██████▋   | 666/1000 [1:11:47<29:45,  5.35s/it] 67%|██████▋   | 667/1000 [1:11:59<40:50,  7.36s/it] 67%|██████▋   | 668/1000 [1:12:01<32:44,  5.92s/it] 67%|██████▋   | 669/1000 [1:12:08<34:21,  6.23s/it] 67%|██████▋   | 670/1000 [1:12:13<31:45,  5.77s/it] 67%|██████▋   | 671/1000 [1:12:18<30:19,  5.53s/it] 67%|██████▋   | 672/1000 [1:12:27<35:30,  6.49s/it] 67%|██████▋   | 673/1000 [1:12:34<36:02,  6.61s/it] 67%|██████▋   | 674/1000 [1:12:42<39:18,  7.24s/it] 68%|██████▊   | 675/1000 [1:12:48<36:31,  6.74s/it] 68%|██████▊   | 676/1000 [1:12:53<32:59,  6.11s/it] 68%|██████▊   | 677/1000 [1:12:58<32:13,  5.99s/it] 68%|██████▊   | 678/1000 [1:13:07<36:48,  6.86s/it] 68%|██████▊   | 679/1000 [1:13:14<36:34,  6.84s/it] 68%|██████▊   | 680/1000 [1:13:24<41:18,  7.75s/it] 68%|██████▊   | 681/1000 [1:13:33<43:21,  8.16s/it] 68%|██████▊   | 682/1000 [1:13:40<42:06,  7.94s/it] 68%|██████▊   | 683/1000 [1:13:47<40:34,  7.68s/it] 68%|██████▊   | 684/1000 [1:13:55<40:58,  7.78s/it] 68%|██████▊   | 685/1000 [1:14:01<37:32,  7.15s/it] 69%|██████▊   | 686/1000 [1:14:05<32:40,  6.24s/it] 69%|██████▊   | 687/1000 [1:14:12<33:27,  6.41s/it] 69%|██████▉   | 688/1000 [1:14:26<44:22,  8.53s/it] 69%|██████▉   | 689/1000 [1:14:29<37:04,  7.15s/it] 69%|██████▉   | 690/1000 [1:14:34<33:25,  6.47s/it] 69%|██████▉   | 691/1000 [1:14:40<32:29,  6.31s/it] 69%|██████▉   | 692/1000 [1:14:49<35:30,  6.92s/it] 69%|██████▉   | 693/1000 [1:14:54<32:25,  6.34s/it] 69%|██████▉   | 694/1000 [1:15:00<32:29,  6.37s/it] 70%|██████▉   | 695/1000 [1:15:10<37:49,  7.44s/it] 70%|██████▉   | 696/1000 [1:15:15<34:13,  6.75s/it] 70%|██████▉   | 697/1000 [1:15:18<28:14,  5.59s/it] 70%|██████▉   | 698/1000 [1:15:22<26:19,  5.23s/it] 70%|██████▉   | 699/1000 [1:15:29<27:52,  5.56s/it] 70%|███████   | 700/1000 [1:15:39<34:14,  6.85s/it] 70%|███████   | 701/1000 [1:15:43<30:35,  6.14s/it] 70%|███████   | 702/1000 [1:15:48<28:03,  5.65s/it] 70%|███████   | 703/1000 [1:15:53<27:42,  5.60s/it] 70%|███████   | 704/1000 [1:15:57<25:28,  5.16s/it] 70%|███████   | 705/1000 [1:16:05<28:45,  5.85s/it] 71%|███████   | 706/1000 [1:16:11<28:41,  5.86s/it] 71%|███████   | 707/1000 [1:16:15<26:34,  5.44s/it] 71%|███████   | 708/1000 [1:16:19<24:22,  5.01s/it] 71%|███████   | 709/1000 [1:16:26<27:31,  5.67s/it] 71%|███████   | 710/1000 [1:16:32<27:09,  5.62s/it] 71%|███████   | 711/1000 [1:16:38<27:57,  5.80s/it] 71%|███████   | 712/1000 [1:16:48<33:31,  6.98s/it] 71%|███████▏  | 713/1000 [1:16:53<30:34,  6.39s/it] 71%|███████▏  | 714/1000 [1:17:02<34:50,  7.31s/it] 72%|███████▏  | 715/1000 [1:17:11<36:23,  7.66s/it] 72%|███████▏  | 716/1000 [1:17:18<35:57,  7.60s/it] 72%|███████▏  | 717/1000 [1:17:24<32:47,  6.95s/it] 72%|███████▏  | 718/1000 [1:17:33<36:34,  7.78s/it] 72%|███████▏  | 719/1000 [1:17:42<37:23,  7.98s/it] 72%|███████▏  | 720/1000 [1:17:47<33:29,  7.18s/it] 72%|███████▏  | 721/1000 [1:17:51<29:22,  6.32s/it] 72%|███████▏  | 722/1000 [1:17:57<28:46,  6.21s/it] 72%|███████▏  | 723/1000 [1:18:02<26:25,  5.72s/it] 72%|███████▏  | 724/1000 [1:18:08<26:39,  5.80s/it] 72%|███████▎  | 725/1000 [1:18:13<25:59,  5.67s/it] 73%|███████▎  | 726/1000 [1:18:18<24:08,  5.29s/it] 73%|███████▎  | 727/1000 [1:18:23<24:05,  5.30s/it] 73%|███████▎  | 728/1000 [1:18:29<24:28,  5.40s/it] 73%|███████▎  | 729/1000 [1:18:36<27:47,  6.15s/it] 73%|███████▎  | 730/1000 [1:18:47<33:09,  7.37s/it] 73%|███████▎  | 731/1000 [1:18:51<28:28,  6.35s/it] 73%|███████▎  | 732/1000 [1:18:57<28:39,  6.42s/it] 73%|███████▎  | 733/1000 [1:19:04<28:59,  6.51s/it] 73%|███████▎  | 734/1000 [1:19:11<29:44,  6.71s/it] 74%|███████▎  | 735/1000 [1:19:17<28:15,  6.40s/it] 74%|███████▎  | 736/1000 [1:19:25<30:40,  6.97s/it] 74%|███████▎  | 737/1000 [1:19:32<29:54,  6.82s/it] 74%|███████▍  | 738/1000 [1:19:37<27:24,  6.28s/it] 74%|███████▍  | 739/1000 [1:19:47<32:04,  7.37s/it] 74%|███████▍  | 740/1000 [1:19:54<32:31,  7.51s/it] 74%|███████▍  | 741/1000 [1:20:01<31:29,  7.29s/it] 74%|███████▍  | 742/1000 [1:20:06<28:21,  6.60s/it] 74%|███████▍  | 743/1000 [1:20:12<26:51,  6.27s/it] 74%|███████▍  | 744/1000 [1:20:18<27:03,  6.34s/it] 74%|███████▍  | 745/1000 [1:20:26<28:55,  6.80s/it] 75%|███████▍  | 746/1000 [1:20:39<37:02,  8.75s/it] 75%|███████▍  | 747/1000 [1:20:48<36:36,  8.68s/it] 75%|███████▍  | 748/1000 [1:20:56<35:58,  8.57s/it] 75%|███████▍  | 749/1000 [1:21:03<33:50,  8.09s/it] 75%|███████▌  | 750/1000 [1:21:08<29:24,  7.06s/it] 75%|███████▌  | 751/1000 [1:21:14<28:24,  6.85s/it] 75%|███████▌  | 752/1000 [1:21:20<26:40,  6.45s/it] 75%|███████▌  | 753/1000 [1:21:28<28:26,  6.91s/it] 75%|███████▌  | 754/1000 [1:21:33<26:50,  6.55s/it] 76%|███████▌  | 755/1000 [1:21:38<24:57,  6.11s/it] 76%|███████▌  | 756/1000 [1:21:43<23:17,  5.73s/it] 76%|███████▌  | 757/1000 [1:21:48<21:33,  5.32s/it] 76%|███████▌  | 758/1000 [1:21:53<21:58,  5.45s/it] 76%|███████▌  | 759/1000 [1:21:59<21:43,  5.41s/it] 76%|███████▌  | 760/1000 [1:22:04<21:06,  5.28s/it] 76%|███████▌  | 761/1000 [1:22:09<21:07,  5.30s/it] 76%|███████▌  | 762/1000 [1:22:17<23:52,  6.02s/it] 76%|███████▋  | 763/1000 [1:22:30<32:18,  8.18s/it] 76%|███████▋  | 764/1000 [1:22:36<29:52,  7.60s/it] 76%|███████▋  | 765/1000 [1:22:44<30:16,  7.73s/it] 77%|███████▋  | 766/1000 [1:22:51<29:15,  7.50s/it] 77%|███████▋  | 767/1000 [1:22:55<24:19,  6.26s/it] 77%|███████▋  | 768/1000 [1:23:03<26:20,  6.81s/it] 77%|███████▋  | 769/1000 [1:23:07<23:19,  6.06s/it] 77%|███████▋  | 770/1000 [1:23:17<28:13,  7.36s/it] 77%|███████▋  | 771/1000 [1:23:22<24:50,  6.51s/it] 77%|███████▋  | 772/1000 [1:23:25<20:31,  5.40s/it] 77%|███████▋  | 773/1000 [1:23:29<19:25,  5.14s/it] 77%|███████▋  | 774/1000 [1:23:36<20:56,  5.56s/it] 78%|███████▊  | 775/1000 [1:23:40<19:25,  5.18s/it] 78%|███████▊  | 776/1000 [1:23:45<19:26,  5.21s/it] 78%|███████▊  | 777/1000 [1:23:54<22:51,  6.15s/it] 78%|███████▊  | 778/1000 [1:23:59<21:37,  5.84s/it] 78%|███████▊  | 779/1000 [1:24:04<21:17,  5.78s/it] 78%|███████▊  | 780/1000 [1:24:10<20:59,  5.72s/it] 78%|███████▊  | 781/1000 [1:24:15<19:52,  5.44s/it] 78%|███████▊  | 782/1000 [1:24:19<18:09,  5.00s/it] 78%|███████▊  | 783/1000 [1:24:26<20:47,  5.75s/it] 78%|███████▊  | 784/1000 [1:24:37<25:36,  7.11s/it] 78%|███████▊  | 785/1000 [1:24:41<22:11,  6.19s/it] 79%|███████▊  | 786/1000 [1:24:44<19:25,  5.45s/it] 79%|███████▊  | 787/1000 [1:24:49<18:16,  5.15s/it] 79%|███████▉  | 788/1000 [1:24:54<18:28,  5.23s/it] 79%|███████▉  | 789/1000 [1:24:59<18:28,  5.25s/it] 79%|███████▉  | 790/1000 [1:25:06<19:37,  5.61s/it] 79%|███████▉  | 791/1000 [1:25:16<24:03,  6.91s/it] 79%|███████▉  | 792/1000 [1:25:20<20:36,  5.95s/it] 79%|███████▉  | 793/1000 [1:25:26<21:29,  6.23s/it] 79%|███████▉  | 794/1000 [1:25:31<19:39,  5.73s/it] 80%|███████▉  | 795/1000 [1:25:35<17:45,  5.20s/it] 80%|███████▉  | 796/1000 [1:25:43<20:26,  6.01s/it] 80%|███████▉  | 797/1000 [1:25:52<23:14,  6.87s/it] 80%|███████▉  | 798/1000 [1:26:00<24:54,  7.40s/it] 80%|███████▉  | 799/1000 [1:26:07<23:50,  7.12s/it] 80%|████████  | 800/1000 [1:26:18<28:12,  8.46s/it] 80%|████████  | 801/1000 [1:26:24<25:32,  7.70s/it] 80%|████████  | 802/1000 [1:26:31<24:01,  7.28s/it] 80%|████████  | 803/1000 [1:26:35<21:23,  6.52s/it] 80%|████████  | 804/1000 [1:26:44<23:06,  7.08s/it] 80%|████████  | 805/1000 [1:26:49<21:11,  6.52s/it] 81%|████████  | 806/1000 [1:26:54<19:44,  6.10s/it] 81%|████████  | 807/1000 [1:26:58<17:22,  5.40s/it] 81%|████████  | 808/1000 [1:27:03<16:47,  5.25s/it] 81%|████████  | 809/1000 [1:27:11<19:09,  6.02s/it] 81%|████████  | 810/1000 [1:27:21<22:58,  7.26s/it] 81%|████████  | 811/1000 [1:27:30<24:22,  7.74s/it] 81%|████████  | 812/1000 [1:27:34<21:12,  6.77s/it] 81%|████████▏ | 813/1000 [1:27:41<21:10,  6.79s/it] 81%|████████▏ | 814/1000 [1:27:50<22:45,  7.34s/it] 82%|████████▏ | 815/1000 [1:27:57<22:15,  7.22s/it] 82%|████████▏ | 816/1000 [1:28:13<30:19,  9.89s/it] 82%|████████▏ | 817/1000 [1:28:14<22:39,  7.43s/it] 82%|████████▏ | 818/1000 [1:28:20<21:21,  7.04s/it] 82%|████████▏ | 819/1000 [1:28:27<20:47,  6.89s/it] 82%|████████▏ | 820/1000 [1:28:32<19:15,  6.42s/it] 82%|████████▏ | 821/1000 [1:28:37<17:51,  5.99s/it] 82%|████████▏ | 822/1000 [1:28:45<19:17,  6.50s/it] 82%|████████▏ | 823/1000 [1:28:55<22:05,  7.49s/it] 82%|████████▏ | 824/1000 [1:29:02<21:57,  7.49s/it] 82%|████████▎ | 825/1000 [1:29:08<20:12,  6.93s/it] 83%|████████▎ | 826/1000 [1:29:12<17:31,  6.05s/it] 83%|████████▎ | 827/1000 [1:29:17<16:32,  5.73s/it] 83%|████████▎ | 828/1000 [1:29:24<17:17,  6.03s/it] 83%|████████▎ | 829/1000 [1:29:30<17:36,  6.18s/it] 83%|████████▎ | 830/1000 [1:29:36<17:09,  6.06s/it] 83%|████████▎ | 831/1000 [1:29:39<14:44,  5.23s/it] 83%|████████▎ | 832/1000 [1:29:42<12:39,  4.52s/it] 83%|████████▎ | 833/1000 [1:29:47<13:07,  4.71s/it] 83%|████████▎ | 834/1000 [1:29:50<11:29,  4.15s/it] 84%|████████▎ | 835/1000 [1:29:53<10:39,  3.88s/it] 84%|████████▎ | 836/1000 [1:29:59<12:00,  4.39s/it] 84%|████████▎ | 837/1000 [1:30:03<11:45,  4.33s/it] 84%|████████▍ | 838/1000 [1:30:09<13:21,  4.95s/it] 84%|████████▍ | 839/1000 [1:30:17<15:02,  5.60s/it] 84%|████████▍ | 840/1000 [1:30:23<15:50,  5.94s/it] 84%|████████▍ | 841/1000 [1:30:29<15:08,  5.72s/it] 84%|████████▍ | 842/1000 [1:30:35<15:20,  5.82s/it] 84%|████████▍ | 843/1000 [1:30:40<14:41,  5.62s/it] 84%|████████▍ | 844/1000 [1:30:42<12:14,  4.71s/it] 84%|████████▍ | 845/1000 [1:30:47<12:18,  4.77s/it] 85%|████████▍ | 846/1000 [1:30:57<15:54,  6.20s/it] 85%|████████▍ | 847/1000 [1:31:05<17:10,  6.74s/it] 85%|████████▍ | 848/1000 [1:31:10<16:04,  6.34s/it] 85%|████████▍ | 849/1000 [1:31:14<14:25,  5.73s/it] 85%|████████▌ | 850/1000 [1:31:19<13:27,  5.38s/it] 85%|████████▌ | 851/1000 [1:31:24<13:18,  5.36s/it] 85%|████████▌ | 852/1000 [1:31:34<16:23,  6.64s/it] 85%|████████▌ | 853/1000 [1:31:42<17:24,  7.11s/it] 85%|████████▌ | 854/1000 [1:31:48<16:12,  6.66s/it] 86%|████████▌ | 855/1000 [1:31:55<16:31,  6.84s/it] 86%|████████▌ | 856/1000 [1:32:02<16:36,  6.92s/it] 86%|████████▌ | 857/1000 [1:32:11<17:40,  7.41s/it] 86%|████████▌ | 858/1000 [1:32:15<14:57,  6.32s/it] 86%|████████▌ | 859/1000 [1:32:20<14:13,  6.05s/it] 86%|████████▌ | 860/1000 [1:32:26<14:28,  6.20s/it] 86%|████████▌ | 861/1000 [1:32:32<13:40,  5.90s/it] 86%|████████▌ | 862/1000 [1:32:39<14:48,  6.44s/it] 86%|████████▋ | 863/1000 [1:32:46<15:07,  6.63s/it] 86%|████████▋ | 864/1000 [1:32:51<13:34,  5.99s/it] 86%|████████▋ | 865/1000 [1:33:00<15:47,  7.02s/it] 87%|████████▋ | 866/1000 [1:33:12<19:00,  8.51s/it] 87%|████████▋ | 867/1000 [1:33:16<15:18,  6.91s/it] 87%|████████▋ | 868/1000 [1:33:19<12:58,  5.90s/it] 87%|████████▋ | 869/1000 [1:33:30<16:12,  7.42s/it] 87%|████████▋ | 870/1000 [1:33:37<15:33,  7.18s/it] 87%|████████▋ | 871/1000 [1:33:43<15:07,  7.03s/it] 87%|████████▋ | 872/1000 [1:33:50<14:36,  6.85s/it] 87%|████████▋ | 873/1000 [1:33:54<12:52,  6.09s/it] 87%|████████▋ | 874/1000 [1:34:02<13:39,  6.50s/it] 88%|████████▊ | 875/1000 [1:34:07<13:05,  6.29s/it] 88%|████████▊ | 876/1000 [1:34:11<11:14,  5.44s/it] 88%|████████▊ | 877/1000 [1:34:15<10:40,  5.21s/it] 88%|████████▊ | 878/1000 [1:34:21<10:29,  5.16s/it] 88%|████████▊ | 879/1000 [1:34:25<09:44,  4.83s/it] 88%|████████▊ | 880/1000 [1:34:31<10:35,  5.30s/it] 88%|████████▊ | 881/1000 [1:34:38<11:34,  5.83s/it] 88%|████████▊ | 882/1000 [1:34:42<10:11,  5.18s/it] 88%|████████▊ | 883/1000 [1:34:48<10:43,  5.50s/it] 88%|████████▊ | 884/1000 [1:34:54<11:03,  5.72s/it] 88%|████████▊ | 885/1000 [1:35:00<11:18,  5.90s/it] 89%|████████▊ | 886/1000 [1:35:06<11:00,  5.79s/it] 89%|████████▊ | 887/1000 [1:35:11<10:26,  5.55s/it] 89%|████████▉ | 888/1000 [1:35:16<10:02,  5.38s/it] 89%|████████▉ | 889/1000 [1:35:20<09:16,  5.01s/it] 89%|████████▉ | 890/1000 [1:35:26<09:48,  5.35s/it] 89%|████████▉ | 891/1000 [1:35:31<09:11,  5.06s/it] 89%|████████▉ | 892/1000 [1:35:36<09:12,  5.12s/it] 89%|████████▉ | 893/1000 [1:35:40<08:41,  4.87s/it] 89%|████████▉ | 894/1000 [1:35:47<09:43,  5.50s/it] 90%|████████▉ | 895/1000 [1:35:56<11:37,  6.64s/it] 90%|████████▉ | 896/1000 [1:36:01<10:36,  6.12s/it] 90%|████████▉ | 897/1000 [1:36:06<09:47,  5.71s/it] 90%|████████▉ | 898/1000 [1:36:15<11:11,  6.59s/it] 90%|████████▉ | 899/1000 [1:36:24<12:10,  7.23s/it] 90%|█████████ | 900/1000 [1:36:28<10:36,  6.37s/it] 90%|█████████ | 901/1000 [1:36:39<13:06,  7.94s/it] 90%|█████████ | 902/1000 [1:36:43<10:38,  6.51s/it] 90%|█████████ | 903/1000 [1:36:49<10:17,  6.37s/it] 90%|█████████ | 904/1000 [1:36:52<08:44,  5.46s/it] 90%|█████████ | 905/1000 [1:36:58<08:38,  5.46s/it] 91%|█████████ | 906/1000 [1:37:01<07:44,  4.94s/it] 91%|█████████ | 907/1000 [1:37:07<07:48,  5.04s/it] 91%|█████████ | 908/1000 [1:37:12<08:03,  5.26s/it] 91%|█████████ | 909/1000 [1:37:17<07:49,  5.16s/it] 91%|█████████ | 910/1000 [1:37:28<10:10,  6.79s/it] 91%|█████████ | 911/1000 [1:37:33<09:20,  6.29s/it] 91%|█████████ | 912/1000 [1:37:38<08:49,  6.02s/it] 91%|█████████▏| 913/1000 [1:37:42<07:34,  5.23s/it] 91%|█████████▏| 914/1000 [1:37:48<08:05,  5.65s/it] 92%|█████████▏| 915/1000 [1:37:56<08:58,  6.33s/it] 92%|█████████▏| 916/1000 [1:38:02<08:32,  6.10s/it] 92%|█████████▏| 917/1000 [1:38:12<10:00,  7.23s/it] 92%|█████████▏| 918/1000 [1:38:18<09:21,  6.85s/it] 92%|█████████▏| 919/1000 [1:38:23<08:49,  6.54s/it] 92%|█████████▏| 920/1000 [1:38:32<09:32,  7.15s/it] 92%|█████████▏| 921/1000 [1:38:37<08:27,  6.43s/it] 92%|█████████▏| 922/1000 [1:38:42<07:45,  5.97s/it] 92%|█████████▏| 923/1000 [1:38:51<08:51,  6.90s/it] 92%|█████████▏| 924/1000 [1:38:55<07:52,  6.22s/it] 92%|█████████▎| 925/1000 [1:39:00<07:09,  5.73s/it] 93%|█████████▎| 926/1000 [1:39:06<07:18,  5.93s/it] 93%|█████████▎| 927/1000 [1:39:12<06:59,  5.74s/it] 93%|█████████▎| 928/1000 [1:39:17<06:37,  5.52s/it] 93%|█████████▎| 929/1000 [1:39:22<06:27,  5.46s/it] 93%|█████████▎| 930/1000 [1:39:30<07:06,  6.10s/it] 93%|█████████▎| 931/1000 [1:39:34<06:17,  5.48s/it] 93%|█████████▎| 932/1000 [1:39:39<06:01,  5.32s/it] 93%|█████████▎| 933/1000 [1:39:44<05:56,  5.32s/it] 93%|█████████▎| 934/1000 [1:39:48<05:32,  5.04s/it] 94%|█████████▎| 935/1000 [1:39:58<06:58,  6.43s/it] 94%|█████████▎| 936/1000 [1:40:08<08:08,  7.63s/it] 94%|█████████▎| 937/1000 [1:40:14<07:21,  7.01s/it] 94%|█████████▍| 938/1000 [1:40:19<06:32,  6.33s/it] 94%|█████████▍| 939/1000 [1:40:24<06:03,  5.95s/it] 94%|█████████▍| 940/1000 [1:40:28<05:26,  5.45s/it] 94%|█████████▍| 941/1000 [1:40:32<05:00,  5.09s/it] 94%|█████████▍| 942/1000 [1:40:37<04:43,  4.88s/it] 94%|█████████▍| 943/1000 [1:40:41<04:25,  4.66s/it] 94%|█████████▍| 944/1000 [1:40:54<06:51,  7.34s/it] 94%|█████████▍| 945/1000 [1:40:59<05:59,  6.54s/it] 95%|█████████▍| 946/1000 [1:41:13<07:46,  8.64s/it] 95%|█████████▍| 947/1000 [1:41:17<06:29,  7.34s/it] 95%|█████████▍| 948/1000 [1:41:25<06:26,  7.43s/it] 95%|█████████▍| 949/1000 [1:41:31<06:04,  7.15s/it] 95%|█████████▌| 950/1000 [1:41:34<05:01,  6.03s/it] 95%|█████████▌| 951/1000 [1:41:39<04:30,  5.51s/it] 95%|█████████▌| 952/1000 [1:41:47<05:07,  6.41s/it] 95%|█████████▌| 953/1000 [1:41:56<05:37,  7.18s/it] 95%|█████████▌| 954/1000 [1:42:02<05:14,  6.84s/it] 96%|█████████▌| 955/1000 [1:42:10<05:18,  7.08s/it] 96%|█████████▌| 956/1000 [1:42:14<04:29,  6.13s/it] 96%|█████████▌| 957/1000 [1:42:25<05:24,  7.54s/it] 96%|█████████▌| 958/1000 [1:42:28<04:26,  6.35s/it] 96%|█████████▌| 959/1000 [1:42:34<04:07,  6.04s/it] 96%|█████████▌| 960/1000 [1:42:38<03:46,  5.67s/it] 96%|█████████▌| 961/1000 [1:42:47<04:10,  6.42s/it] 96%|█████████▌| 962/1000 [1:42:51<03:45,  5.92s/it] 96%|█████████▋| 963/1000 [1:42:59<04:01,  6.53s/it] 96%|█████████▋| 964/1000 [1:43:05<03:49,  6.39s/it] 96%|█████████▋| 965/1000 [1:43:18<04:49,  8.26s/it] 97%|█████████▋| 966/1000 [1:43:26<04:34,  8.07s/it] 97%|█████████▋| 967/1000 [1:43:35<04:42,  8.55s/it] 97%|█████████▋| 968/1000 [1:43:39<03:50,  7.21s/it] 97%|█████████▋| 969/1000 [1:43:48<03:57,  7.67s/it] 97%|█████████▋| 970/1000 [1:43:52<03:13,  6.46s/it] 97%|█████████▋| 971/1000 [1:44:01<03:30,  7.27s/it] 97%|█████████▋| 972/1000 [1:44:05<02:54,  6.22s/it] 97%|█████████▋| 973/1000 [1:44:13<03:07,  6.95s/it] 97%|█████████▋| 974/1000 [1:44:19<02:52,  6.65s/it] 98%|█████████▊| 975/1000 [1:44:26<02:49,  6.79s/it] 98%|█████████▊| 976/1000 [1:44:32<02:33,  6.39s/it] 98%|█████████▊| 977/1000 [1:44:40<02:36,  6.82s/it] 98%|█████████▊| 978/1000 [1:44:45<02:17,  6.24s/it] 98%|█████████▊| 979/1000 [1:44:49<01:57,  5.59s/it] 98%|█████████▊| 980/1000 [1:44:56<02:01,  6.09s/it] 98%|█████████▊| 981/1000 [1:45:00<01:47,  5.66s/it] 98%|█████████▊| 982/1000 [1:45:07<01:48,  6.03s/it] 98%|█████████▊| 983/1000 [1:45:13<01:42,  6.02s/it] 98%|█████████▊| 984/1000 [1:45:19<01:34,  5.93s/it] 98%|█████████▊| 985/1000 [1:45:26<01:31,  6.08s/it] 99%|█████████▊| 986/1000 [1:45:31<01:23,  5.99s/it] 99%|█████████▊| 987/1000 [1:45:40<01:27,  6.72s/it] 99%|█████████▉| 988/1000 [1:45:45<01:15,  6.26s/it] 99%|█████████▉| 989/1000 [1:45:52<01:09,  6.36s/it] 99%|█████████▉| 990/1000 [1:45:57<01:01,  6.17s/it] 99%|█████████▉| 991/1000 [1:46:00<00:47,  5.24s/it] 99%|█████████▉| 992/1000 [1:46:06<00:42,  5.27s/it] 99%|█████████▉| 993/1000 [1:46:10<00:34,  4.94s/it] 99%|█████████▉| 994/1000 [1:46:19<00:36,  6.14s/it]100%|█████████▉| 995/1000 [1:46:25<00:30,  6.11s/it]100%|█████████▉| 996/1000 [1:46:37<00:31,  7.86s/it]100%|█████████▉| 997/1000 [1:46:41<00:20,  6.75s/it]100%|█████████▉| 998/1000 [1:46:45<00:12,  6.03s/it]100%|█████████▉| 999/1000 [1:46:55<00:07,  7.14s/it]100%|██████████| 1000/1000 [1:47:00<00:00,  6.52s/it]100%|██████████| 1000/1000 [1:47:00<00:00,  6.42s/it]
