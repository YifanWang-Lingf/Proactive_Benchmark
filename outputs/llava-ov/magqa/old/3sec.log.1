/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
ClipTestArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
attn_implementation=flash_attention_2,
augmentation=False,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
consecutive_n_frames_threshold=1,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
dataset_config=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=False,
embed_mark=2fps_384_1+3x3,
end_idx=2000,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=no,
eval_use_gather_object=False,
evaluation_strategy=None,
finetune_modules=['connector', 'mm_projector', 'response_head', 'related_head'],
first_n_frames_no_generate=0,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
frame_fps=2.0,
frame_num_tokens=49,
frame_resolution=384,
frame_token_cls=False,
frame_token_pooled=[7, 7],
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
grounding_mode=False,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
input_dir=/share2/wangyq/projects/MMDuet/datasets/shot2story/videos,
is_online_model=True,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
live_version=test,
llm_pretrained=lmms-lab/llava-onevision-qwen2-7b-ov,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=outputs/debug/runs/Apr19_12-55-39_ubuntu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lora_alpha=32,
lora_modules=model\.layers.*(q_proj|k_proj|v_proj|o_proj|gate_proj|up_proj|down_proj)$,
lora_pretrained=None,
lora_r=16,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_num_frames=400,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
no_output_before_user_input=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=outputs/debug,
output_fname=outputs/llava-ov/magqa/3sec.jsonl.1,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_assistant_turns=False,
remove_unused_columns=True,
repetition_penalty=None,
report_to=['tensorboard', 'wandb'],
response_min_interval_frames=None,
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=outputs/debug,
running_list_length=20,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
score_heads=informative_score,
seed=42,
skip_memory_metrics=True,
split_batches=None,
start_idx=1000,
stream_end_prob_threshold=None,
stream_end_score_sum_threshold=None,
stream_loss_weight=1.0,
system_prompt=A multimodal AI assistant is helping users with some activities. Below is their conversation, interleaved with the list of video frames received by the assistant.,
test_fname=/share2/wangyq/projects/MMDuet/datasets/shot2story/annotations/magqa_test.json,
tf32=None,
threshold_z=None,
time_instruction_format=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
v_placeholder=<image>,
video_chunk_sec=3,
video_pooling_stride=4,
vision_pretrained=google/siglip-large-patch16-384,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Loaded LLaVA model: lmms-lab/llava-onevision-qwen2-7b-ov
You are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 11966.63it/s]
Loading vision tower: google/siglip-so400m-patch14-384
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.patch_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.patch_embedding.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.position_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.probe: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.attention.in_proj_weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.attention.in_proj_bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.attention.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.attention.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)
  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:21<01:04, 21.47s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:43<00:44, 22.00s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:06<00:22, 22.32s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:12<00:00, 16.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:12<00:00, 18.22s/it]
Model Class: LlavaQwenForCausalLM
  0%|          | 0/1000 [00:00<?, ?it/s]/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/share2/wangyq/miniconda3_2558/envs/videollm-online/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:612: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  0%|          | 1/1000 [00:19<5:32:55, 20.00s/it]  0%|          | 2/1000 [00:32<4:21:35, 15.73s/it]  0%|          | 3/1000 [00:38<3:06:58, 11.25s/it]  0%|          | 4/1000 [00:47<2:53:59, 10.48s/it]  0%|          | 5/1000 [00:53<2:22:43,  8.61s/it]  1%|          | 6/1000 [00:58<2:06:07,  7.61s/it]  1%|          | 7/1000 [01:03<1:49:03,  6.59s/it]  1%|          | 8/1000 [01:09<1:44:24,  6.31s/it]  1%|          | 9/1000 [01:19<2:04:30,  7.54s/it]  1%|          | 10/1000 [01:25<1:56:55,  7.09s/it]  1%|          | 11/1000 [01:33<2:02:33,  7.44s/it]  1%|          | 12/1000 [01:54<3:09:01, 11.48s/it]  1%|▏         | 13/1000 [02:03<2:58:50, 10.87s/it]  1%|▏         | 14/1000 [02:09<2:31:31,  9.22s/it]  2%|▏         | 15/1000 [02:15<2:15:49,  8.27s/it]  2%|▏         | 16/1000 [02:23<2:13:28,  8.14s/it]  2%|▏         | 17/1000 [02:32<2:16:46,  8.35s/it]  2%|▏         | 18/1000 [02:41<2:21:28,  8.64s/it]  2%|▏         | 19/1000 [02:45<2:00:43,  7.38s/it]  2%|▏         | 20/1000 [02:59<2:29:27,  9.15s/it]  2%|▏         | 21/1000 [03:07<2:24:40,  8.87s/it]  2%|▏         | 22/1000 [03:14<2:18:53,  8.52s/it]  2%|▏         | 23/1000 [03:24<2:22:49,  8.77s/it]  2%|▏         | 24/1000 [03:33<2:22:11,  8.74s/it]  2%|▎         | 25/1000 [03:40<2:15:40,  8.35s/it]  3%|▎         | 26/1000 [03:46<2:03:57,  7.64s/it]  3%|▎         | 27/1000 [03:55<2:09:15,  7.97s/it]  3%|▎         | 28/1000 [03:59<1:52:27,  6.94s/it]  3%|▎         | 29/1000 [04:04<1:42:48,  6.35s/it]  3%|▎         | 30/1000 [04:13<1:53:46,  7.04s/it]  3%|▎         | 31/1000 [04:20<1:56:01,  7.18s/it]  3%|▎         | 32/1000 [04:26<1:46:19,  6.59s/it]  3%|▎         | 33/1000 [04:31<1:42:12,  6.34s/it]  3%|▎         | 34/1000 [04:36<1:35:05,  5.91s/it]  4%|▎         | 35/1000 [04:41<1:27:47,  5.46s/it]  4%|▎         | 36/1000 [04:49<1:39:42,  6.21s/it]  4%|▎         | 37/1000 [04:56<1:47:48,  6.72s/it]  4%|▍         | 38/1000 [05:07<2:04:03,  7.74s/it]  4%|▍         | 39/1000 [05:14<2:02:56,  7.68s/it]  4%|▍         | 40/1000 [05:22<2:02:27,  7.65s/it]  4%|▍         | 41/1000 [05:34<2:25:28,  9.10s/it]  4%|▍         | 42/1000 [05:43<2:23:53,  9.01s/it]  4%|▍         | 43/1000 [05:48<2:02:53,  7.70s/it]  4%|▍         | 44/1000 [05:51<1:41:04,  6.34s/it]  4%|▍         | 45/1000 [05:58<1:47:07,  6.73s/it]  5%|▍         | 46/1000 [06:08<2:00:47,  7.60s/it]  5%|▍         | 47/1000 [06:14<1:50:38,  6.97s/it]  5%|▍         | 48/1000 [06:23<2:01:39,  7.67s/it]  5%|▍         | 49/1000 [06:30<1:58:26,  7.47s/it]  5%|▌         | 50/1000 [06:35<1:46:04,  6.70s/it]  5%|▌         | 51/1000 [06:39<1:32:57,  5.88s/it]  5%|▌         | 52/1000 [06:49<1:51:54,  7.08s/it]  5%|▌         | 53/1000 [06:52<1:36:14,  6.10s/it]  5%|▌         | 54/1000 [06:59<1:39:05,  6.29s/it]  6%|▌         | 55/1000 [07:05<1:37:27,  6.19s/it]  6%|▌         | 56/1000 [07:13<1:45:16,  6.69s/it]  6%|▌         | 57/1000 [07:18<1:37:13,  6.19s/it]  6%|▌         | 58/1000 [07:26<1:47:41,  6.86s/it]  6%|▌         | 59/1000 [07:30<1:34:20,  6.02s/it]  6%|▌         | 60/1000 [07:35<1:28:33,  5.65s/it]  6%|▌         | 61/1000 [07:45<1:46:35,  6.81s/it]  6%|▌         | 62/1000 [07:52<1:50:18,  7.06s/it]  6%|▋         | 63/1000 [07:57<1:37:01,  6.21s/it]  6%|▋         | 64/1000 [08:05<1:48:50,  6.98s/it]  6%|▋         | 65/1000 [08:13<1:50:32,  7.09s/it]  7%|▋         | 66/1000 [08:21<1:57:35,  7.55s/it]  7%|▋         | 67/1000 [08:27<1:46:48,  6.87s/it]  7%|▋         | 68/1000 [08:34<1:47:26,  6.92s/it]  7%|▋         | 69/1000 [08:41<1:49:07,  7.03s/it]  7%|▋         | 70/1000 [08:50<1:58:14,  7.63s/it]  7%|▋         | 71/1000 [08:56<1:51:07,  7.18s/it]  7%|▋         | 72/1000 [09:02<1:42:33,  6.63s/it]  7%|▋         | 73/1000 [09:07<1:35:42,  6.19s/it]  7%|▋         | 74/1000 [09:15<1:47:01,  6.93s/it]  8%|▊         | 75/1000 [09:21<1:40:04,  6.49s/it]  8%|▊         | 76/1000 [09:27<1:36:33,  6.27s/it]  8%|▊         | 77/1000 [09:32<1:33:24,  6.07s/it]  8%|▊         | 78/1000 [09:40<1:42:53,  6.70s/it]  8%|▊         | 79/1000 [09:49<1:49:48,  7.15s/it]  8%|▊         | 80/1000 [09:55<1:45:04,  6.85s/it]  8%|▊         | 81/1000 [10:02<1:49:13,  7.13s/it]  8%|▊         | 82/1000 [10:10<1:50:33,  7.23s/it]  8%|▊         | 83/1000 [10:18<1:56:29,  7.62s/it]  8%|▊         | 84/1000 [10:28<2:02:51,  8.05s/it]  8%|▊         | 85/1000 [10:35<1:59:29,  7.84s/it]  9%|▊         | 86/1000 [10:41<1:53:34,  7.46s/it]  9%|▊         | 87/1000 [10:47<1:44:14,  6.85s/it]  9%|▉         | 88/1000 [10:54<1:46:45,  7.02s/it]  9%|▉         | 89/1000 [11:01<1:45:20,  6.94s/it]  9%|▉         | 90/1000 [11:03<1:24:29,  5.57s/it]  9%|▉         | 91/1000 [11:11<1:31:47,  6.06s/it]  9%|▉         | 92/1000 [11:13<1:16:16,  5.04s/it]  9%|▉         | 93/1000 [11:22<1:31:48,  6.07s/it]  9%|▉         | 94/1000 [11:26<1:25:14,  5.65s/it] 10%|▉         | 95/1000 [11:32<1:26:46,  5.75s/it] 10%|▉         | 96/1000 [11:38<1:24:01,  5.58s/it] 10%|▉         | 97/1000 [11:42<1:18:31,  5.22s/it] 10%|▉         | 98/1000 [11:52<1:38:12,  6.53s/it] 10%|▉         | 99/1000 [11:57<1:35:15,  6.34s/it] 10%|█         | 100/1000 [12:02<1:28:10,  5.88s/it] 10%|█         | 101/1000 [12:10<1:37:38,  6.52s/it] 10%|█         | 102/1000 [12:19<1:48:17,  7.24s/it] 10%|█         | 103/1000 [12:28<1:54:42,  7.67s/it] 10%|█         | 104/1000 [12:33<1:42:27,  6.86s/it] 10%|█         | 105/1000 [12:40<1:42:30,  6.87s/it] 11%|█         | 106/1000 [12:48<1:48:25,  7.28s/it] 11%|█         | 107/1000 [13:01<2:13:20,  8.96s/it] 11%|█         | 108/1000 [13:11<2:19:35,  9.39s/it] 11%|█         | 109/1000 [13:17<2:03:49,  8.34s/it] 11%|█         | 110/1000 [13:25<2:02:46,  8.28s/it] 11%|█         | 111/1000 [13:30<1:46:45,  7.21s/it] 11%|█         | 112/1000 [13:36<1:41:15,  6.84s/it] 11%|█▏        | 113/1000 [13:45<1:52:26,  7.61s/it] 11%|█▏        | 114/1000 [13:52<1:47:16,  7.26s/it] 12%|█▏        | 115/1000 [14:00<1:51:06,  7.53s/it] 12%|█▏        | 116/1000 [14:08<1:51:12,  7.55s/it] 12%|█▏        | 117/1000 [14:12<1:36:31,  6.56s/it] 12%|█▏        | 118/1000 [14:15<1:20:54,  5.50s/it] 12%|█▏        | 119/1000 [14:24<1:34:59,  6.47s/it] 12%|█▏        | 120/1000 [14:27<1:20:42,  5.50s/it] 12%|█▏        | 121/1000 [14:34<1:27:34,  5.98s/it] 12%|█▏        | 122/1000 [14:39<1:25:36,  5.85s/it] 12%|█▏        | 123/1000 [15:04<2:47:37, 11.47s/it] 12%|█▏        | 124/1000 [15:10<2:23:49,  9.85s/it] 12%|█▎        | 125/1000 [15:16<2:06:16,  8.66s/it] 13%|█▎        | 126/1000 [15:21<1:49:33,  7.52s/it] 13%|█▎        | 127/1000 [15:33<2:09:52,  8.93s/it] 13%|█▎        | 128/1000 [15:40<2:00:12,  8.27s/it] 13%|█▎        | 129/1000 [15:50<2:08:04,  8.82s/it] 13%|█▎        | 130/1000 [15:52<1:38:10,  6.77s/it] 13%|█▎        | 131/1000 [15:56<1:26:15,  5.96s/it] 13%|█▎        | 132/1000 [16:05<1:39:12,  6.86s/it] 13%|█▎        | 133/1000 [16:15<1:51:53,  7.74s/it] 13%|█▎        | 134/1000 [16:21<1:43:18,  7.16s/it] 14%|█▎        | 135/1000 [16:28<1:43:07,  7.15s/it] 14%|█▎        | 136/1000 [16:35<1:43:14,  7.17s/it] 14%|█▎        | 137/1000 [16:41<1:38:43,  6.86s/it] 14%|█▍        | 138/1000 [16:49<1:41:47,  7.08s/it] 14%|█▍        | 139/1000 [16:57<1:47:53,  7.52s/it] 14%|█▍        | 140/1000 [17:04<1:45:42,  7.37s/it] 14%|█▍        | 141/1000 [17:10<1:40:07,  6.99s/it] 14%|█▍        | 142/1000 [17:16<1:32:55,  6.50s/it] 14%|█▍        | 143/1000 [17:25<1:43:13,  7.23s/it] 14%|█▍        | 144/1000 [17:39<2:15:04,  9.47s/it] 14%|█▍        | 145/1000 [17:49<2:14:56,  9.47s/it] 15%|█▍        | 146/1000 [17:53<1:54:18,  8.03s/it] 15%|█▍        | 147/1000 [18:02<1:54:30,  8.05s/it] 15%|█▍        | 148/1000 [18:11<1:58:48,  8.37s/it] 15%|█▍        | 149/1000 [18:19<2:00:35,  8.50s/it] 15%|█▌        | 150/1000 [18:24<1:42:07,  7.21s/it] 15%|█▌        | 151/1000 [18:29<1:35:51,  6.77s/it] 15%|█▌        | 152/1000 [18:45<2:14:13,  9.50s/it] 15%|█▌        | 153/1000 [18:54<2:11:24,  9.31s/it] 15%|█▌        | 154/1000 [19:00<1:55:02,  8.16s/it] 16%|█▌        | 155/1000 [19:04<1:39:05,  7.04s/it] 16%|█▌        | 156/1000 [19:15<1:54:04,  8.11s/it] 16%|█▌        | 157/1000 [19:23<1:55:58,  8.25s/it] 16%|█▌        | 158/1000 [19:29<1:47:36,  7.67s/it] 16%|█▌        | 159/1000 [19:48<2:34:31, 11.02s/it] 16%|█▌        | 160/1000 [19:54<2:13:43,  9.55s/it] 16%|█▌        | 161/1000 [20:00<1:57:55,  8.43s/it] 16%|█▌        | 162/1000 [20:05<1:41:22,  7.26s/it] 16%|█▋        | 163/1000 [20:15<1:55:09,  8.25s/it] 16%|█▋        | 164/1000 [20:22<1:49:29,  7.86s/it] 16%|█▋        | 165/1000 [20:28<1:40:30,  7.22s/it] 17%|█▋        | 166/1000 [20:36<1:45:04,  7.56s/it] 17%|█▋        | 167/1000 [20:51<2:15:54,  9.79s/it] 17%|█▋        | 168/1000 [20:57<1:58:58,  8.58s/it] 17%|█▋        | 169/1000 [21:04<1:50:40,  7.99s/it] 17%|█▋        | 170/1000 [21:10<1:43:33,  7.49s/it] 17%|█▋        | 171/1000 [21:20<1:51:42,  8.09s/it] 17%|█▋        | 172/1000 [21:26<1:44:37,  7.58s/it] 17%|█▋        | 173/1000 [21:37<2:00:10,  8.72s/it] 17%|█▋        | 174/1000 [21:42<1:42:13,  7.43s/it] 18%|█▊        | 175/1000 [21:48<1:36:02,  6.98s/it] 18%|█▊        | 176/1000 [21:55<1:37:21,  7.09s/it] 18%|█▊        | 177/1000 [22:08<2:01:44,  8.88s/it] 18%|█▊        | 178/1000 [22:17<2:00:22,  8.79s/it] 18%|█▊        | 179/1000 [22:25<1:57:28,  8.59s/it] 18%|█▊        | 180/1000 [22:34<1:58:14,  8.65s/it] 18%|█▊        | 181/1000 [22:39<1:45:48,  7.75s/it] 18%|█▊        | 182/1000 [22:44<1:34:21,  6.92s/it] 18%|█▊        | 183/1000 [22:50<1:29:57,  6.61s/it] 18%|█▊        | 184/1000 [22:55<1:24:42,  6.23s/it] 18%|█▊        | 185/1000 [23:00<1:18:43,  5.80s/it] 19%|█▊        | 186/1000 [23:07<1:21:03,  5.97s/it] 19%|█▊        | 187/1000 [23:18<1:42:47,  7.59s/it] 19%|█▉        | 188/1000 [23:26<1:46:16,  7.85s/it] 19%|█▉        | 189/1000 [23:33<1:41:42,  7.52s/it] 19%|█▉        | 190/1000 [23:39<1:35:05,  7.04s/it] 19%|█▉        | 191/1000 [23:51<1:55:41,  8.58s/it] 19%|█▉        | 192/1000 [23:58<1:47:15,  7.97s/it] 19%|█▉        | 193/1000 [24:06<1:46:16,  7.90s/it] 19%|█▉        | 194/1000 [24:11<1:37:09,  7.23s/it] 20%|█▉        | 195/1000 [24:17<1:30:44,  6.76s/it] 20%|█▉        | 196/1000 [24:24<1:31:22,  6.82s/it] 20%|█▉        | 197/1000 [24:33<1:39:10,  7.41s/it] 20%|█▉        | 198/1000 [24:36<1:20:52,  6.05s/it] 20%|█▉        | 199/1000 [24:42<1:22:37,  6.19s/it] 20%|██        | 200/1000 [24:48<1:21:08,  6.09s/it] 20%|██        | 201/1000 [24:59<1:39:45,  7.49s/it] 20%|██        | 202/1000 [25:04<1:31:49,  6.90s/it] 20%|██        | 203/1000 [25:11<1:29:54,  6.77s/it] 20%|██        | 204/1000 [25:18<1:31:34,  6.90s/it] 20%|██        | 205/1000 [25:23<1:23:28,  6.30s/it] 21%|██        | 206/1000 [25:29<1:21:40,  6.17s/it] 21%|██        | 207/1000 [25:35<1:22:27,  6.24s/it] 21%|██        | 208/1000 [25:43<1:27:42,  6.65s/it] 21%|██        | 209/1000 [25:59<2:04:20,  9.43s/it] 21%|██        | 210/1000 [26:11<2:16:09, 10.34s/it] 21%|██        | 211/1000 [26:13<1:43:10,  7.85s/it] 21%|██        | 212/1000 [26:28<2:09:25,  9.86s/it] 21%|██▏       | 213/1000 [26:35<2:00:49,  9.21s/it] 21%|██▏       | 214/1000 [26:44<2:00:33,  9.20s/it] 22%|██▏       | 215/1000 [26:49<1:43:19,  7.90s/it] 22%|██▏       | 216/1000 [26:55<1:35:45,  7.33s/it] 22%|██▏       | 217/1000 [27:01<1:28:28,  6.78s/it] 22%|██▏       | 218/1000 [27:09<1:33:57,  7.21s/it] 22%|██▏       | 219/1000 [27:19<1:43:26,  7.95s/it] 22%|██▏       | 220/1000 [27:27<1:45:26,  8.11s/it] 22%|██▏       | 221/1000 [27:33<1:37:14,  7.49s/it] 22%|██▏       | 222/1000 [27:41<1:39:24,  7.67s/it] 22%|██▏       | 223/1000 [27:49<1:40:27,  7.76s/it] 22%|██▏       | 224/1000 [27:55<1:32:00,  7.11s/it] 22%|██▎       | 225/1000 [28:01<1:27:36,  6.78s/it] 23%|██▎       | 226/1000 [28:08<1:30:33,  7.02s/it] 23%|██▎       | 227/1000 [28:16<1:30:29,  7.02s/it] 23%|██▎       | 228/1000 [28:21<1:23:36,  6.50s/it] 23%|██▎       | 229/1000 [28:25<1:13:56,  5.75s/it] 23%|██▎       | 230/1000 [28:32<1:19:35,  6.20s/it] 23%|██▎       | 231/1000 [28:39<1:22:07,  6.41s/it] 23%|██▎       | 232/1000 [28:45<1:21:16,  6.35s/it] 23%|██▎       | 233/1000 [28:55<1:35:16,  7.45s/it] 23%|██▎       | 234/1000 [29:02<1:34:07,  7.37s/it] 24%|██▎       | 235/1000 [29:11<1:38:27,  7.72s/it] 24%|██▎       | 236/1000 [29:18<1:36:19,  7.56s/it] 24%|██▎       | 237/1000 [29:24<1:28:36,  6.97s/it] 24%|██▍       | 238/1000 [29:35<1:44:33,  8.23s/it] 24%|██▍       | 239/1000 [29:44<1:48:22,  8.54s/it] 24%|██▍       | 240/1000 [29:55<1:56:13,  9.18s/it] 24%|██▍       | 241/1000 [30:07<2:08:30, 10.16s/it] 24%|██▍       | 242/1000 [30:15<2:00:10,  9.51s/it] 24%|██▍       | 243/1000 [30:23<1:52:01,  8.88s/it] 24%|██▍       | 244/1000 [30:35<2:04:13,  9.86s/it] 24%|██▍       | 245/1000 [30:44<2:03:06,  9.78s/it] 25%|██▍       | 246/1000 [30:50<1:48:14,  8.61s/it] 25%|██▍       | 247/1000 [31:00<1:51:59,  8.92s/it] 25%|██▍       | 248/1000 [31:07<1:44:22,  8.33s/it] 25%|██▍       | 249/1000 [31:25<2:20:43, 11.24s/it] 25%|██▌       | 250/1000 [31:29<1:53:06,  9.05s/it] 25%|██▌       | 251/1000 [31:37<1:49:17,  8.76s/it] 25%|██▌       | 252/1000 [31:47<1:55:25,  9.26s/it] 25%|██▌       | 253/1000 [31:52<1:37:22,  7.82s/it] 25%|██▌       | 254/1000 [32:00<1:36:59,  7.80s/it] 26%|██▌       | 255/1000 [32:05<1:27:59,  7.09s/it] 26%|██▌       | 256/1000 [32:18<1:49:13,  8.81s/it] 26%|██▌       | 257/1000 [32:23<1:33:56,  7.59s/it] 26%|██▌       | 258/1000 [32:32<1:40:59,  8.17s/it] 26%|██▌       | 259/1000 [32:37<1:30:31,  7.33s/it] 26%|██▌       | 260/1000 [32:46<1:34:00,  7.62s/it] 26%|██▌       | 261/1000 [32:51<1:25:49,  6.97s/it] 26%|██▌       | 262/1000 [32:55<1:14:24,  6.05s/it] 26%|██▋       | 263/1000 [33:06<1:30:25,  7.36s/it] 26%|██▋       | 264/1000 [33:18<1:50:27,  9.01s/it] 26%|██▋       | 265/1000 [33:27<1:50:00,  8.98s/it] 27%|██▋       | 266/1000 [33:32<1:33:59,  7.68s/it] 27%|██▋       | 267/1000 [33:37<1:25:48,  7.02s/it] 27%|██▋       | 268/1000 [33:42<1:16:19,  6.26s/it] 27%|██▋       | 269/1000 [33:52<1:31:05,  7.48s/it] 27%|██▋       | 270/1000 [34:03<1:41:22,  8.33s/it] 27%|██▋       | 271/1000 [34:08<1:31:05,  7.50s/it] 27%|██▋       | 272/1000 [34:13<1:22:56,  6.84s/it] 27%|██▋       | 273/1000 [34:20<1:20:28,  6.64s/it] 27%|██▋       | 274/1000 [34:25<1:15:16,  6.22s/it] 28%|██▊       | 275/1000 [34:28<1:05:47,  5.45s/it] 28%|██▊       | 276/1000 [34:36<1:13:56,  6.13s/it] 28%|██▊       | 277/1000 [34:49<1:38:37,  8.18s/it] 28%|██▊       | 278/1000 [35:04<2:02:46, 10.20s/it] 28%|██▊       | 279/1000 [35:10<1:46:08,  8.83s/it] 28%|██▊       | 280/1000 [35:13<1:26:21,  7.20s/it] 28%|██▊       | 281/1000 [35:18<1:18:35,  6.56s/it] 28%|██▊       | 282/1000 [35:25<1:21:04,  6.78s/it] 28%|██▊       | 283/1000 [35:32<1:19:01,  6.61s/it] 28%|██▊       | 284/1000 [35:50<2:00:05, 10.06s/it] 28%|██▊       | 285/1000 [35:54<1:38:46,  8.29s/it] 29%|██▊       | 286/1000 [36:01<1:34:33,  7.95s/it] 29%|██▊       | 287/1000 [36:10<1:37:51,  8.23s/it] 29%|██▉       | 288/1000 [36:16<1:29:45,  7.56s/it] 29%|██▉       | 289/1000 [36:27<1:42:56,  8.69s/it] 29%|██▉       | 290/1000 [36:32<1:29:24,  7.56s/it] 29%|██▉       | 291/1000 [36:46<1:50:51,  9.38s/it] 29%|██▉       | 292/1000 [36:55<1:49:18,  9.26s/it] 29%|██▉       | 293/1000 [37:04<1:48:45,  9.23s/it] 29%|██▉       | 294/1000 [37:12<1:43:52,  8.83s/it] 30%|██▉       | 295/1000 [37:24<1:55:41,  9.85s/it] 30%|██▉       | 296/1000 [37:29<1:38:48,  8.42s/it] 30%|██▉       | 297/1000 [37:34<1:27:04,  7.43s/it] 30%|██▉       | 298/1000 [37:37<1:09:18,  5.92s/it] 30%|██▉       | 299/1000 [37:48<1:28:38,  7.59s/it] 30%|███       | 300/1000 [37:56<1:28:07,  7.55s/it] 30%|███       | 301/1000 [38:05<1:32:56,  7.98s/it] 30%|███       | 302/1000 [38:12<1:29:21,  7.68s/it] 30%|███       | 303/1000 [38:23<1:43:28,  8.91s/it] 30%|███       | 304/1000 [38:26<1:22:23,  7.10s/it] 30%|███       | 305/1000 [38:32<1:18:23,  6.77s/it] 31%|███       | 306/1000 [38:40<1:20:43,  6.98s/it] 31%|███       | 307/1000 [38:44<1:12:16,  6.26s/it] 31%|███       | 308/1000 [38:49<1:05:43,  5.70s/it] 31%|███       | 309/1000 [38:56<1:12:09,  6.27s/it] 31%|███       | 310/1000 [39:05<1:21:42,  7.11s/it] 31%|███       | 311/1000 [39:07<1:03:50,  5.56s/it] 31%|███       | 312/1000 [39:12<1:00:54,  5.31s/it] 31%|███▏      | 313/1000 [39:20<1:09:08,  6.04s/it] 31%|███▏      | 314/1000 [39:25<1:05:49,  5.76s/it] 32%|███▏      | 315/1000 [39:31<1:06:05,  5.79s/it] 32%|███▏      | 316/1000 [39:36<1:03:25,  5.56s/it] 32%|███▏      | 317/1000 [39:45<1:15:25,  6.63s/it] 32%|███▏      | 318/1000 [39:51<1:12:21,  6.37s/it] 32%|███▏      | 319/1000 [40:00<1:23:38,  7.37s/it] 32%|███▏      | 320/1000 [40:11<1:33:28,  8.25s/it] 32%|███▏      | 321/1000 [40:22<1:44:31,  9.24s/it] 32%|███▏      | 322/1000 [40:32<1:47:28,  9.51s/it] 32%|███▏      | 323/1000 [40:39<1:36:52,  8.59s/it] 32%|███▏      | 324/1000 [40:50<1:46:37,  9.46s/it] 32%|███▎      | 325/1000 [40:53<1:25:20,  7.59s/it] 33%|███▎      | 326/1000 [41:05<1:39:16,  8.84s/it] 33%|███▎      | 327/1000 [41:13<1:33:54,  8.37s/it] 33%|███▎      | 328/1000 [41:20<1:29:51,  8.02s/it] 33%|███▎      | 329/1000 [41:27<1:27:05,  7.79s/it] 33%|███▎      | 330/1000 [41:34<1:23:36,  7.49s/it] 33%|███▎      | 331/1000 [41:39<1:15:43,  6.79s/it] 33%|███▎      | 332/1000 [41:48<1:23:42,  7.52s/it] 33%|███▎      | 333/1000 [42:00<1:38:22,  8.85s/it] 33%|███▎      | 334/1000 [42:10<1:41:44,  9.17s/it] 34%|███▎      | 335/1000 [42:20<1:45:06,  9.48s/it] 34%|███▎      | 336/1000 [42:24<1:24:24,  7.63s/it] 34%|███▎      | 337/1000 [42:29<1:16:20,  6.91s/it] 34%|███▍      | 338/1000 [42:33<1:08:35,  6.22s/it] 34%|███▍      | 339/1000 [42:39<1:06:44,  6.06s/it] 34%|███▍      | 340/1000 [42:45<1:04:53,  5.90s/it] 34%|███▍      | 341/1000 [42:53<1:12:44,  6.62s/it] 34%|███▍      | 342/1000 [43:00<1:12:46,  6.64s/it] 34%|███▍      | 343/1000 [43:05<1:08:12,  6.23s/it] 34%|███▍      | 344/1000 [43:13<1:14:29,  6.81s/it] 34%|███▍      | 345/1000 [43:17<1:06:08,  6.06s/it] 35%|███▍      | 346/1000 [43:33<1:38:57,  9.08s/it] 35%|███▍      | 347/1000 [43:36<1:16:59,  7.08s/it] 35%|███▍      | 348/1000 [43:43<1:16:38,  7.05s/it] 35%|███▍      | 349/1000 [43:49<1:13:34,  6.78s/it] 35%|███▌      | 350/1000 [43:59<1:23:05,  7.67s/it] 35%|███▌      | 351/1000 [44:04<1:14:35,  6.90s/it] 35%|███▌      | 352/1000 [44:10<1:10:44,  6.55s/it] 35%|███▌      | 353/1000 [44:14<1:03:10,  5.86s/it] 35%|███▌      | 354/1000 [44:25<1:20:05,  7.44s/it] 36%|███▌      | 355/1000 [44:29<1:08:04,  6.33s/it] 36%|███▌      | 356/1000 [44:39<1:19:48,  7.44s/it] 36%|███▌      | 357/1000 [44:45<1:15:30,  7.05s/it] 36%|███▌      | 358/1000 [44:51<1:11:16,  6.66s/it] 36%|███▌      | 359/1000 [45:01<1:22:16,  7.70s/it] 36%|███▌      | 360/1000 [45:05<1:11:54,  6.74s/it] 36%|███▌      | 361/1000 [45:14<1:19:51,  7.50s/it] 36%|███▌      | 362/1000 [45:19<1:11:33,  6.73s/it] 36%|███▋      | 363/1000 [45:26<1:09:54,  6.59s/it] 36%|███▋      | 364/1000 [45:34<1:15:14,  7.10s/it] 36%|███▋      | 365/1000 [45:41<1:14:59,  7.09s/it] 37%|███▋      | 366/1000 [45:50<1:22:22,  7.80s/it] 37%|███▋      | 367/1000 [45:57<1:19:01,  7.49s/it] 37%|███▋      | 368/1000 [46:02<1:11:22,  6.78s/it] 37%|███▋      | 369/1000 [46:08<1:09:02,  6.56s/it] 37%|███▋      | 370/1000 [46:15<1:09:17,  6.60s/it] 37%|███▋      | 371/1000 [46:22<1:10:23,  6.71s/it] 37%|███▋      | 372/1000 [46:30<1:12:33,  6.93s/it] 37%|███▋      | 373/1000 [46:39<1:21:33,  7.81s/it] 37%|███▋      | 374/1000 [46:47<1:20:15,  7.69s/it] 38%|███▊      | 375/1000 [46:55<1:21:43,  7.85s/it] 38%|███▊      | 376/1000 [47:09<1:40:21,  9.65s/it] 38%|███▊      | 377/1000 [47:18<1:38:21,  9.47s/it] 38%|███▊      | 378/1000 [47:26<1:33:40,  9.04s/it] 38%|███▊      | 379/1000 [47:31<1:21:54,  7.91s/it] 38%|███▊      | 380/1000 [47:41<1:27:43,  8.49s/it] 38%|███▊      | 381/1000 [47:47<1:18:47,  7.64s/it] 38%|███▊      | 382/1000 [47:55<1:20:23,  7.81s/it] 38%|███▊      | 383/1000 [48:06<1:31:40,  8.92s/it] 38%|███▊      | 384/1000 [48:14<1:28:41,  8.64s/it] 38%|███▊      | 385/1000 [48:24<1:32:25,  9.02s/it] 39%|███▊      | 386/1000 [48:33<1:29:50,  8.78s/it] 39%|███▊      | 387/1000 [48:41<1:28:16,  8.64s/it] 39%|███▉      | 388/1000 [48:57<1:51:06, 10.89s/it] 39%|███▉      | 389/1000 [49:07<1:47:32, 10.56s/it] 39%|███▉      | 390/1000 [49:16<1:44:15, 10.25s/it] 39%|███▉      | 391/1000 [49:21<1:27:24,  8.61s/it] 39%|███▉      | 392/1000 [49:27<1:17:53,  7.69s/it] 39%|███▉      | 393/1000 [49:41<1:38:51,  9.77s/it] 39%|███▉      | 394/1000 [49:45<1:20:37,  7.98s/it] 40%|███▉      | 395/1000 [49:51<1:15:09,  7.45s/it] 40%|███▉      | 396/1000 [49:59<1:15:34,  7.51s/it] 40%|███▉      | 397/1000 [50:03<1:05:32,  6.52s/it] 40%|███▉      | 398/1000 [50:13<1:14:08,  7.39s/it] 40%|███▉      | 399/1000 [50:16<1:01:52,  6.18s/it] 40%|████      | 400/1000 [50:28<1:18:10,  7.82s/it] 40%|████      | 401/1000 [50:38<1:26:33,  8.67s/it] 40%|████      | 402/1000 [50:45<1:21:44,  8.20s/it] 40%|████      | 403/1000 [50:53<1:20:06,  8.05s/it] 40%|████      | 404/1000 [51:00<1:17:36,  7.81s/it] 40%|████      | 405/1000 [51:07<1:15:38,  7.63s/it] 41%|████      | 406/1000 [51:11<1:02:29,  6.31s/it] 41%|████      | 407/1000 [51:16<57:56,  5.86s/it]   41%|████      | 408/1000 [51:21<57:27,  5.82s/it] 41%|████      | 409/1000 [51:30<1:05:59,  6.70s/it] 41%|████      | 410/1000 [51:37<1:06:05,  6.72s/it] 41%|████      | 411/1000 [51:43<1:04:55,  6.61s/it] 41%|████      | 412/1000 [51:48<58:29,  5.97s/it]   41%|████▏     | 413/1000 [51:54<1:00:53,  6.22s/it] 41%|████▏     | 414/1000 [52:05<1:13:43,  7.55s/it] 42%|████▏     | 415/1000 [52:11<1:08:59,  7.08s/it] 42%|████▏     | 416/1000 [52:18<1:09:55,  7.18s/it] 42%|████▏     | 417/1000 [52:30<1:21:50,  8.42s/it] 42%|████▏     | 418/1000 [52:39<1:23:52,  8.65s/it] 42%|████▏     | 419/1000 [52:44<1:13:50,  7.62s/it] 42%|████▏     | 420/1000 [52:50<1:07:37,  7.00s/it] 42%|████▏     | 421/1000 [52:56<1:04:15,  6.66s/it] 42%|████▏     | 422/1000 [53:04<1:08:14,  7.08s/it] 42%|████▏     | 423/1000 [53:12<1:11:41,  7.46s/it] 42%|████▏     | 424/1000 [53:18<1:08:37,  7.15s/it] 42%|████▎     | 425/1000 [53:33<1:29:01,  9.29s/it] 43%|████▎     | 426/1000 [53:41<1:27:19,  9.13s/it] 43%|████▎     | 427/1000 [53:47<1:17:30,  8.12s/it] 43%|████▎     | 428/1000 [53:52<1:08:54,  7.23s/it] 43%|████▎     | 429/1000 [53:56<58:49,  6.18s/it]   43%|████▎     | 430/1000 [54:00<51:52,  5.46s/it] 43%|████▎     | 431/1000 [54:11<1:07:02,  7.07s/it] 43%|████▎     | 432/1000 [54:21<1:15:20,  7.96s/it] 43%|████▎     | 433/1000 [54:27<1:10:09,  7.42s/it] 43%|████▎     | 434/1000 [54:33<1:06:39,  7.07s/it] 44%|████▎     | 435/1000 [54:44<1:18:11,  8.30s/it] 44%|████▎     | 436/1000 [54:50<1:11:47,  7.64s/it] 44%|████▎     | 437/1000 [54:55<1:03:55,  6.81s/it] 44%|████▍     | 438/1000 [55:00<58:53,  6.29s/it]   44%|████▍     | 439/1000 [55:03<49:43,  5.32s/it] 44%|████▍     | 440/1000 [55:09<51:07,  5.48s/it] 44%|████▍     | 441/1000 [55:18<1:00:43,  6.52s/it] 44%|████▍     | 442/1000 [55:35<1:29:10,  9.59s/it] 44%|████▍     | 443/1000 [55:41<1:18:37,  8.47s/it] 44%|████▍     | 444/1000 [55:48<1:13:49,  7.97s/it] 44%|████▍     | 445/1000 [55:53<1:07:26,  7.29s/it] 45%|████▍     | 446/1000 [56:00<1:04:55,  7.03s/it] 45%|████▍     | 447/1000 [56:03<54:38,  5.93s/it]   45%|████▍     | 448/1000 [56:07<49:22,  5.37s/it] 45%|████▍     | 449/1000 [56:13<50:35,  5.51s/it] 45%|████▌     | 450/1000 [56:25<1:07:59,  7.42s/it] 45%|████▌     | 451/1000 [56:32<1:07:11,  7.34s/it] 45%|████▌     | 452/1000 [56:36<57:46,  6.33s/it]   45%|████▌     | 453/1000 [56:41<54:42,  6.00s/it] 45%|████▌     | 454/1000 [56:47<53:28,  5.88s/it] 46%|████▌     | 455/1000 [56:59<1:11:18,  7.85s/it] 46%|████▌     | 456/1000 [57:08<1:12:20,  7.98s/it] 46%|████▌     | 457/1000 [57:14<1:08:01,  7.52s/it] 46%|████▌     | 458/1000 [57:18<58:05,  6.43s/it]   46%|████▌     | 459/1000 [57:32<1:19:51,  8.86s/it] 46%|████▌     | 460/1000 [57:35<1:03:38,  7.07s/it] 46%|████▌     | 461/1000 [57:40<58:12,  6.48s/it]   46%|████▌     | 462/1000 [57:46<54:17,  6.05s/it] 46%|████▋     | 463/1000 [57:51<53:43,  6.00s/it] 46%|████▋     | 464/1000 [57:57<53:52,  6.03s/it] 46%|████▋     | 465/1000 [58:05<58:36,  6.57s/it] 47%|████▋     | 466/1000 [58:15<1:07:47,  7.62s/it] 47%|████▋     | 467/1000 [58:23<1:08:46,  7.74s/it] 47%|████▋     | 468/1000 [58:34<1:14:56,  8.45s/it] 47%|████▋     | 469/1000 [58:38<1:04:43,  7.31s/it] 47%|████▋     | 470/1000 [58:45<1:04:06,  7.26s/it] 47%|████▋     | 471/1000 [58:58<1:18:43,  8.93s/it] 47%|████▋     | 472/1000 [59:03<1:07:11,  7.63s/it] 47%|████▋     | 473/1000 [59:15<1:19:17,  9.03s/it] 47%|████▋     | 474/1000 [59:21<1:10:10,  8.00s/it] 48%|████▊     | 475/1000 [59:29<1:09:51,  7.98s/it] 48%|████▊     | 476/1000 [59:35<1:06:29,  7.61s/it] 48%|████▊     | 477/1000 [59:42<1:05:00,  7.46s/it] 48%|████▊     | 478/1000 [59:51<1:08:59,  7.93s/it] 48%|████▊     | 479/1000 [1:00:00<1:09:56,  8.05s/it] 48%|████▊     | 480/1000 [1:00:04<1:00:04,  6.93s/it] 48%|████▊     | 481/1000 [1:00:11<1:00:53,  7.04s/it] 48%|████▊     | 482/1000 [1:00:15<52:35,  6.09s/it]   48%|████▊     | 483/1000 [1:00:22<54:31,  6.33s/it] 48%|████▊     | 484/1000 [1:00:36<1:14:21,  8.65s/it] 48%|████▊     | 485/1000 [1:00:44<1:10:57,  8.27s/it] 49%|████▊     | 486/1000 [1:00:50<1:06:05,  7.72s/it] 49%|████▊     | 487/1000 [1:00:55<58:23,  6.83s/it]   49%|████▉     | 488/1000 [1:00:58<48:11,  5.65s/it] 49%|████▉     | 489/1000 [1:01:10<1:05:21,  7.67s/it] 49%|████▉     | 490/1000 [1:01:23<1:17:30,  9.12s/it] 49%|████▉     | 491/1000 [1:01:30<1:12:06,  8.50s/it] 49%|████▉     | 492/1000 [1:01:35<1:04:03,  7.57s/it] 49%|████▉     | 493/1000 [1:01:40<56:42,  6.71s/it]   49%|████▉     | 494/1000 [1:01:47<57:40,  6.84s/it] 50%|████▉     | 495/1000 [1:01:52<53:28,  6.35s/it] 50%|████▉     | 496/1000 [1:01:59<55:41,  6.63s/it] 50%|████▉     | 497/1000 [1:02:08<1:00:53,  7.26s/it] 50%|████▉     | 498/1000 [1:02:13<55:50,  6.67s/it]   50%|████▉     | 499/1000 [1:02:20<56:03,  6.71s/it] 50%|█████     | 500/1000 [1:02:29<1:00:24,  7.25s/it] 50%|█████     | 501/1000 [1:02:32<49:37,  5.97s/it]   50%|█████     | 502/1000 [1:02:37<48:56,  5.90s/it] 50%|█████     | 503/1000 [1:02:46<54:34,  6.59s/it] 50%|█████     | 504/1000 [1:02:51<51:36,  6.24s/it] 50%|█████     | 505/1000 [1:02:54<42:45,  5.18s/it] 51%|█████     | 506/1000 [1:02:57<36:53,  4.48s/it] 51%|█████     | 507/1000 [1:03:01<37:42,  4.59s/it] 51%|█████     | 508/1000 [1:03:07<39:30,  4.82s/it] 51%|█████     | 509/1000 [1:03:12<40:53,  5.00s/it] 51%|█████     | 510/1000 [1:03:20<48:49,  5.98s/it] 51%|█████     | 511/1000 [1:03:26<46:31,  5.71s/it] 51%|█████     | 512/1000 [1:03:31<45:11,  5.56s/it] 51%|█████▏    | 513/1000 [1:03:36<44:25,  5.47s/it] 51%|█████▏    | 514/1000 [1:03:46<54:55,  6.78s/it] 52%|█████▏    | 515/1000 [1:03:49<46:28,  5.75s/it] 52%|█████▏    | 516/1000 [1:04:04<1:08:07,  8.45s/it] 52%|█████▏    | 517/1000 [1:04:11<1:03:34,  7.90s/it] 52%|█████▏    | 518/1000 [1:04:22<1:11:19,  8.88s/it] 52%|█████▏    | 519/1000 [1:04:32<1:13:47,  9.20s/it] 52%|█████▏    | 520/1000 [1:04:36<1:02:18,  7.79s/it] 52%|█████▏    | 521/1000 [1:04:45<1:05:30,  8.21s/it] 52%|█████▏    | 522/1000 [1:04:51<58:35,  7.35s/it]   52%|█████▏    | 523/1000 [1:04:59<1:00:54,  7.66s/it] 52%|█████▏    | 524/1000 [1:05:05<57:06,  7.20s/it]   52%|█████▎    | 525/1000 [1:05:13<58:25,  7.38s/it] 53%|█████▎    | 526/1000 [1:05:21<58:45,  7.44s/it] 53%|█████▎    | 527/1000 [1:05:27<56:40,  7.19s/it] 53%|█████▎    | 528/1000 [1:05:39<1:08:05,  8.66s/it] 53%|█████▎    | 529/1000 [1:05:44<57:57,  7.38s/it]   53%|█████▎    | 530/1000 [1:05:51<57:43,  7.37s/it] 53%|█████▎    | 531/1000 [1:05:55<49:16,  6.30s/it] 53%|█████▎    | 532/1000 [1:06:02<50:19,  6.45s/it] 53%|█████▎    | 533/1000 [1:06:05<42:26,  5.45s/it] 53%|█████▎    | 534/1000 [1:06:10<42:10,  5.43s/it] 54%|█████▎    | 535/1000 [1:06:15<40:48,  5.26s/it] 54%|█████▎    | 536/1000 [1:06:21<41:34,  5.38s/it] 54%|█████▎    | 537/1000 [1:06:26<40:47,  5.29s/it] 54%|█████▍    | 538/1000 [1:06:29<36:07,  4.69s/it] 54%|█████▍    | 539/1000 [1:06:34<37:10,  4.84s/it] 54%|█████▍    | 540/1000 [1:06:41<41:15,  5.38s/it] 54%|█████▍    | 541/1000 [1:06:46<40:16,  5.26s/it] 54%|█████▍    | 542/1000 [1:06:54<46:38,  6.11s/it] 54%|█████▍    | 543/1000 [1:07:04<54:46,  7.19s/it] 54%|█████▍    | 544/1000 [1:07:14<1:00:45,  7.99s/it] 55%|█████▍    | 545/1000 [1:07:20<58:07,  7.66s/it]   55%|█████▍    | 546/1000 [1:07:31<1:05:22,  8.64s/it] 55%|█████▍    | 547/1000 [1:07:39<1:03:59,  8.48s/it] 55%|█████▍    | 548/1000 [1:07:43<53:30,  7.10s/it]   55%|█████▍    | 549/1000 [1:07:51<55:02,  7.32s/it] 55%|█████▌    | 550/1000 [1:08:03<1:04:59,  8.66s/it] 55%|█████▌    | 551/1000 [1:08:15<1:11:44,  9.59s/it] 55%|█████▌    | 552/1000 [1:08:25<1:13:40,  9.87s/it] 55%|█████▌    | 553/1000 [1:08:29<59:53,  8.04s/it]   55%|█████▌    | 554/1000 [1:08:35<55:47,  7.50s/it] 56%|█████▌    | 555/1000 [1:08:47<1:05:49,  8.87s/it] 56%|█████▌    | 556/1000 [1:08:52<55:47,  7.54s/it]   56%|█████▌    | 557/1000 [1:08:58<53:47,  7.29s/it] 56%|█████▌    | 558/1000 [1:09:04<50:14,  6.82s/it] 56%|█████▌    | 559/1000 [1:09:14<55:48,  7.59s/it] 56%|█████▌    | 560/1000 [1:09:19<50:06,  6.83s/it] 56%|█████▌    | 561/1000 [1:09:27<52:12,  7.14s/it] 56%|█████▌    | 562/1000 [1:09:30<44:54,  6.15s/it] 56%|█████▋    | 563/1000 [1:09:38<48:00,  6.59s/it] 56%|█████▋    | 564/1000 [1:09:42<43:13,  5.95s/it] 56%|█████▋    | 565/1000 [1:09:47<40:03,  5.52s/it] 57%|█████▋    | 566/1000 [1:09:51<36:16,  5.01s/it] 57%|█████▋    | 567/1000 [1:09:57<38:56,  5.40s/it] 57%|█████▋    | 568/1000 [1:10:05<45:01,  6.25s/it] 57%|█████▋    | 569/1000 [1:10:11<43:57,  6.12s/it] 57%|█████▋    | 570/1000 [1:10:24<58:09,  8.12s/it] 57%|█████▋    | 571/1000 [1:10:32<57:52,  8.09s/it] 57%|█████▋    | 572/1000 [1:10:39<55:01,  7.71s/it] 57%|█████▋    | 573/1000 [1:10:45<50:44,  7.13s/it] 57%|█████▋    | 574/1000 [1:10:50<47:01,  6.62s/it] 57%|█████▊    | 575/1000 [1:10:56<45:18,  6.40s/it] 58%|█████▊    | 576/1000 [1:11:10<1:01:13,  8.66s/it] 58%|█████▊    | 577/1000 [1:11:18<59:53,  8.50s/it]   58%|█████▊    | 578/1000 [1:11:26<58:37,  8.33s/it] 58%|█████▊    | 579/1000 [1:11:31<51:31,  7.34s/it] 58%|█████▊    | 580/1000 [1:11:38<51:28,  7.35s/it] 58%|█████▊    | 581/1000 [1:11:47<54:09,  7.76s/it] 58%|█████▊    | 582/1000 [1:11:57<58:09,  8.35s/it] 58%|█████▊    | 583/1000 [1:12:02<51:14,  7.37s/it] 58%|█████▊    | 584/1000 [1:12:08<48:33,  7.00s/it] 58%|█████▊    | 585/1000 [1:12:17<52:04,  7.53s/it] 59%|█████▊    | 586/1000 [1:12:25<54:15,  7.86s/it] 59%|█████▊    | 587/1000 [1:12:31<49:44,  7.23s/it] 59%|█████▉    | 588/1000 [1:12:45<1:03:36,  9.26s/it] 59%|█████▉    | 589/1000 [1:12:50<54:59,  8.03s/it]   59%|█████▉    | 590/1000 [1:12:58<53:54,  7.89s/it] 59%|█████▉    | 591/1000 [1:13:11<1:05:24,  9.60s/it] 59%|█████▉    | 592/1000 [1:13:26<1:15:55, 11.17s/it] 59%|█████▉    | 593/1000 [1:13:33<1:06:12,  9.76s/it] 59%|█████▉    | 594/1000 [1:13:39<59:47,  8.84s/it]   60%|█████▉    | 595/1000 [1:13:46<55:32,  8.23s/it] 60%|█████▉    | 596/1000 [1:13:52<51:22,  7.63s/it] 60%|█████▉    | 597/1000 [1:13:59<48:21,  7.20s/it] 60%|█████▉    | 598/1000 [1:14:05<45:36,  6.81s/it] 60%|█████▉    | 599/1000 [1:14:15<52:21,  7.83s/it] 60%|██████    | 600/1000 [1:14:24<55:54,  8.39s/it] 60%|██████    | 601/1000 [1:14:29<48:07,  7.24s/it] 60%|██████    | 602/1000 [1:14:34<43:08,  6.50s/it] 60%|██████    | 603/1000 [1:14:41<43:32,  6.58s/it] 60%|██████    | 604/1000 [1:14:49<48:06,  7.29s/it] 60%|██████    | 605/1000 [1:14:52<38:05,  5.79s/it] 61%|██████    | 606/1000 [1:14:57<36:01,  5.49s/it] 61%|██████    | 607/1000 [1:15:07<45:30,  6.95s/it] 61%|██████    | 608/1000 [1:15:12<41:41,  6.38s/it] 61%|██████    | 609/1000 [1:15:21<46:44,  7.17s/it] 61%|██████    | 610/1000 [1:15:30<49:36,  7.63s/it] 61%|██████    | 611/1000 [1:15:36<47:26,  7.32s/it] 61%|██████    | 612/1000 [1:15:48<56:22,  8.72s/it] 61%|██████▏   | 613/1000 [1:15:55<52:55,  8.21s/it] 61%|██████▏   | 614/1000 [1:16:06<57:22,  8.92s/it] 62%|██████▏   | 615/1000 [1:16:22<1:10:18, 10.96s/it] 62%|██████▏   | 616/1000 [1:16:25<55:05,  8.61s/it]   62%|██████▏   | 617/1000 [1:16:34<56:07,  8.79s/it] 62%|██████▏   | 618/1000 [1:16:39<49:03,  7.71s/it] 62%|██████▏   | 619/1000 [1:16:46<47:01,  7.41s/it] 62%|██████▏   | 620/1000 [1:16:53<45:37,  7.20s/it] 62%|██████▏   | 621/1000 [1:16:58<42:49,  6.78s/it] 62%|██████▏   | 622/1000 [1:17:08<47:42,  7.57s/it] 62%|██████▏   | 623/1000 [1:17:19<54:33,  8.68s/it] 62%|██████▏   | 624/1000 [1:17:23<45:26,  7.25s/it] 62%|██████▎   | 625/1000 [1:17:27<39:25,  6.31s/it] 63%|██████▎   | 626/1000 [1:17:32<36:56,  5.93s/it] 63%|██████▎   | 627/1000 [1:17:43<46:22,  7.46s/it] 63%|██████▎   | 628/1000 [1:17:48<41:59,  6.77s/it] 63%|██████▎   | 629/1000 [1:17:54<39:42,  6.42s/it] 63%|██████▎   | 630/1000 [1:18:01<40:32,  6.57s/it] 63%|██████▎   | 631/1000 [1:18:19<1:01:30, 10.00s/it] 63%|██████▎   | 632/1000 [1:18:23<51:35,  8.41s/it]   63%|██████▎   | 633/1000 [1:18:29<46:46,  7.65s/it] 63%|██████▎   | 634/1000 [1:18:34<41:15,  6.76s/it] 64%|██████▎   | 635/1000 [1:18:39<36:57,  6.07s/it] 64%|██████▎   | 636/1000 [1:18:47<41:50,  6.90s/it] 64%|██████▎   | 637/1000 [1:19:05<1:01:36, 10.18s/it] 64%|██████▍   | 638/1000 [1:19:10<52:32,  8.71s/it]   64%|██████▍   | 639/1000 [1:19:13<41:19,  6.87s/it] 64%|██████▍   | 640/1000 [1:19:19<38:49,  6.47s/it] 64%|██████▍   | 641/1000 [1:19:24<37:32,  6.28s/it] 64%|██████▍   | 642/1000 [1:19:36<47:18,  7.93s/it] 64%|██████▍   | 643/1000 [1:19:40<39:58,  6.72s/it] 64%|██████▍   | 644/1000 [1:19:46<39:09,  6.60s/it] 64%|██████▍   | 645/1000 [1:19:52<36:30,  6.17s/it] 65%|██████▍   | 646/1000 [1:19:58<36:26,  6.18s/it] 65%|██████▍   | 647/1000 [1:20:14<54:05,  9.20s/it] 65%|██████▍   | 648/1000 [1:20:21<50:54,  8.68s/it] 65%|██████▍   | 649/1000 [1:20:32<54:00,  9.23s/it] 65%|██████▌   | 650/1000 [1:20:37<47:17,  8.11s/it] 65%|██████▌   | 651/1000 [1:20:42<40:08,  6.90s/it] 65%|██████▌   | 652/1000 [1:20:47<37:28,  6.46s/it] 65%|██████▌   | 653/1000 [1:20:52<34:29,  5.96s/it] 65%|██████▌   | 654/1000 [1:21:03<43:44,  7.58s/it] 66%|██████▌   | 655/1000 [1:21:10<43:07,  7.50s/it] 66%|██████▌   | 656/1000 [1:21:25<55:23,  9.66s/it] 66%|██████▌   | 657/1000 [1:21:30<46:40,  8.17s/it] 66%|██████▌   | 658/1000 [1:21:39<49:03,  8.61s/it] 66%|██████▌   | 659/1000 [1:21:45<44:19,  7.80s/it] 66%|██████▌   | 660/1000 [1:21:51<40:49,  7.20s/it] 66%|██████▌   | 661/1000 [1:21:58<39:27,  6.98s/it] 66%|██████▌   | 662/1000 [1:22:03<36:10,  6.42s/it] 66%|██████▋   | 663/1000 [1:22:11<38:47,  6.91s/it] 66%|██████▋   | 664/1000 [1:22:15<33:59,  6.07s/it] 66%|██████▋   | 665/1000 [1:22:23<37:26,  6.71s/it] 67%|██████▋   | 666/1000 [1:22:28<33:26,  6.01s/it] 67%|██████▋   | 667/1000 [1:22:42<47:03,  8.48s/it] 67%|██████▋   | 668/1000 [1:22:45<37:50,  6.84s/it] 67%|██████▋   | 669/1000 [1:22:52<39:02,  7.08s/it] 67%|██████▋   | 670/1000 [1:22:58<36:15,  6.59s/it] 67%|██████▋   | 671/1000 [1:23:03<34:08,  6.23s/it] 67%|██████▋   | 672/1000 [1:23:13<40:38,  7.44s/it] 67%|██████▋   | 673/1000 [1:23:22<41:36,  7.64s/it] 67%|██████▋   | 674/1000 [1:23:31<44:50,  8.25s/it] 68%|██████▊   | 675/1000 [1:23:42<48:24,  8.94s/it] 68%|██████▊   | 676/1000 [1:23:46<41:00,  7.60s/it] 68%|██████▊   | 677/1000 [1:23:52<37:17,  6.93s/it] 68%|██████▊   | 678/1000 [1:24:00<40:14,  7.50s/it] 68%|██████▊   | 679/1000 [1:24:12<46:01,  8.60s/it] 68%|██████▊   | 680/1000 [1:24:24<52:19,  9.81s/it] 68%|██████▊   | 681/1000 [1:24:34<51:54,  9.76s/it] 68%|██████▊   | 682/1000 [1:24:45<54:19, 10.25s/it] 68%|██████▊   | 683/1000 [1:24:52<48:08,  9.11s/it] 68%|██████▊   | 684/1000 [1:25:01<47:42,  9.06s/it] 68%|██████▊   | 685/1000 [1:25:08<44:30,  8.48s/it] 69%|██████▊   | 686/1000 [1:25:14<40:50,  7.80s/it] 69%|██████▊   | 687/1000 [1:25:24<43:59,  8.43s/it] 69%|██████▉   | 688/1000 [1:25:39<54:03, 10.39s/it] 69%|██████▉   | 689/1000 [1:25:42<42:43,  8.24s/it] 69%|██████▉   | 690/1000 [1:25:50<41:24,  8.02s/it] 69%|██████▉   | 691/1000 [1:25:58<42:10,  8.19s/it] 69%|██████▉   | 692/1000 [1:26:08<44:18,  8.63s/it] 69%|██████▉   | 693/1000 [1:26:13<39:18,  7.68s/it] 69%|██████▉   | 694/1000 [1:26:24<43:46,  8.58s/it] 70%|██████▉   | 695/1000 [1:26:35<47:14,  9.29s/it] 70%|██████▉   | 696/1000 [1:26:41<42:17,  8.35s/it] 70%|██████▉   | 697/1000 [1:26:44<33:50,  6.70s/it] 70%|██████▉   | 698/1000 [1:26:52<35:14,  7.00s/it] 70%|██████▉   | 699/1000 [1:27:00<36:40,  7.31s/it] 70%|███████   | 700/1000 [1:27:10<41:38,  8.33s/it] 70%|███████   | 701/1000 [1:27:15<36:30,  7.32s/it] 70%|███████   | 702/1000 [1:27:20<32:53,  6.62s/it] 70%|███████   | 703/1000 [1:27:26<31:24,  6.35s/it] 70%|███████   | 704/1000 [1:27:31<29:37,  6.00s/it] 70%|███████   | 705/1000 [1:27:39<32:35,  6.63s/it] 71%|███████   | 706/1000 [1:27:46<32:02,  6.54s/it] 71%|███████   | 707/1000 [1:27:51<29:47,  6.10s/it] 71%|███████   | 708/1000 [1:27:55<27:23,  5.63s/it] 71%|███████   | 709/1000 [1:28:03<30:36,  6.31s/it] 71%|███████   | 710/1000 [1:28:09<29:04,  6.02s/it] 71%|███████   | 711/1000 [1:28:16<30:23,  6.31s/it] 71%|███████   | 712/1000 [1:28:30<41:54,  8.73s/it] 71%|███████▏  | 713/1000 [1:28:36<37:13,  7.78s/it] 71%|███████▏  | 714/1000 [1:28:47<41:41,  8.75s/it] 72%|███████▏  | 715/1000 [1:29:00<47:55, 10.09s/it] 72%|███████▏  | 716/1000 [1:29:08<44:36,  9.42s/it] 72%|███████▏  | 717/1000 [1:29:13<39:19,  8.34s/it] 72%|███████▏  | 718/1000 [1:29:23<40:45,  8.67s/it] 72%|███████▏  | 719/1000 [1:29:35<45:47,  9.78s/it] 72%|███████▏  | 720/1000 [1:29:41<40:18,  8.64s/it] 72%|███████▏  | 721/1000 [1:29:47<35:33,  7.65s/it] 72%|███████▏  | 722/1000 [1:29:53<33:54,  7.32s/it] 72%|███████▏  | 723/1000 [1:30:02<36:06,  7.82s/it] 72%|███████▏  | 724/1000 [1:30:09<34:56,  7.59s/it] 72%|███████▎  | 725/1000 [1:30:14<31:16,  6.82s/it] 73%|███████▎  | 726/1000 [1:30:19<28:20,  6.21s/it] 73%|███████▎  | 727/1000 [1:30:24<27:11,  5.97s/it] 73%|███████▎  | 728/1000 [1:30:34<31:49,  7.02s/it] 73%|███████▎  | 729/1000 [1:30:44<35:45,  7.92s/it] 73%|███████▎  | 730/1000 [1:30:56<40:49,  9.07s/it] 73%|███████▎  | 731/1000 [1:31:00<34:27,  7.69s/it] 73%|███████▎  | 732/1000 [1:31:09<35:47,  8.01s/it] 73%|███████▎  | 733/1000 [1:31:18<37:49,  8.50s/it] 73%|███████▎  | 734/1000 [1:31:27<37:17,  8.41s/it] 74%|███████▎  | 735/1000 [1:31:33<33:47,  7.65s/it] 74%|███████▎  | 736/1000 [1:31:42<36:39,  8.33s/it] 74%|███████▎  | 737/1000 [1:31:50<34:51,  7.95s/it] 74%|███████▍  | 738/1000 [1:31:55<32:00,  7.33s/it] 74%|███████▍  | 739/1000 [1:32:07<37:10,  8.54s/it] 74%|███████▍  | 740/1000 [1:32:15<36:28,  8.42s/it] 74%|███████▍  | 741/1000 [1:32:22<34:28,  7.99s/it] 74%|███████▍  | 742/1000 [1:32:27<30:50,  7.17s/it] 74%|███████▍  | 743/1000 [1:32:34<29:50,  6.97s/it] 74%|███████▍  | 744/1000 [1:32:41<30:25,  7.13s/it] 74%|███████▍  | 745/1000 [1:32:49<31:24,  7.39s/it] 75%|███████▍  | 746/1000 [1:33:04<40:17,  9.52s/it] 75%|███████▍  | 747/1000 [1:33:12<39:15,  9.31s/it] 75%|███████▍  | 748/1000 [1:33:22<38:52,  9.26s/it] 75%|███████▍  | 749/1000 [1:33:29<37:00,  8.85s/it] 75%|███████▌  | 750/1000 [1:33:34<31:50,  7.64s/it] 75%|███████▌  | 751/1000 [1:33:41<30:46,  7.41s/it] 75%|███████▌  | 752/1000 [1:33:47<29:02,  7.03s/it] 75%|███████▌  | 753/1000 [1:33:57<32:47,  7.96s/it] 75%|███████▌  | 754/1000 [1:34:04<31:04,  7.58s/it] 76%|███████▌  | 755/1000 [1:34:12<30:59,  7.59s/it] 76%|███████▌  | 756/1000 [1:34:17<28:27,  7.00s/it] 76%|███████▌  | 757/1000 [1:34:23<26:17,  6.49s/it] 76%|███████▌  | 758/1000 [1:34:30<26:51,  6.66s/it] 76%|███████▌  | 759/1000 [1:34:35<25:25,  6.33s/it] 76%|███████▌  | 760/1000 [1:34:44<28:22,  7.09s/it] 76%|███████▌  | 761/1000 [1:34:51<27:57,  7.02s/it] 76%|███████▌  | 762/1000 [1:35:00<30:00,  7.57s/it] 76%|███████▋  | 763/1000 [1:35:16<39:50, 10.09s/it] 76%|███████▋  | 764/1000 [1:35:28<41:34, 10.57s/it] 76%|███████▋  | 765/1000 [1:35:36<38:33,  9.85s/it] 77%|███████▋  | 766/1000 [1:35:44<36:02,  9.24s/it] 77%|███████▋  | 767/1000 [1:35:47<28:54,  7.44s/it] 77%|███████▋  | 768/1000 [1:35:59<34:35,  8.95s/it] 77%|███████▋  | 769/1000 [1:36:04<29:42,  7.71s/it] 77%|███████▋  | 770/1000 [1:36:16<34:06,  8.90s/it] 77%|███████▋  | 771/1000 [1:36:21<29:35,  7.76s/it] 77%|███████▋  | 772/1000 [1:36:26<26:03,  6.86s/it] 77%|███████▋  | 773/1000 [1:36:32<25:10,  6.66s/it] 77%|███████▋  | 774/1000 [1:36:40<26:57,  7.16s/it] 78%|███████▊  | 775/1000 [1:36:45<24:14,  6.47s/it] 78%|███████▊  | 776/1000 [1:36:51<23:09,  6.20s/it] 78%|███████▊  | 777/1000 [1:36:58<24:57,  6.72s/it] 78%|███████▊  | 778/1000 [1:37:07<26:36,  7.19s/it] 78%|███████▊  | 779/1000 [1:37:14<26:34,  7.22s/it] 78%|███████▊  | 780/1000 [1:37:20<24:37,  6.71s/it] 78%|███████▊  | 781/1000 [1:37:24<22:28,  6.16s/it] 78%|███████▊  | 782/1000 [1:37:29<20:45,  5.71s/it] 78%|███████▊  | 783/1000 [1:37:40<26:37,  7.36s/it] 78%|███████▊  | 784/1000 [1:37:52<31:33,  8.76s/it] 78%|███████▊  | 785/1000 [1:37:57<27:19,  7.63s/it] 79%|███████▊  | 786/1000 [1:38:01<23:12,  6.50s/it] 79%|███████▊  | 787/1000 [1:38:06<21:38,  6.10s/it] 79%|███████▉  | 788/1000 [1:38:15<24:32,  6.95s/it] 79%|███████▉  | 789/1000 [1:38:23<24:53,  7.08s/it] 79%|███████▉  | 790/1000 [1:38:29<23:59,  6.86s/it] 79%|███████▉  | 791/1000 [1:38:42<30:23,  8.72s/it] 79%|███████▉  | 792/1000 [1:38:49<28:07,  8.11s/it] 79%|███████▉  | 793/1000 [1:38:56<26:48,  7.77s/it] 79%|███████▉  | 794/1000 [1:39:01<24:08,  7.03s/it] 80%|███████▉  | 795/1000 [1:39:05<20:39,  6.05s/it] 80%|███████▉  | 796/1000 [1:39:14<23:46,  6.99s/it] 80%|███████▉  | 797/1000 [1:39:25<28:12,  8.34s/it] 80%|███████▉  | 798/1000 [1:39:33<27:42,  8.23s/it] 80%|███████▉  | 799/1000 [1:39:40<26:10,  7.81s/it] 80%|████████  | 800/1000 [1:39:53<30:39,  9.20s/it] 80%|████████  | 801/1000 [1:40:00<28:08,  8.48s/it] 80%|████████  | 802/1000 [1:40:07<27:08,  8.22s/it] 80%|████████  | 803/1000 [1:40:11<22:59,  7.00s/it] 80%|████████  | 804/1000 [1:40:19<23:25,  7.17s/it] 80%|████████  | 805/1000 [1:40:24<20:53,  6.43s/it] 81%|████████  | 806/1000 [1:40:30<20:27,  6.33s/it] 81%|████████  | 807/1000 [1:40:35<19:45,  6.14s/it] 81%|████████  | 808/1000 [1:40:40<18:39,  5.83s/it] 81%|████████  | 809/1000 [1:40:50<21:57,  6.90s/it] 81%|████████  | 810/1000 [1:40:59<24:23,  7.70s/it] 81%|████████  | 811/1000 [1:41:10<26:40,  8.47s/it] 81%|████████  | 812/1000 [1:41:15<23:21,  7.45s/it] 81%|████████▏ | 813/1000 [1:41:23<23:57,  7.69s/it] 81%|████████▏ | 814/1000 [1:41:33<26:12,  8.45s/it] 82%|████████▏ | 815/1000 [1:41:41<25:14,  8.19s/it] 82%|████████▏ | 816/1000 [1:41:59<34:00, 11.09s/it] 82%|████████▏ | 817/1000 [1:42:00<25:18,  8.30s/it] 82%|████████▏ | 818/1000 [1:42:08<24:52,  8.20s/it] 82%|████████▏ | 819/1000 [1:42:16<24:28,  8.11s/it] 82%|████████▏ | 820/1000 [1:42:22<22:09,  7.39s/it] 82%|████████▏ | 821/1000 [1:42:27<20:16,  6.80s/it] 82%|████████▏ | 822/1000 [1:42:36<21:24,  7.22s/it] 82%|████████▏ | 823/1000 [1:42:46<24:21,  8.26s/it] 82%|████████▏ | 824/1000 [1:42:55<24:32,  8.37s/it] 82%|████████▎ | 825/1000 [1:43:01<22:07,  7.59s/it] 83%|████████▎ | 826/1000 [1:43:06<19:44,  6.81s/it] 83%|████████▎ | 827/1000 [1:43:11<18:40,  6.48s/it] 83%|████████▎ | 828/1000 [1:43:19<19:16,  6.72s/it] 83%|████████▎ | 829/1000 [1:43:26<19:19,  6.78s/it] 83%|████████▎ | 830/1000 [1:43:31<18:12,  6.43s/it] 83%|████████▎ | 831/1000 [1:43:36<16:52,  5.99s/it] 83%|████████▎ | 832/1000 [1:43:40<14:52,  5.31s/it] 83%|████████▎ | 833/1000 [1:43:46<15:31,  5.58s/it] 83%|████████▎ | 834/1000 [1:43:49<13:22,  4.84s/it] 84%|████████▎ | 835/1000 [1:43:53<12:26,  4.52s/it] 84%|████████▎ | 836/1000 [1:44:00<14:05,  5.16s/it] 84%|████████▎ | 837/1000 [1:44:04<13:25,  4.94s/it] 84%|████████▍ | 838/1000 [1:44:10<14:06,  5.22s/it] 84%|████████▍ | 839/1000 [1:44:19<16:54,  6.30s/it] 84%|████████▍ | 840/1000 [1:44:26<17:49,  6.69s/it] 84%|████████▍ | 841/1000 [1:44:31<16:21,  6.17s/it] 84%|████████▍ | 842/1000 [1:44:37<15:37,  5.93s/it] 84%|████████▍ | 843/1000 [1:44:42<14:52,  5.69s/it] 84%|████████▍ | 844/1000 [1:44:44<12:21,  4.75s/it] 84%|████████▍ | 845/1000 [1:44:50<12:50,  4.97s/it] 85%|████████▍ | 846/1000 [1:45:00<16:30,  6.43s/it] 85%|████████▍ | 847/1000 [1:45:08<17:59,  7.06s/it] 85%|████████▍ | 848/1000 [1:45:14<16:41,  6.59s/it] 85%|████████▍ | 849/1000 [1:45:18<15:04,  5.99s/it] 85%|████████▌ | 850/1000 [1:45:23<14:10,  5.67s/it] 85%|████████▌ | 851/1000 [1:45:28<13:33,  5.46s/it] 85%|████████▌ | 852/1000 [1:45:38<16:59,  6.89s/it] 85%|████████▌ | 853/1000 [1:45:47<18:16,  7.46s/it] 85%|████████▌ | 854/1000 [1:45:53<17:02,  7.01s/it] 86%|████████▌ | 855/1000 [1:46:01<17:10,  7.10s/it] 86%|████████▌ | 856/1000 [1:46:07<16:56,  7.06s/it] 86%|████████▌ | 857/1000 [1:46:16<18:11,  7.63s/it] 86%|████████▌ | 858/1000 [1:46:20<15:19,  6.48s/it] 86%|████████▌ | 859/1000 [1:46:26<15:02,  6.40s/it] 86%|████████▌ | 860/1000 [1:46:34<15:24,  6.60s/it] 86%|████████▌ | 861/1000 [1:46:39<14:27,  6.24s/it] 86%|████████▌ | 862/1000 [1:46:46<14:59,  6.52s/it] 86%|████████▋ | 863/1000 [1:46:54<15:52,  6.95s/it] 86%|████████▋ | 864/1000 [1:46:58<13:54,  6.13s/it] 86%|████████▋ | 865/1000 [1:47:07<15:47,  7.02s/it] 87%|████████▋ | 866/1000 [1:47:21<19:52,  8.90s/it] 87%|████████▋ | 867/1000 [1:47:24<16:19,  7.36s/it] 87%|████████▋ | 868/1000 [1:47:29<14:04,  6.40s/it] 87%|████████▋ | 869/1000 [1:47:39<16:46,  7.68s/it] 87%|████████▋ | 870/1000 [1:47:46<16:18,  7.53s/it] 87%|████████▋ | 871/1000 [1:47:54<16:04,  7.48s/it] 87%|████████▋ | 872/1000 [1:48:00<15:24,  7.22s/it] 87%|████████▋ | 873/1000 [1:48:06<14:26,  6.82s/it] 87%|████████▋ | 874/1000 [1:48:14<15:00,  7.14s/it] 88%|████████▊ | 875/1000 [1:48:21<14:22,  6.90s/it] 88%|████████▊ | 876/1000 [1:48:25<12:28,  6.04s/it] 88%|████████▊ | 877/1000 [1:48:30<12:12,  5.95s/it] 88%|████████▊ | 878/1000 [1:48:36<12:00,  5.91s/it] 88%|████████▊ | 879/1000 [1:48:41<11:05,  5.50s/it] 88%|████████▊ | 880/1000 [1:48:49<12:30,  6.26s/it] 88%|████████▊ | 881/1000 [1:48:56<13:05,  6.60s/it] 88%|████████▊ | 882/1000 [1:49:01<11:46,  5.99s/it] 88%|████████▊ | 883/1000 [1:49:08<12:44,  6.54s/it] 88%|████████▊ | 884/1000 [1:49:15<12:41,  6.57s/it] 88%|████████▊ | 885/1000 [1:49:22<12:46,  6.67s/it] 89%|████████▊ | 886/1000 [1:49:28<12:27,  6.56s/it] 89%|████████▊ | 887/1000 [1:49:34<11:48,  6.27s/it] 89%|████████▉ | 888/1000 [1:49:39<11:08,  5.97s/it] 89%|████████▉ | 889/1000 [1:49:44<10:29,  5.67s/it] 89%|████████▉ | 890/1000 [1:49:51<10:59,  5.99s/it] 89%|████████▉ | 891/1000 [1:49:55<09:58,  5.49s/it] 89%|████████▉ | 892/1000 [1:50:02<10:40,  5.93s/it] 89%|████████▉ | 893/1000 [1:50:07<09:59,  5.60s/it] 89%|████████▉ | 894/1000 [1:50:14<10:39,  6.03s/it] 90%|████████▉ | 895/1000 [1:50:25<12:59,  7.42s/it] 90%|████████▉ | 896/1000 [1:50:30<11:55,  6.88s/it] 90%|████████▉ | 897/1000 [1:50:36<11:00,  6.41s/it] 90%|████████▉ | 898/1000 [1:50:46<12:50,  7.55s/it] 90%|████████▉ | 899/1000 [1:50:54<13:09,  7.82s/it] 90%|█████████ | 900/1000 [1:50:58<11:05,  6.65s/it] 90%|█████████ | 901/1000 [1:51:11<14:03,  8.52s/it] 90%|█████████ | 902/1000 [1:51:14<11:23,  6.97s/it] 90%|█████████ | 903/1000 [1:51:21<11:08,  6.89s/it] 90%|█████████ | 904/1000 [1:51:26<10:06,  6.31s/it] 90%|█████████ | 905/1000 [1:51:33<10:04,  6.36s/it] 91%|█████████ | 906/1000 [1:51:36<08:43,  5.57s/it] 91%|█████████ | 907/1000 [1:51:42<08:55,  5.75s/it] 91%|█████████ | 908/1000 [1:51:49<09:14,  6.02s/it] 91%|█████████ | 909/1000 [1:51:55<09:03,  5.97s/it] 91%|█████████ | 910/1000 [1:52:12<13:42,  9.14s/it] 91%|█████████ | 911/1000 [1:52:17<12:00,  8.09s/it] 91%|█████████ | 912/1000 [1:52:23<10:45,  7.33s/it] 91%|█████████▏| 913/1000 [1:52:27<09:24,  6.48s/it] 91%|█████████▏| 914/1000 [1:52:35<09:39,  6.73s/it] 92%|█████████▏| 915/1000 [1:52:43<10:14,  7.23s/it] 92%|█████████▏| 916/1000 [1:52:48<09:24,  6.72s/it] 92%|█████████▏| 917/1000 [1:53:00<11:08,  8.06s/it] 92%|█████████▏| 918/1000 [1:53:06<10:09,  7.43s/it] 92%|█████████▏| 919/1000 [1:53:12<09:40,  7.16s/it] 92%|█████████▏| 920/1000 [1:53:22<10:37,  7.97s/it] 92%|█████████▏| 921/1000 [1:53:28<09:36,  7.30s/it] 92%|█████████▏| 922/1000 [1:53:33<08:50,  6.79s/it] 92%|█████████▏| 923/1000 [1:53:44<10:14,  7.98s/it] 92%|█████████▏| 924/1000 [1:53:50<09:12,  7.27s/it] 92%|█████████▎| 925/1000 [1:53:56<08:34,  6.86s/it] 93%|█████████▎| 926/1000 [1:54:02<08:24,  6.81s/it] 93%|█████████▎| 927/1000 [1:54:08<08:00,  6.58s/it] 93%|█████████▎| 928/1000 [1:54:14<07:34,  6.31s/it] 93%|█████████▎| 929/1000 [1:54:20<07:24,  6.27s/it] 93%|█████████▎| 930/1000 [1:54:29<08:15,  7.09s/it] 93%|█████████▎| 931/1000 [1:54:35<07:47,  6.77s/it] 93%|█████████▎| 932/1000 [1:54:41<07:22,  6.50s/it] 93%|█████████▎| 933/1000 [1:54:47<07:05,  6.36s/it] 93%|█████████▎| 934/1000 [1:54:55<07:33,  6.87s/it] 94%|█████████▎| 935/1000 [1:55:05<08:30,  7.85s/it] 94%|█████████▎| 936/1000 [1:55:17<09:31,  8.93s/it] 94%|█████████▎| 937/1000 [1:55:24<08:41,  8.27s/it] 94%|█████████▍| 938/1000 [1:55:31<08:09,  7.90s/it] 94%|█████████▍| 939/1000 [1:55:36<07:16,  7.16s/it] 94%|█████████▍| 940/1000 [1:55:40<06:16,  6.27s/it] 94%|█████████▍| 941/1000 [1:55:45<05:43,  5.82s/it] 94%|█████████▍| 942/1000 [1:55:49<05:13,  5.41s/it] 94%|█████████▍| 943/1000 [1:55:56<05:25,  5.71s/it] 94%|█████████▍| 944/1000 [1:56:14<08:49,  9.45s/it] 94%|█████████▍| 945/1000 [1:56:19<07:26,  8.11s/it] 95%|█████████▍| 946/1000 [1:56:35<09:30, 10.57s/it] 95%|█████████▍| 947/1000 [1:56:42<08:14,  9.32s/it] 95%|█████████▍| 948/1000 [1:56:51<08:05,  9.34s/it] 95%|█████████▍| 949/1000 [1:56:59<07:28,  8.79s/it] 95%|█████████▌| 950/1000 [1:57:03<06:17,  7.55s/it] 95%|█████████▌| 951/1000 [1:57:08<05:35,  6.84s/it] 95%|█████████▌| 952/1000 [1:57:18<06:05,  7.62s/it] 95%|█████████▌| 953/1000 [1:57:28<06:38,  8.49s/it] 95%|█████████▌| 954/1000 [1:57:36<06:13,  8.11s/it] 96%|█████████▌| 955/1000 [1:57:44<06:10,  8.24s/it] 96%|█████████▌| 956/1000 [1:57:49<05:12,  7.09s/it] 96%|█████████▌| 957/1000 [1:58:02<06:21,  8.88s/it] 96%|█████████▌| 958/1000 [1:58:05<05:04,  7.25s/it] 96%|█████████▌| 959/1000 [1:58:11<04:47,  7.00s/it] 96%|█████████▌| 960/1000 [1:58:17<04:19,  6.48s/it] 96%|█████████▌| 961/1000 [1:58:26<04:44,  7.30s/it] 96%|█████████▌| 962/1000 [1:58:31<04:16,  6.76s/it] 96%|█████████▋| 963/1000 [1:58:40<04:32,  7.37s/it] 96%|█████████▋| 964/1000 [1:58:47<04:17,  7.14s/it] 96%|█████████▋| 965/1000 [1:59:02<05:32,  9.51s/it] 97%|█████████▋| 966/1000 [1:59:12<05:26,  9.61s/it] 97%|█████████▋| 967/1000 [1:59:22<05:19,  9.69s/it] 97%|█████████▋| 968/1000 [1:59:26<04:15,  7.99s/it] 97%|█████████▋| 969/1000 [1:59:40<05:08,  9.97s/it] 97%|█████████▋| 970/1000 [1:59:44<04:03,  8.10s/it] 97%|█████████▋| 971/1000 [1:59:54<04:11,  8.68s/it] 97%|█████████▋| 972/1000 [1:59:59<03:29,  7.48s/it] 97%|█████████▋| 973/1000 [2:00:08<03:38,  8.09s/it] 97%|█████████▋| 974/1000 [2:00:14<03:15,  7.53s/it] 98%|█████████▊| 975/1000 [2:00:25<03:31,  8.46s/it] 98%|█████████▊| 976/1000 [2:00:31<03:02,  7.59s/it] 98%|█████████▊| 977/1000 [2:00:39<02:57,  7.71s/it] 98%|█████████▊| 978/1000 [2:00:44<02:35,  7.07s/it] 98%|█████████▊| 979/1000 [2:00:48<02:10,  6.23s/it] 98%|█████████▊| 980/1000 [2:00:56<02:14,  6.74s/it] 98%|█████████▊| 981/1000 [2:01:02<02:01,  6.39s/it] 98%|█████████▊| 982/1000 [2:01:09<01:59,  6.64s/it] 98%|█████████▊| 983/1000 [2:01:19<02:07,  7.51s/it] 98%|█████████▊| 984/1000 [2:01:26<01:59,  7.50s/it] 98%|█████████▊| 985/1000 [2:01:33<01:49,  7.32s/it] 99%|█████████▊| 986/1000 [2:01:40<01:39,  7.08s/it] 99%|█████████▊| 987/1000 [2:01:49<01:40,  7.74s/it] 99%|█████████▉| 988/1000 [2:01:55<01:26,  7.25s/it] 99%|█████████▉| 989/1000 [2:02:02<01:17,  7.08s/it] 99%|█████████▉| 990/1000 [2:02:09<01:10,  7.03s/it] 99%|█████████▉| 991/1000 [2:02:12<00:53,  5.92s/it] 99%|█████████▉| 992/1000 [2:02:18<00:48,  6.00s/it] 99%|█████████▉| 993/1000 [2:02:23<00:40,  5.72s/it] 99%|█████████▉| 994/1000 [2:02:33<00:41,  6.89s/it]100%|█████████▉| 995/1000 [2:02:40<00:34,  6.91s/it]100%|█████████▉| 996/1000 [2:02:53<00:34,  8.72s/it]100%|█████████▉| 997/1000 [2:02:57<00:21,  7.32s/it]100%|█████████▉| 998/1000 [2:03:02<00:13,  6.56s/it]100%|█████████▉| 999/1000 [2:03:13<00:07,  7.90s/it]100%|██████████| 1000/1000 [2:03:17<00:00,  7.00s/it]100%|██████████| 1000/1000 [2:03:17<00:00,  7.40s/it]
